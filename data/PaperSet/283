Gene expression Analysis of gene expression data using a linear mixed model/finite mixture model approach: application to regional differences in the human brain the United Kingdom Brain Expression Consortium (UKBEC) Motivation: Gene expression data exhibit common information over the genome. This article shows how data can be analysed from an efficient whole-genome perspective. Further, the methods have been developed so that users with limited expertise in bioinformatics and statistical computing techniques could use and modify this procedure to their own needs. The method outlined first uses a large-scale linear mixed model for the expression data genome-wide, and then uses finite mixture models to separate differentially expressed (DE) from non-DE transcripts. These methods are illustrated through application to an exceptional UK Brain Expression Consortium involving 12 human frozen post-mortem brain regions. Results: Fitting linear mixed models has allowed variation in gene expression between different biological states (e.g. brain regions, gender, age) to be investigated. The model can be extended to allow for differing levels of variation between different biological states. Predicted values of the random effects show the effects of each transcript in a particular biological state. Using the UK Brain Expression Consortium data, this approach yielded striking patterns of co-regional gene expression. Fitting the finite mixture model to the effects within each state provides a convenient method to filter transcripts that are DE: these DE transcripts can then be extracted for advanced functional analysis.Research efforts in molecular and cellular biology are vital to develop our understanding of, for example, the human central nervous system (CNS) function, and to dissect the functional complexity and the progress of diseases. These rapid advancements were mainly determined by the genomic, transcriptomic and diagnostic technological innovations . These technologies, such as expression arrays and RNA sequencing, are applicable at the genome-wide scale, and their application results in more insightful information being produced, and this can lead towards more informed investigations and in turn through to clinical applications. However, the analyses and the volume of the data that are generated from these complex techniques are computationally challenging. To date, there is no standard protocol available to normalize and process the raw data from these experiments into a manageable format that can then be further interrogated on a personal computer and to align and assemble the data to a reference genome by scientists with no great experience in bioinformatics . With expression arrays and RNA sequencing increasing in coverage and reducing in cost, they are now widely used with increasingly large sample sizes. Consequently, there is an ongoing need to develop and assess computationally efficient means of analysing big data. Although there is no universal approach to gene expression analysis, the conventional approach is to consider a model for one gene at a time, and consider if this particular gene is differentially expressed (DE) between any of the biological states considered, for example, as implemented in the limma package of R . Naturally, this results in an extreme multiple testing situation, which is usually overcome by using an appropriate false discovery rate (FDR) control, such as the q-value procedure. The alternative is to consider a global model for all the expression data and replace the many thousands of null and alternative hypotheses by more global tests to dissect causes of variation in gene expression. Such an approach would have two advantages. First, transcripts are not independent identities, and there is much common information across the genome, so combining this would be expected to increase power of signal detection. Second, although a more philosophical point, we question whether it is best practice to consider many thousands of separate hypotheses: as students, we were taught the importance of writing out the null and alternative hypotheses To whom correspondence should be addressed.for every statistical test, and this pragmatic approach would seem to fail here. So a global or genome-wide approach can be achieved by fitting linear mixed models (LMMs) to all the data, for example, as has been done by, with the variation between transcripts as another source of variation.suggested a similar process using hierarchical models. One potential limitation of the approach of using an LMM fitted to all the expression data is that it may not allow for differences in variability in expression across the biological states of the system under study. For example, in the human brain, some regions of the brain show greater levels of variation in gene expression than in other regions, However, improvements in LMM software, for example, as described inand Pinheiro and Bates (2000), allowing flexible variance structure modelling, make it worthwhile again considering the use of large-scale LMMs in expression data analysis. Fitting LMMs by themselves does not lead to the determination of which individual transcripts are DE, nor do the LMMs identify clusters of transcripts with similar expression profiles. However, fitting of finite mixture models has been used successfully in this endeavour, to separate out potential DE from nonDE transcripts . The following article demonstrates how these two approaches can be adopted in the analysis of a large gene expression dataset. The methods are described to allow easy implementation in other situations. While some of this approach has been described in previous papers , the present article provides additional detail and extends these methods. To illustrate the methods, one of the most comprehensive gene expression databases related to human brain tissue, namely, the UK Brain Expression Consortium (UKBEC), is used, highlighting different expression patterns across the brain regions .The estimates of the overall expression mean and the variance components from each random subset, as well as their combined estimates, are shown in. It is clear that the estimates from both subsets are similar and consistent with sampling fluctuations. There is very little between-array variation, as indicated by its small estimated variance component 2 A 0:007 AE 0:0002.There was a relatively large estimated variance component to assess between-transcript variation 2 T 2:032 AE 0:0209. (BLUPs of overall transcripts effects can be obtained to highlight differing overall expression levels between transcripts.) For the transcript-specific effects in individual regions, the level of variation differed widely: CRBL showed the greatest level of variation 2 TR, 1 0:799 AE 0:0083, followed by WHMT 2 TR, 12 0:411 AE 0:0044, with HYPO displaying the least variation 2 TR, 4 0:042 AE 0:0008. Note the formal REML LRTs indicate the heterogeneous model was superior to a homogeneous model with a common between-transcript variance: for Part 1, LRT 21 044 and for Part 2, LRT 20 327, and compared with a chi-square distribution with 11 df returns P 0 in both. Estimated residual variation 2 0:202 AE 0:0001 accounted for between 6.6 and 8.8 of the variation in expression levels (depending on the region). Supplementaryshows a boxplot of the BLUPs of the transcript region interaction effects. It corroborates the large estimated variance for CRBL and WHMT, and the small variance for HYPO. Supplementaryshows the correlation matrix (Pearson correlation, r) between these region-specific transcript effects. Correlations (almost) 40.5 in absolute value have been coloured (red: positive; blue: negative), and those 40.75 are shown in bold. It is noteworthy that the expression levels in CRBL were not substantially correlated with those in other regions, the greatest being with OCTX (r 0.318). The highest correlations (all positive) were between the three cortex regions (FCTX, OCTX and TCTX). To illustrate,shows a smoothed scatter plot (using the smoothScatter function in the geneplotter package of R) indicating the strong correlations between FCTX and OCTX (r 0.86). Supplementaryshows similar pairs of regions for which jrj40.5, including a negative correlation between TCTX and SPCO (r 0.75).shows details of the region-specific transcript effects for HYPO. (Similar results for the other 11 regions can be found in the Supplementary Figs S6S9).shows the histogram of the distributions of transcripts effects, supported by a corresponding QQ plot in. They show evidence of some extreme effects, i.e. differential gene expression, both upregulated and downregulated, as indicated by the departures from the line in the QQ plot. Similar findings are found in the other regions. The two-component mixture model was fitted to the transcript effects within each region, and these are summarized in. On average, just 530 of transcripts show differential expression in any one region, although this varies from 16 (HYPO) to 45 (OCTX). In terms of variation in expression, the DE transcripts show on average 3.3 greater standard deviations than non-DE transcripts, and this ratio is fairly consistent across regions (r 0.90). Note that using results for mixture distributions, the overall REML variance estimates incan be approximately reproduced here, as 2 TR, k 1 2 1 1 1 2 0 (because the means of both mixture components can be taken as 0). To illustrate here for CRBL, fromshows the QQ plot for the fitted mixture model for HYPO: while not a perfect fit, they fit the expression effects reasonably well, indicating that a two-component mixture model is adequate. (Two transcripts are identified on that plot as being even more extreme: one upregulated and one downregulated).shows the distribution of the probabilities of transcripts being DE in HYPO. What is apparent for each region is a cluster of transcripts with extremely high probabilities of being DE, i.e. DE transcripts. Transcripts with a probability 40.8 can be selected as being DE. The average of the values (1 ) for transcripts declared DE is the FDR: for the current dataset, this is calculated as 4.3 
