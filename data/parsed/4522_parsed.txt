data_and taggerone joint named_entity and normalization with semi_markov models_motivation text_mining is increasingly used to manage the accelerating pace of the biomedical_literature many text_mining applications depend on accurate named_entity ner and normalization grounding while high_performing machine_learning trainable for many entity types exist for ner normalization_methods are usually specialized to a single_entity type ner and normalization systems are also typically used in a serial pipeline causing cascading errors and limiting the ability of the ner system to directly exploit the lexical information provided by the normalization methods we propose the first machine_learning model for joint ner and normalization during both training and prediction the model is trainable for arbitrary entity types and consists of a semi_markov structured linear classifier with a rich feature approach for ner and supervised semantic indexing for normalization we also introduce taggerone a java implementation of our model as a general toolkit for joint ner and normalization taggerone is not specific to any entity type requiring only annotated training data and a corresponding lexicon and has been optimized for high_throughput results we validated taggerone with multiple gold_standard corpora containing both mention and concept level annotations benchmarking results show that taggerone achieves high performance on diseases ncbi disease corpus ner f score normalization f score and chemicals biocreative cdr corpus ner f score normalization f score these results compare favorably to the previous state of the art notwithstanding the greater_flexibility of the model we conclude that jointly modeling ner and normalization greatly improves_performance availability_and the taggerone source_code and an online demonstration are available at http www ncbi nlm nih gov bionlp taggerone contact zhiyong lu nih govmany tasks in biomedical information_extraction rely on accurate named_entity ner the identification of text spans mentioning a concept of a specific class such as disease or chemical recent research has demonstrated that a particular ner approachnamely conditional_random with a rich feature_set consistently achieves high performance on a variety of ner tasks when provided with an appropriate training corpus and a relatively small investment in feature engineering this approach has been used to identify a wide_variety of entities including genes and proteins diseases chemicals and anatomic entities however many end_user tasks also require normalization grounding the identification of the concept mentioned within a controlled_vocabulary or ontology making the utility of ner on its own relatively low we recently demonstrated dnorm the first machine_learning method for disease normalization this method used supervised semantic indexing trained with pairwise learning to rank to score the mentions returned by a conditional_random ner system banner against the disease names from a controlled_vocabulary the method focuses_primarily semantic term variation such as when an author refers to the concept renal_insufficiency with the phrase decreased renal_function our experiments demonstrated the method to be highly_effective for disease normalization like many normalization systems however dnorm uses a pipeline architecture the tasks of ner and normalization are performed serially making errors cascading from one component to the next a common problem our error analysis of dnorm for example demonstrated that over half of the overall system errors were caused by ner errors that the normalization component could not recover one way to overcome cascading errors is to perform ner and normalization simultaneously dictionary systems do this by directly matching text to the names in a controlled_vocabulary unfortunately ner systems employing machine_learning typically have higher performance to the best of our knowledge a machine_learning that trains a joint model of ner and normalization has not been previously_proposed in this work we propose a model that simultaneously performs ner and normalizationfocusing on term variationduring both training and prediction we evaluate our model on two corpora containing both mention and concept annotations one contains disease_entities the other contains both disease and chemical_entities provides an example text with both disease and chemical annotations we achieve state of the art performance on both diseases and chemicals we validate taggerone by applying it to two corpora containing both mention and concept level annotations the ncbi disease corpus and the biocreative v chemical disease relation task corpus overall statistics for each dataset are provided in the ncbi disease corpus consists of pubmed_abstracts separated into training development and test subsets the ncbi disease corpus is annotated with disease mentions using concept identifiers from either mesh or omim the biocreative v chemical disease relation bc cdr corpus consists of pubmed_abstracts separated into training and test_sets we created a holdout set by separating the sample set abstracts from the remainder of the training_set the bc cdr corpus enables experiments simultaneously modeling multiple entity types it is annotated with concept identifiers from mesh for both chemical and disease mentions we use two evaluation measures since our model performs both ner and normalization the ner measure is at the mention level we require the predicted span and entity type to exactly match the annotated span and entity type the normalization measure is at the abstract level comparing the set of concepts predicted for the document to the set annotated independent of their location within the text we report both measures in terms of micro averaged precision recall and f score we perform two sets of experiments the first set of experiments evaluates the ability of the model to generalize to unseen text and whether joint ner and normalization improves_performance over performing ner separately this set of experiments models diseases and chemicals separately the second set of experiments evaluates the ability of the model to simultaneously handle multiple entity types both diseases and chemicals the single_entity performance demonstrates both that our model is effective and that jointly modeling ner and normalization improves_performance our results significantly improve on dnorm for diseases and on tmchem for chemicals analyzing the dnorm and taggerone results provides insight into the advantage of joint prediction dnorm often misses phrases that require term variation to be resolved for the phrase to be recognized as an entity such as abnormal involuntary motor movements annotated as mesh identifier d drug_induced dyskinesia the experiment jointly modeling chemicals and diseases demonstrates that the model maintains high performance while modeling multiple entity types modeling multiple entity types simultaneously may be advantageous when the entity types are more difficult to distinguish such as with anatomical types our results on the ncbi disease corpus are the highest of which we are aware the only normalization system with published results on the ncbi disease corpus besides dnorm is the sieve based system of d their evaluation measure calculates the proportion of mentions correctly normalized given perfect ner using this measure their system scored taggerone scores the recent disease subtask at the biocreative v chemical disease relation task provides an excellent comparison for our system the uet cam system performs joint ner and normalization for prediction but unlike taggerone does not perform joint training it achieved an f score of the highest performing system at the bc cdr disease subtask achieved precision recall for f score we note that expanding the lexicon was a significant feature in most participating systems in this manuscript our goal is to automatically learn the best mapping to an existing lexicon these two approaches are complementary however we are not aware of any previous performance_evaluations on the chemical_entities of the bc cdr corpus we originally trained our model using an averaged perceptron ner performance was similar but normalization performance was several percent lower data not shown we believe this was due to using the same update size for both the ner and normalization weights our use of semi markov_models allows us to scale the normalization vectors for the mentions to unit length performance degrades significantly when this scaling is not performed data not shown we conclude that jointly modeling named_entity and normalization results in improved_performance for both tasks our model is not entity specific and we expect it to generalize to arbitrary ner and normalization problems in biomedicine in this work we have demonstrated this capability for both diseases and chemicals in future work we intend to integrate a more robust disambiguation method to allow entity types such as genes and proteins to be addressed we are also interested in investigation its application to the general domain while our goal has been to learn the best mapping to an existing lexicon expanding the lexicon is a complementary_approach used by many normalization systems b c we anticipate that applying our method to an expanded lexicon would further increase performance an interesting research direction enabled by this work is the possibility of using data not annotated jointly sources of annotations at the document level are significantly more abundant than annotations at the mention level we anticipate our model may enable entity level distant supervision by providing a joint model of both ner and normalization that handles term variation taggerone results on the bc cdr corpus when both disease and chemical mentions are trained within a single model precision recall and f score are micro averaged and respectively abbreviated as p r and f 
