when less is more 'slicing' sequencing_data improves read decoding accuracy and de_novo assembly_quality motivation as the invention of dna_sequencing in the s computational_biologists have had to deal with the problem of de_novo with limited or insufficient depth of sequenc ing in this work we investigate the opposite problem that is the challenge of dealing with excessive depth of sequencing results we explore the effect of ultra_deep data in two domains i the problem of decoding reads to bacterial_artificial bac clones in the context of the combinatorial pooling design we have recently_proposed and ii the problem of de_novo of bac clones using real ultra_deep data we show that when the depth of sequencing increases over a certain threshold sequencing_errors make these two problems harder and harder instead of easier as one would expect with error_free data and as a consequence the quality of the solution degrades with more and more data for the first problem we propose an effective solution based on divide_and we slice a large dataset into smaller samples of optimal size decode each slice independently and then merge the results experimental_results on over barley bacs and over cowpea bacs demonstrate a significant_improvement in the quality of the decoding and the final assembly for the second problem we show for the first time that modern de_novo assemblers cannot take advantage of ultra_deep data availability_and python scripts to process slices and resolve decoding conflicts are available fromwe have recently_introduced in a novel protocol for clone by clone de_novo genome_sequencing that leverages recent_advances in combinatorial pooling design also known as group testing in our sequencing protocol subsets of non redundant genome tiling bacterial_artificial bacs are chosen to form intersecting pools then groups of pools are sequenced on an illumina_sequencing instrument via low multiplex dnabarcoding sequenced_reads can be assigned decoded to specific bacs by relying on the combinatorial structure of the pooling design since the identity of each bac is encoded within the pooling pattern the identity of each read is similarly encoded within the pattern of pools in which it occurs finally bacs are assembled individually simplifying the problem of resolving genome_wide repetitive_sequences in we reported preliminary assembly statistics on the performance of our protocol in four barley hordeum_vulgare bac sets hv hv further analysis on additional barley bac sets and two genome_wide bac sets for cowpea vigna unguiculata revealed that the raw_sequence for some datasets was of significantly lower_quality i e higher sequencing_error than others we realized that our decoding strategy solely based on the software hashfilter was insufficient to deal with the amount of noise in poor_quality datasets we attempted to i trim clean the reads more aggressively or with different methods ii identify low quality tiles on the flow cell and remove the corresponding reads e g tiles on the bottom middle swath iii identify positions in the reads possibly affected by sequencing bubbles and iv post process the reads using available error_correction software_tools e g quake reptile unfortunately none of these steps accomplished a dramatic_increase in the percentage of reads that could be assigned to bacs indicating that the quality of the dataset did not improve very much these attempts to improve the outcome led however to a serendipitous discovery we noticed that when hashfilter processed only a portion of the dataset the proportion of assigned decoded reads increased this observation initially seemed counterintuitive we expected that feeding less data into our algorithm meant that we had less information to work with thus decrease the decoding performance instead the explanation is that when data is corrupted more noisy_data is not better but worse the study reported here directly addresses the observation that when dealing with large_quantities of imperfect sequencing_data less can be more more specifically we report i an extensive analysis of the trade_off between the size of the datasets and the ability of decoding reads to individual bacs ii a method based on slicing datasets that significantly_improves the number of decoded reads and the quality of the resulting bac assemblies iii an analysis of bac assembly_quality as a function of the depth of sequencing for both real and synthetic data our algorithmic solution relies on a divide_and approach as illustrated in once all the decoded reads are assigned to bacs using the procedure above velvet is executed to assemble each bac individually as was done in we generated multiple assembly for several choices of velvets l mer hash size step of the assembly reported is the one that maximizes the n n indicates the length for which the set of all contigs of that length or longer contains at least half of the total size of all contigs we employed several metrics to evaluate the improvement in read decoding and assembly enabled by the slicing algorithm for one of the barley sets hv we executed hashfilter using several choices of k kon the full m reads dataset i e with no slicing as well as with k using the slicing algorithm described ealrier the first five rows ofsummarize the decoding results first observe that as we increase k the number of decoded reads increases monotonically however if one fixes k in this case k which is the maximum allowed by hashfilter slicing hv in slices of m reads increases significantly the number of decoded reads compared with available for assembly analysis of the number of assignments to ghost bacs also shows significant_improvement in the decoding accuracy when using slicing of the reads are assigned to unused bac signatures compared with when hashfilter is used on the full dataset we carried_out a similar analysis on slicing sequencing_data improves read decoding and assemblyhv when the full dataset was processed with hashfilter k the number of reads assigned to ghost bacs was very high m reads out of m when the optimal slicing is used k only reads out of m are assigned to ghost bacs also observe inhow the improved decoding affects the quality of the assembly for hv when comparing no slicing to slicing based decoding the average n jumps from to bp both for k and the number of reads used by velvet in the assembly increases from to for hv we also measured the number of decoded reads that map with and mismatches to the assembly of a subset of bacs that are available from reports the average_percentage of decoded reads either from the full dataset or from the optimal slicing that bowtie can map to the based assemblies observe how the slicing step improves by the number of reads_mapped to the corresponding bac assembly suggesting a similar improvement in decoding accuracy similar improvements in decoding accuracy were observed on the other datasets data not shown on hv we investigated the effect of the slice size on the decoding and assembly statistics earlier we claimed that the optimal size corresponds to the peak of the graphs in for instance notice that the peak for hv is m reads we decoded and assembled reads using slicing sizes of m reads as well as non optimal slice size of m reads the experimental_results are shown in observe that the decoding with m does not achieve the same decoding accuracy or assembly_quality of the slicing with m but again both are significantly better than without slicing again notice inhow improving the read decoding affects the quality of the assembly the average n increases from bp k no slicing to bp k optimal slicing and the number of reads used by velvet in the assembly increases from to respectively for hv genes were known to belong to a specific bac clone the assembly using slicing based coding recovered at least of the sequence of of them compared with using no slicing finally we compared the performance of our slicing method against the experimental_results in which were obtained by running hashfilter with no data slicing k the basic decoding and assembly statistics when no slicing is used are reported in first observe the large_variability of results among the sets although the average number of decoded reads for k is m there are sets which have less than half that amount hv and hv and sets have more than twice the average e g hv as a consequence the average fold coverage ranges from hv to hv in general the assembly statistics without slicing based decoding are not very satisfactory the n ranges from hv to bp hv the percentage of reads used by velvet ranges from hv to hv and hv the percentage of known genes covered at least of their length by the assemblies ranged from hv to hv when we decoded the same datasets using the optimal slice size using this time k the assemblies improved drastically the decoding and assembly statistics are summarized in note that each set has its optimal size and the corresponding number of slices first observe how the number of decoded reads increased_significantly for most datasets e g m for hv m for hv m for hv m for vu and m for vu only for two datasets the number of decoded reads decreased_slightly by m reads in hv and by m in hv for all the datasets the average n increased significantlyfrom an average of about to kbp see supplementary dataset for detailed assembly statistics on each dataset even for datasets for decoding and assembly statistics for hv comparing no slicing and slicing with two different slice sizes m reads is optimal according to the peak in which slicing decreased the number reads hv and hv the n increased_significantly the number of reads used by velvet increased from an average of the fraction of known genes that were recovered by the assemblies increased from to we recognize that the improvement from tables to is not just due to the slicing but also to the increased k from to we have already addressed this point in where we showed that increasing k from to helps the decoding assembly but the main boost in accuracy and quality is due to slicing recall that the assemblies in tables and were carried_out using velvet with l and choosing the assembly with the largest n on the hv dataset we have also tested velvet with fixed l spades with l and idbaud with l see supplementary velvet best n and spades performance were comparable while idba ud achieved lower n we also tested velvet with l and spades with l on all the other datasets supplementary setting l for velvet led to less bloated assemblies somewhat comparable to spades output as a final step we investigated how the depth of sequencing affects bac assembly_quality to this end we multiplexed barley bacs on one lane of the illumina_hiseq using custom multiplexing adapters the size of these bacs ranged kbp see supplementary after demultiplexing the sequenced_reads we obtained m bases paired_end insert_size of bases we quality trimmed the reads then cleaned them of spurious sequencing adaptors finally reads affected by e coli contamination or bac vector were discarded the final number of cleaned reads was m with an average length of bases the depth of sequencing for the bacs ranged from to see supplementary another set of barley bacs was sequenced by the department of energy joint genome institute using sanger long_reads all bacs were sequenced and finished using phred phrap consed to a targeted depth of the primary dna_sequences for each of these bacs were assembled in one contig although two of them were considered partial_sequence the intersection between the set of bacs sequenced using the illumina instrument and the set of bacs sequenced using sanger is a set of seven bacs highlighted in bold in supplementary but one of these seven bacs is not full_length l we used the six full_length sanger based bac assemblies as the ground_truth to assess the quality of the assemblies from illumina read at increasing_depth of sequencing to this end we generated datasets corresponding to and depth of sequencing for each of the six bacs by sampling uniformly short_reads from the high depth datasets for each choice of the depth of sequencing we generated different datasets for a total of datasets we assembled the reads on each dataset with velvet v with hash value k to minimize the probability of false overlaps and collected statistics for the resulting assemblies shows the value of n a the size ofthe largest contig b the percentage of the target bac not available in the assembly c and number of assembly errors d for increasing_depth of sequencing each point in the graph is the average over the datasets and error_bars indicate the sd in order to compute the number of assembly errors we used the tool developed for the gage competition according to gage the number of assembly errors is defined as the number of locations with insertion deletions of at least six nucleotides plus the number of translocations and inversions a few observations onare in order first note that both the n and the size of the longest contig reach a maximum in the range depending on the bac also observe that in order to minimize the percentage of bac missed by the assembly one needs to keep the depth of sequencing below too much depth decreases the coverage of the target finally it is very clear from d that as the depth of sequencing increases so do the number of assembly errors with the exception of one bac we have also investigated whether similar observations could be drawn for other assemblers in we report the same assembly statistics namely a the value of n b the size of the largest contig c the percentage of the target bac not available in the assembly and d number of assembly errors for increasing_depth of sequencing for one of the bacs this time we used three assemblers namely velvet spades v and idba ud statistics for all bacs are available in supplementary figs s although there are performance differences among the three assemblers the common trend is that as the coverage increases the n and the size of the largest contig decreases while the percentage of the bac missing and the number of assembly errors increases among the three assemblers spades appears to be less affected by high coverage spades was run with hash values k and option careful other parameters were default idba ud was run with hash values k other parameters were default the reported assembly is the one chosen by idba ud independently from us the authors of made similar observations on assembly degratadation in their study the authors assembled e coli mb saccharomyces kudriavzevii mb and caenorhabditis elegans mb using soapdenovo velvet abyss meraculous and idba ud at increasing sequencing_depths up to their analysis showed that the optimum sequencing_depth for assembling these genomes is about depending on the specific genome and assembler finally we analyzed the performance of idba ud spades and velvet on simulated reads we generated bp paired_end from the sanger assembly of bac b using the read simulator wgsim github com lh wgsim at and depth of sequencing insert length was bp with a standard_deviation of bp for each depth of sequencing we generated simulated reads at and sequencing_error substitutions insertions_and were not allowed idba ud was executed with hash values k other parameters were default velvet was run with k we repeated the simulations times for idba ud and times for velvet and spades in supplementary we report the usual assembly statistics namely n largest contig percentage missing and number of assembly errors for velvet idba ud and spades on these datasets observe that with perfect reads error_rate ultra deep coverage does not affect the performance of idba ud and velvet with higher and higher sequencing_errors however similar behaviors to the assembly of real_data can be observed for idba ud and velvet n and longest contig rapidly decrease and missing portions of the bac and number of mis assemblies increase surprisingly spades seems to be immune to higher sequencing_error because the introduction of dna_sequencing in the s scientists had to come up with clever solutions to deal with the problem ofde novo genome_assembly with limited depth of sequencing as the cost of sequencing keeps decreasing one can expect that computational_biologists will have to deal with the opposite problem excessive amount of sequencing_data the lander waterman roach theory has been the theoretical_foundation to estimate gap and contig lengths as a function of the depth of sequencing we do not have a theory that would explain why the quality of the assembly starts degrading when the depth is too high possible factors include the presence in real_data of chimeric reads sequencing_errors and read duplications or their combination thereof in this study we report on the de_novo of bac clones which are relatively short dna_fragments kbp with current sequencing_technology it is very easy to reach depth of sequencing in the range of and study how the assembly_quality changes as the amount of sequencing_data increases our experiments show that when the depth of sequencing exceeds a threshold the overall quality of the assembly starts degrading this appears to be a common problem for several de_novo assemblers the same behavior is observed for the problem of decoding reads to their source bac which is the main focus of this article the important question is how to deal with the problem of excessive sequencing_depth for the decoding problem we have presented an effective divide_and solution we slice the data in subsamples decode each slice independently then merge the results in order to handle conflicts in the bac assignments i e reads that appear in multiple slices that are decoded to different sets of bacs we devised a simple set of voting rules the question that is still open is what to do for the assembly problem one could assemble slices of the data independently but it is not clear how to merge the resulting assemblies in general we believe that the problem of de_novo sequence_assembly must be revisited from the ground up under the assumption of ultra deep coverage 
