bamhash a checksum program for verifying the integrity of sequence_data large resequencing projects require a significant amount of storage for raw sequences as well as alignment files because the raw sequences are redundant once the alignment has been generated it is possible to keep only the alignment files we present bamhash a checksum based method to ensure that the read_pairs in fastq_files match exactly the read_pairs stored in bam_files regardless of the ordering of reads bamhash can be used to verify the integrity of the files stored and discover any discrepancies thus bamhash can be used to determine if it is safe to delete the fastq_files storing raw sequencing read after alignment without the loss of data resequencing projects where individuals are sequenced from a species with a known reference_genome generate a significant amount of raw sequences that are then aligned to the reference_genome data storage becomes an issue as the cost of sequencing decreases and the throughput of current_sequencing keeps increasing raw sequencing_reads are generally stored in fastq file_format usually compressed after read_mapping the resulting alignment is stored in a bam_file this bam_file is then sorted and processed further but most importantly it contains all the original information of the fastq file sorted bam_files yield a better compression compared with unsorted bam_files as well as allowing random lookup over genomic_regions for this reason almost all post alignment analysis e g variant_calling realignment and local_assembly are done on the sorted bam_file rather than the original fastq file because the bam_file contains all the information of the fastq file it is justifiable to delete the fastq file after alignment after all the contents of the fastq file can be regenerated from bam_file however before deleting the fastq file we need to be sure that there is no loss of data i e that the sequences in the fastq file are exactly the same as the sequences in the bam_file the two files could differ due to a number of reasons any errors in the alignment pipeline could generate inconsistent files although the alignment pipelines are based on well tested tools they are meant to operate under normal conditions and their behavior can be unpredictable in the presence of hardware failure or running out of disk_space thus it is important to be able to independently verify the output of the entire pipeline we present bamhash a tool for verifying the data integrity between a fastq and a bam_file the program computes a bit fingerprint from the sequences and read names for both fastq and bam_files the method is highly_sensitive to changes in the input so a change in a single nucleotide will result in different fingerprints the probability of generating the same fingerprint by chance is astronomically small the role of this tool is to flag any fastq and bam_files that have different fingerprints and mark the fastq_files as unsafe for deletion bamhash plays the same role as the md sum program which computes a fingerprint of files comparing md sum fingerprints of fastq and bam_files would not yield a comparable result since the formatting and ordering are different our method is fast and memory_efficient it can compute the fingerprint of a bam_file from fold coverage human sequencing_experiment in min to assess the performance of bamhash we compared the running time for processing bam_files to viewing with samtools the dataset chosen was a whole_genome sequencing_experiment aligned to grch human reference using bwa mem all datasets were generated at the laboratory at decode genetics and were processed with the same pipeline the bam_file consists of million read_pairs at coverage bamhash required min to compute the hash values whereas samtools required min to parse the bam_file and count lines we note that the program is largely i o bound it runs on a single core which is underutilized as most of the time is spent waiting for data from the disk the role of bamhash is to detect differences between the read sets of raw fastq and aligned bam_files this discrepancy can arise due to mistakes in the pipeline bugs in alignment code or disk failures when the data integrity has been verified the original fastq_files can be safely discarded thus freeing up storage space additionally bamhash will be useful when porting alignments to a new reference_genome such a pipeline would create intermediate fastq_files which would then be aligned to the new reference the old bam_file can be removed only if the bamhash signature agrees with the newly created alignment bamhash can only detect differences between exact_matches of set of reads not how they differ in many scenarios low quality reads are discarded before alignment or reads that do not map are discarded from the bam_file in this case the set of reads in the final bam_file is a subset of the original set of reads unfortunately no fingerprinting method can detect if the bam reads are a subset of the fastq reads this is because fingerprinting is a restricted form of communication between two parties the bam hasher and the fastq hasher and lower bounds on the communication complexity of the set disjointness problem dictate a lower_bound of xn bits of communication to simply answer the question of whether two sets are disjoint namely the set of bam reads and the complement of the set of fastq reads 
