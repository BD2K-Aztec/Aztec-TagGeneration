genome_analysis musket a multistage k_mer spectrum based error corrector for illumina sequence_data motivation the imperfect sequence_data produced by next_generation have motivated the development of a number of short_read error correctors in recent_years the majority of methods focus on the correction of substitution_errors which are the dominant error source in data produced by illumina sequencing_technology existing_tools either score high in terms of recall or precision but not consistently_high in terms of both measures results in this article we present musket an efficient multistage k_mer based corrector for illumina short_read we use the k_mer spectrum approach and introduce three correction techniques in a multistage workflow two sided conservative correction one sided aggressive correction and voting based refinement our performance evaluation results in terms of correction quality and de_novo measures reveal that musket is consistently one of the top performing correctors in addition musket is multi_threaded using a masterslave model and demonstrates superior parallel scalability compared with all other evaluated correctors as well as a highly_competitive overall execution time availability musket is available at http musket the emergence and rapid progress of next_generation ngs_technologies has enabled the high_throughput production of short dna_sequences reads at low_cost the ever increasing throughput and decreasing cost has significantly altered the landscape of whole_genome providing an opportunity for scientists to initiate whole_genome sequencing_projects for almost any organism including those whose genomes span billions of base_pairs such as the giant panda and humans many ngs sequencing_technologies have been developed among which illumina is most widely used however reads produced from ngs_platforms are never perfect and can contain various types of sequencing_errors including substitutions and indels insertions_or these sequencing_errors complicate data_processing for many biological_applications such as de_novo and short_read to improve data_quality one frequently used approach is to trim reads from the more error_prone ends which unfortunately results in loss of information this has ignited the interest of researchers in conceiving more sophisticated algorithms to detect and correct sequencing_errors in ngs_data for example schroder as substitution is the dominant error type for data produced by illumina sequencing_technology most approaches focus on correcting this type of errors the core of substitution error based_methods is to compute consensus bases using the highly redundant coverage_information when a sequencing_error occurs in a read that originated from a certain position on the genome all reads covering the erroneous position could be piled up to compute the consensus base considering that sequencing_errors are generally random and infrequent this consensus base is likely to be correct however as we assume that the source genome is unknown beforehand we can neither determine the read locations on the genome nor the correctness of reads directly instead reads that cover overlapping genomic positions can be inferred by assuming that they typically share_common substrings furthermore we can approximate the source genome using a k_mer spectrum which was first introduced by given a dataset with sufficient coverage of a genome the k_mer spectrum is defined as the set of all k_mers in the dataset where the k_mers whose multiplicity exceeds a coverage cut_off are deemed to be trusted and otherwise untrusted the first k_mer spectrum based corrector was proposed by using an iterative spectral alignment problem approach cuda ec and decgpu accelerated this spectral alignment problem approach using graphics_processing computing to further improve correcting quality introduced a dynamic_programming that corrects errors by minimizing edit distances the soap corrector adopted a similar approach quake introduced a probabilistic_model derived from base quality_scores of reads for a k_mer quake accumulates the correctness probability of all its occurrences and defines this sum as the multiplicity of the k_mer instead of just the number of occurrences reptile relies on a hamming graph to resolve ambiguities for a tile based correction whereas hammer combines the hamming graph with a probabilistic_model to deal with datasets with non uniform coverage hitec supports multiple k_mer sizes in a single launch to whom correspondence should be addressed and relies on witness clusters that are constructed from suffix_arrays sga uses a memoryefficient burrows_wheeler transform and an fm_index to represent the k_mer spectrum in addition to k_mer spectrum based_approaches some correctors based on other techniques have been developed shrec uses a generalized suffix trie to detect and correct substitution_errors which is further extended by hybrid shrec to additionally correct indels coral and echo are two correctors that use the concept of multiple_sequence using k_mers as seeds coral corrects errors by constructing consensus_sequences from multiple_alignments and echo by computing consensus bases using a maximum a posterior estimate over position specific substitution matrices in this article we present musket multistage k_mer spectrumbased corrector an efficient substitution error based corrector for illumina sequence_data based on a k_mer spectrum approach we introduce three techniques namely two sided conservative correction one sided aggressive correction and voting based refinement to form a multistage correction workflow the performance of musket is evaluated using both simulated_and for short_read originating from small_sized escherichia_coli medium_sized human_chromosome and large_sized human_chromosome genomes in terms of correction quality musket is consistently one of the top performing correctors compared with hitec sga shrec coral quake reptile and decgpu in terms of de_novo using the sga assembler musket yields better performance than all other evaluated correctors with respect to some commonly used metrics in addition musket provides support for multithreading using a masterslave model and demonstrates superior scalability on a shared memory multi cpu workstation as well as highly_competitive overall execution speed we have evaluated the performance of musket using both simulated and real short_read datasets from the following three perspectives i correction quality ii impact on de_novo genome assembly_quality and iii speed parallel scalability and memory_consumption performance is compared with several publicly_available correctors hitec v sga v shrec v coral v quake v reptile v and decgpu v for all correctors we have used the default_settings and disabled read trimming discarding for coral we have disabled the correction of indels as quake sometimes exceptionally exits we have manually run each step of quake in this article all tests are conducted on a workstation with six core intel_xeon x ghz cpus and gb random_access running linux ubuntu lts we have presented musket an efficient substitution error based error corrector for short dna_reads produced by illumina sequencing_technology this reference free error corrector uses the k_mer spectrum approach and aims at correcting as many errors as possible while introducing few new errors three correction techniques including two sided conservative correction one sided aggressive correction and voting based refinement have been introduced to form a multistage correction workflow we have assessed the performance of musket in comparison with several established error correctors hitec sga shrec coral quake reptile and decgpu the assessment is conducted using both simulated and real reads in terms of correction quality and de_novo measures in terms of correction quality musket is consistently one of the top performing correctors for reads simulated from the small_sized e coli_genome as well as large_sized human_chromosomes the best performance of musket is obtained using simulated reads from the human_chromosomes which suggests that musket can perform well on genomic_sequences with complex repeat structures in terms of de_novo musket is superior to all other evaluated correctors in terms of the following metrics n contig size error corrected n contig size number of contig errors and genome_coverage through this study we have found that pre assembly error_correction does not always guarantee to yield better assembly_quality in terms of all metrics but have demonstrated the capability of improving contig contiguities and reducing mis assemblies besides de_novo short_read error correctors can also be used before short_read as many state of the art short_read tolerate multiple sequencing_errors in the full read_length substitution error based error correctors might not be able to make significant impact on the improvement of alignment performance hence the use of musket might be more valuable before de_novo than short_read in addition musket uses multi threading based on a master slave model to leverage the compute power of common sharedmemory multi cpu platforms this approach results in superior parallel scalability compared with all other evaluated correctors in terms of the number of cpu threads as well as in overall execution time all existing standalone correctors target haploid genome_sequencing showed that error_correction can benefit single_nucleotide calling in haploid genomes however the effect of error_correction on diploid genomes has not been explored yet a heterozygous site results in a percentage of k_mers supporting alternative alleles in a moderate or high coverage dataset we would expect these k_mers to be within the distribution of trusted k_mers however this might not be the case when the coverage is low or allelic distribution is imbalanced in such cases diploid single_nucleotide could be lost as a result of false_positive corrections up to date almost all correctors based on k_mer spectrum use a single coverage cut_off to differentiate trusted k_mers from the untrusted ones this static coverage cut_off ignores the confidence difference in the correctness of bases that is represented as base quality_scores in ngs_reads hence a dynamic coverage cut_off as in coral salmela and schroderschroder might be advantageous to further improve correction quality of correctors based on k_mer spectrums a possible solution might be the use of a position specific cut_off matrix derived from base quality_scores for a single_read conflicts of interest none declared 
