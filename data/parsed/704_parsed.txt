data_and beetl fastq a searchable compressed archive for dna_reads motivation fastq is a standard file_format for dna sequencing_data which stores both nucleotides and quality_scores a typical sequen_cing study can easily generate hundreds of gigabytes of fastq_files while public archives such as ena and ncbi and large international collaborations such as the cancer_genome can accumulate many terabytes of data in this format compression tools such as gzip are often used to reduce the storage burden but have the disadvantage that the data must be decompressed before they can be used here we present beetl fastq a tool that not only compresses fastq formatted dna_reads more compactly than gzip but also permits rapid search for k_mer queries within the archived sequences importantly the full fastq record of each matching read or read_pair is returned allowing the search results to be piped directly to any of the many standard_tools that accept fastq data as input results we show that terabytes of human reads in fastq format can be transformed into terabytes of indexed files from where we can search for and a million of mers in and s respectively plus ms per output read useful applications of the search capability are highlighted including the genotyping of structural variant breakpoints and in silico pull_down experiments in which only the reads that cover a region of interest are selectively extracted for the purposes of variant_calling or visualization availability_and beetl fastq is part of the beetl library available as a github repository atmuch has been written about disruptive changes in dna_sequencing over the last decade and the need for compact ways to store the vast datasets that these new technologies have facilitated the raw_reads in a sequencing project are commonly stored in an ascii based format called fastq the entry for each read comprising a read id string that holds free_text metadata associated with the read a string for the sequence itself and a string of quality_scores that encodes accuracy estimates for each base the general_purpose compression tool gzip www gzip org jean loup gailly and mark adler is free and widely available and is hence an appealing option for compressing fastq_files many bioinformatics_tools can read gzip compressed data directly via the api provided by the zlib library www zlib net jean loup gailly and mark adler which avoids the need to create the uncompressed file as an intermediate but nevertheless still incurs the computational_overhead of decompressing the entire file which is considerable for the large file sizes we are typically dealing with for many applications we would like random_access to the data without the need to decompress the file in its entirety this need was recognized by the authors of the samtools package which features two programs razip a tool available from razip sourceforge net appears to have similar aims to the eponymous samtools program but seems to be an entirely separate development we deal exclusively with samtools razip here and bgzip that take a blockcompressed approach to random_access while retaining some degree of backward compatibility with gzip the data are divided_into contiguous blocks that are compressed individually allowing decompression to commence from any point in the file while limiting the overhead to the need to decompress the data that precede that point within its block given this functionality it is simple to index a set of records by some key of interest by sorting them in order of that key blockcompressing them and retaining the mapping from key value to file offset for a subsampling of the records several of samtools user level tools work in this way e g tabix and indexed bam_files are both indexed by genomic coordinate whereas faidx uses the name of the sequence as a key however we wish to search for any substring within the sequences which is not possible with a key based indexing strategy instead we use an approach based on the_burrows or bwt the bwt is a reversible transformation of a string that acts as a compression booster permuting the symbols of the string in a way that tends to enable other text compression techniques to work more effectively when subsequently applied when the bwt transformed string is decompressed again the reversible nature of the transform allows the original string to be recovered from it although it was originally developed with compression in mind showed that in combination with some relatively small additional data_structures the compressed bwt can act as a compressed index that facilitates rapid search within the original string this concept has been highly influential in bioinformatics being the means by which bwt based aligners such as bwa and bowtie accelerate searches against a reference_genome the core idea is that exact occurrences of some query within the original string can be found by applying a recursive backward search_procedure to its bwt to whom correspondence should be addressed having found some exact_matches to our query within the reads in this way we continue the recursion to extend these hits into the entire sequences of the reads that contain them once the extension reaches the boundary of a read a lookup into an additional table allows the original positions of the reads in the fastq file to be deduced this last piece of information enables the quality_score and read id strings to be extracted from razip compressed files that have been indexed using their ordering within the original fastq file as a key specifically given a query dna_sequence q our tool can provide in increasing order of computational_overhead the number of occurrences of q in the reads the full sequence of each of the reads that contain q the quality_score strings associated with the sequences that contain q the read ids of the reads whose sequences contain q for paired_read data the read ids sequences and quality_scores of the reads that are paired with the sequences that contain q three potential applications demonstrate the usefulness of this fast k_mer search an in silico pull_down experiment in which only the reads that cover a region of interest are selectively extracted for the purpose of variant_calling or visualization a de_novo of reads from insertion breakpoints and the genotyping of structural_variants by means of tracking the k_mers that overlap their breakpoints our focus here has been to represent the input_data in a lossless fashion buthighlights that the relative storage needs of the sequences quality_scores and read ids perhaps do not reflect their relative importance as the sequences and their indexes take up only of the total size barely more than the consumed by the read ids there remains some scope for further lossless_compression in earlier work that focused exclusively on compression we achieved sub bpb on the sequences using zip www zip org igor pavlov here however we sacrifice some compression for the faster decompression and thus faster search that our simple byte code format gives us the byte coding can be further optimized and it may also be advantageous to switch between run length encoding and naive bits_per encoding on a per block basis choosing whichever of the two strategies best compresses each block however to achieve significant further compression some degree of lossy_compression is likely to be necessary each of the three data streams is potentially amenable to this and our methods can of course still be applied to the resulting data the free format of the read ids limits our ability to comment generally on the prospects of compressing such data further nevertheless it could be argued that for paired data the most useful metadata to retain is which read is paired with which this could be simply encapsulated in an array containing one pointer per read consuming ologn bits the space taken up by the sequences themselves would be reduced by error_correction two possible strategies being the trimming of low quality read ends as demonstrated by or a k_mer based_approach such as musket lastly the majority of the archives size is taken up by the quality_scores more recent illumina data default to a reducedrepresentation quality scoring_scheme that makes use of only of the or so possible quality values but the pg data we tested still follow the full resolution scoring_scheme the newer scheme would likely reduce the size of the scores by about half we have also described a complementary_approach that uses the k_mer context adjacent to a given base to decide whether its quality_score can be discarded without likely detriment to variant_calling accuracy conflicts of interest all authors are employees of illumina inc a public company that develops and markets systems for genetic_analysis and receive shares as part of their compensation 
