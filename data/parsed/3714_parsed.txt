probabilistic suffix_array efficient modeling and prediction of protein_families motivation markov_models are very popular for analyzing complex sequences such as protein_sequences whose sources are unknown or whose underlying statistical characteristics are not well understood a major_problem is the computational_complexity involved with using markov_models especially the exponential_growth of their size with the order of the model the probabilistic suffix_tree pst and its improved variant sparse probabilistic suffix_tree spst have been proposed to address some of the key_problems with markov_models the use of the suffix_tree however implies that the space requirement for the pst spst could still be high results we present the probabilistic suffix_array psa a data_structure for representing information in variable_length markov_chains the psa essentially encodes information in a markov_model by providing a time and space efficient alternative to the pst spst given a sequence of length n construction and learning in the psa is done in o n time and space independent of the markov order prediction using the psa is performed in o mlog n time where m is the pattern length and is the symbol alphabet in terms of modeling and prediction_accuracy using protein_families from pfam spst and psa produced similar results spst psa but slightly_lower than hmmer a modified algorithm for psa prediction improved the performance to or just from hmmer results the average_maximum practical construction space for the protein_families tested was n n bytes using the psa n n bytes using spst and n n bytes for hmmer the psa was times_faster to construct than the spst and times_faster than hmmer markov_models are often used for modeling complex sequences such as protein_sequences whose underlying statistical characteristics are not well understood this is especially the to whom correspondence should be addressed case when the sequences exhibit short_term for a shortterm memory of length say l the sequences can be modeled using markov_models of order l or using hidden_markov the models provide efficient mechanisms to compute the required conditional_probabilities and also for generating sequences from the models the problem is that the size of markov_models increases exponentially with increasing memory length l thus they are practical only for low order models with short memory lengths this leads to another problem such low order markov_models often provide a poor approximation of the true sequence being modeled it is known that learning and inference with hmms is computationally very challenging probabilistic suffix models such as probabilistic suffix_trees psts have been proposed byto address some of the key_problems with markov_models they showed the equivalence between psts and a subclass of probabilistic finite automata pfa called probabilistic suffix automata pfa psa for a given pst there is an algorithm to construct a pfa psa whose size is the same as that of the pst within a constant factor further the distribution generated by a pst is guaranteed to be within a small distance from that generated using the psf psa as measured by the kullbackleibler divergence their probabilistic suffix models however require o ln time and space to construct where n is the sequence_length the algorithm to construct the pfa psa from the pst also runs in o ln time later showed how the pst can be constructed in o n time independent of the order l using traditional suffix links used in constructing suffix_trees sts and the notion of reverse suffix links psts and probabilistic suffix automatons are related to contextbased models which are extensively used in sequence prediction and data compression the use of these context models as a surrogate for variable_length markov_models with applications in sequence prediction are reviewed in earlier the pst was used to model dna_sequences in modeling and prediction of protein_families using the pst was reported in importantly using the pst prediction was performed without any prior alignment of the protein_sequences proposed an improved variation of the pst called sparse probabilistic suffix_tree spst for use in protein_family prediction unlike the usual pst the spst allows some contexts to be grouped together to form an equivalent class essentially this grouping results in a form of pruning of the pst however the node depths are not fixed but could vary dependingthe proposed data_structure is based on the standard sa and hence can be implemented using compressed index structures such as compressed suffix_arrays csas showed that for a general alphabet with the csa can be constructed in o n log n processing time and stored in loglog n n log n o n loglogn bits such that each lookup operation can be performed in o loglog n time similarly in theory the pst or spst can be implemented using compressed suffix_trees csts showed that a cst with full functionality including suffix links can be constructed in o n time and represented using o n log bits of storage space he showed that the cst can be implemented using nh h n o n bits where h h is the order h entropy of the original sequence for the csa the storage requirement can be reduced to nh h o n loglogn log n bits these will no doubt result in a lower memory requirement when compared with our current result using standard sas however apart from the usual slowdown in processing_speed using csas or csts some types of traversals and new attributes required for the psa or pst spst could be quite difficult to implement on a standard csa or cst this type of improvement will be an interesting direction for further work on space efficient probabilistic suffix structures the proposed methodindicated results are true_positive rates in percentage for psa w using average probability and w using maximum probability is not directly_affected by the computer word length bit or bit so_far as the data_structure including input_data can fit into main_memory for very large_datasets where this may not be the case methods will need to be considered for more efficient i o operations ferragina in terms of prediction_accuracy a potential approach to improve the performance will be to consider prediction based on short fragments of the test sequence these will provide more locality in the analysis and hence may be able to improve the ability for detecting distant homology one way to use the predictions from different fragments will be to choose the one with the family with the maximum predicted probability over all the fragments as the predicted family however other approaches are also possible for instance by combining the predictions using some well defined protocol we tested this approach using overlapping windows of various sizes w the best results were obtained at w using average of window probabilities and at w using the maximum probability provides a summary of the results showing that with the improvement the average psa prediction_accuracy is now below that of hmmer detailed results are in supplementary table s further as can be seen from the results supplementary tables s and s there are cases where the psa performed signifcantly better than hmm e g by as much as for fad binding or for fa hydroxylase thus a potential improvement could be obtained by combining prediction results from the psa with those from non pst based_methods such as the profile hmm for instance using an approriate classifier fusion scheme we acknowledge some basic problems with using vlmms in general whether pst spst or psa one problem is that these tend to provide a global characterization of the sequence and hence may have problems when similarity between sequences is localized within a smaller region of the proteins or is interspersed between islands of non similar regions this will thus affect the ability to detect distant homology or the ability to handle a previously_unseen mutation in a protein_sequence that is being analyzed this last problem is akin to the zero frequency problem and various methods have been proposed for instance using a default probability_distribution or a poisson_process model a related problem with the current psa implementation is that the conditioning context for the symbol probability reverts to the longest observed suffix meaning that the conditional_probability would be based on a small number of counts this may not always correspond to the true probability one solution would be to use some kind ofwe have presented the psa a data_structure for representing information in vlmms the psa provides the same functionality as the pst but at a significantly_reduced time and space requirement given a sequence of length n construction and learning in the psa is done in o n time and o n space independent of the markov order prediction using the psa is performed in o mlog n time where m is the pattern length and is the symbol alphabet we have shown practical results on the comparative performance of psa spst and hmmer using protein_families from pfam in terms of modeling and prediction spst and psa produce equivalent results spst psa these were slightly_lower than the obtained using hmmer on the same dataset an improved version of psa prediction predicting families using fragments from the test sequence improved psa perfomance to thus reducing the performance gap with hmmer to just the average practical construction space for the protein_families tested was n bytes using the psa n bytes using spst and n bytes for hmmer the maximum practical space needed was n for psa n for spst and n for hmmer with respect to construction time the psa was times_faster than spst and times_faster than hmmer our results show that the psa provides an accuracy similar to that of pst spst and hmmer in protein_family prediction but at a significantly_lower time and space requirement 
