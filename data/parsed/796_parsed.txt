bless bloom filter_based error_correction solution for high_throughput sequencing_reads motivation rapid advances in next_generation ngs_technology have led to exponential_increase in the amount of genomic_information however ngs_reads contain far more errors than data from traditional sequencing methods and downstream genomic analysis results can be improved by correcting the errors unfortunately all the previous error_correction methods required a large amount of memory making it unsuitable to process reads from large_genomes with commodity computers results we present a novel algorithm that produces accurate correction results with much less memory compared with previous solutions the algorithm named bloom filter_based error_correction solution for high_throughput sequencing_reads bless uses a single minimum sized bloom_filter and is also able to tolerate a higher false_positive thus allowing us to correct errors with a memory_usage reduction on average compared with previous methods meanwhile bless can extend reads like dna assemblers to correct errors at the end of reads evaluations using real and simulated reads showed that bless could generate more accurate results than existing solutions after errors were corrected using bless of initially unaligned reads could be aligned correctly additionally de_novo results became longer with fewer assembly errors recent_advances ngs_technologies have made it possible to rapidly generate high_throughput data at a much lower cost than traditional_sanger technology ngs_technologies enable cost efficient genomic applications including de_novo of many non model_organisms identifying functional_elements in genomes and finding variations within a population in addition to short read_length a main_challenge in analyzing ngs_data is its higher_error than traditional sequencing_technology and it has been demonstrated that error_correction can improve the quality of genome_assembly and population_genomics analysis previous error_correction methods can be divided_into four major categories i k_mer spectrum based ii suffix_tree array based b iii multiple_sequence msa based and iv hidden_markov hmm_based however none of these previous methods has successfully corrected errors in ngs_reads from large_genomes without consuming a large amount of memory that is not accessible to most researchers see detailed discussions in the supplementary methods previous evaluations showed that some error_correction tools require gigabyte gb of memory to correct errors in genomes with mb and the others need tens of gb of memory for a human_genome previous_approaches would need hundreds of gb of memory even if a computer with hundreds of gb of memory is available running such memoryhungry tools degrades the efficiency of the computer while the error_correction tool runs we cannot do any other job using the computer if most of the memory is occupied by the error_correction tool this can be a critical problem for data centers where a large amount of data should be processed in parallel in several works bloom filters or counting bloom filters were used to save a k_mer spectrum which includes all the strings of length k i e k_mers that exist more than a certain number of times in reads b although bloom_filter is a memory_efficient data_structure the memory reduction by previous bloom_filter based_methods did not reach their maximum potential because of the following four reasons i the size of a bloom_filter should be proportional to the number of distinct k_mers in reads and the number of distinct k_mers was conservatively estimated thus could be much higher than the actual number ii they could not remove the effect of false_positives from bloom filters to make the false_positive of the bloom filters small the size of bloom filters were made large iii because they could not distinguish error_free k_mers from erroneous ones before a bloom_filter was constructed both of the k_mers needed to be saved in bloom filters iv multiple bloom filters or counting bloom filters were needed to count the multiplicity of each k_mer besides the large memory_consumption of the existing_methods another problem encountered during the error_correction process is that there exist many identical or very similar subsequences in a genome i e repeats because of these repeats an erroneous subsequence can sometimes be converted to multiple error_free subsequences making_it to determine the right choice in this article we present a new bloom filter_based error_correction algorithm called bless bless belongs to the k_mer spectrum based method but it is designed to remove the aforementioned limitations that previous k_mer spectrum based_solutions had our new approach has three important new features bless is designed to target high memory_efficiency for error_correction to be run on a commodity computer the k_mers that exist more than a certain number of times in reads are sorted out and programmed into a bloom_filter bless can handle repeats in genomes better than previous k_mer spectrum based_methods which leads to higher accuracy this is because bless is able to use longer k_mers compared with previous methods longer k_mers resolve repeats better bless can extend reads to correct errors at the end of reads as accurately as other parts of the reads sometimes an erroneous k_mer may be identified as an error_free one because of an irregularly large multiplicity of the k_mer false_positives from the bloom_filter can also cause the same problem bless extends the reads to find multiple k_mers that cover the erroneous bases at the end of the reads to improve error_correction at the end of the reads to identify erroneous k_mers in reads we need to count the multiplicity of each k_mer counting k_mers without extensive memory is challenging bless uses the disk based k_mer algorithm like disk streaming of k_mers dsk and k_mer counter kmc however bless needs to save only half of the k_mers that dsk does in hash tables because it does not distinguish a kmer and its reverse complement to evaluate the performance of bless this study used real ngs_reads generated with the illumina technology as well as simulated reads these reads were corrected using bless as well as six previously_published our results show that the accuracy of bless is the best while it only consumes of the memory_usage of all the compared methods on average our results further show that correcting errors using bless allowed us to align of previously unaligned reads to the reference_genome accurately bless also increased ng of scaffolds by and decreased assembly errors by based on the results from velvet to assess the performance of bless we corrected errors in five different read sets from various genomes using bless and six other error_correction methods all the evaluations were done on a server with two intel_xeon x ghz processors gb of memory and scientific linux the version and parameters of all tools used in the experiments can be found in the supplementary document current ngs_technologies produce errors in reads which can influence the quality of downstream_analysis many methods have been developed to correct such sequencing_errors however most previous methods cannot correct errors in large_genomes even if a few methods succeeded in correcting large_genomes their outputs still contain many uncorrected errors in addition the memory requirement for the existing_tools has been still too large for most researchers who might only have access to computers with a moderate amount of memory in this work we present a novel error_correction algorithm for ngs_reads called bless which has two novel features i bless consumes much less memory than previous methods bless can sort out minimum k_mers needed to correct errors and program the k_mers in a minimum sized bloom_filter this makes bless consume much less memory than any other error correction_method including previous bloom filter_based ones ii bless also generates more accurate results for reads from genomes with many short repeats this is mainly because bless is not limited by the choices of the length of the k_mer see software options in the supplementary document for details while the maximum k is usually in other methods bless is able to remove ambiguities in the error_correction process by choosing large_numbers for k without increasing memory_usage furthermore bless is efficient at correcting errors close to the ends of the reads bless corrects errors at the ends of the reads by extending the ends bless was compared with previous top performers using real and simulated reads the experimental_results showed that bless generated more accurate results than previous_algorithms while consuming only of the memory_usage of the compared methods on average moreover running bless improved the length and accuracy of de_novo results for all the three widely used assemblers velvet soapdenovo and sga bless also made of unaligned reads exactly aligned to reference_sequences as pointed_out before bless can choose large values for k the only drawback to large k values is that the average multiplicity of such k_mers drops if the average multiplicity of k_mers is too low we cannot precisely distinguish erroneous k_mers using their multiplicity nevertheless it is important to note thatbecause the dna_sequencing cost keeps dropping and the throughput keeps increasing we expect to solve this problem by increasing the depth of reads the memory_usage of bless is proportional only to the number of solid k_mers because the solid k_mers eventually represent the k_mers that exist in the genome_sequence the number of solid k_mers remains_constant even as the number of input reads escalates therefore memory_consumption of bless will not increase even when read_depth increases as shown in supplementary table s there are two future avenues to pursue first although the runtime of bless is already competitive as shown in supporting multiple threads will improve blesss runtime further blesss wall clock time decomposition for d is depicted in supplementary most of the time is spent counting the number of distinct solid k_mers and correcting errors the runtime of both processes can be improved through parallelization in the counting step k_mers are distributed into n files and each file is processed in succession this step can be parallelized without degrading memory_usage the error_correction process of a read is independent of other reads and therefore the error_correction part can be easily parallelized second developing a method to automatically choose k may be added recent work showed that a k_mer multiplicity histogram can be made in a short time by sampling reads and an optimal k value can be found using the histograms this sampling based_approach will also work for bless it will be helpful to reduce the runtime because it can prevent users from running bless multiple times with different k values 
