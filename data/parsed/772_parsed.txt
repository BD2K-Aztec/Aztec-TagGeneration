sequence_analysis biopig a hadoop based analytic toolkit for large_scale sequence_data motivation the recent revolution in sequencing_technologies has led to an exponential_growth of sequence_data as a result most of the current bioinformatics_tools become obsolete as they fail to scale with data to tackle this data deluge here we introduce the biopig sequence_analysis toolkit as one of the solutions that scale to data and computation results we built biopig on the apaches hadoop mapreduce system and the pig data flow language compared with traditional serial and mpi based_algorithms biopig has three major_advantages first biopigs programmability greatly_reduces development time for parallel bioinformatics applications second testing biopig with up to gb sequences demonstrates that it scales automatically with size of data and finally biopig can be ported without modification on many hadoop infrastructures as tested with magellan system at national energy research scientific computing center and the amazon elastic compute cloud in summary biopig represents a novel program framework with the potential to greatly accelerate data intensive bioinformatics_analysis availability_and biopig is released as open_source under the bsd_license at https sites google com a lbl gov biopig contact zhongwang lbl govadvances in dna_sequencing are enabling new applications ranging from personalized_medicine to biofuel development to environmental sampling historically the bottleneck for such applications has been the cost of sequencingthe estimated_cost of sequencing the first human_genome gb completed a decade_ago is estimated at billion http www genome gov current next_generation have extraordinary throughput at a much lower cost thereby greatly reducing the cost of sequencing a human_genome to http www genome gov sequencingcosts accessed december the price hit by the end of http dnadtc com products aspx accessed december with the cost of sequencing rapidly dropping extremely large_scale sequencing_projects are emerging such as the genomes_project that aims to study the genomic_variation among large populations of humans genomes and the cow rumen deep metagenomes project that aims to discover new biomass degrading_enzymes encoded by complex microbial_community as a result the rate of growth of sequence_data is now outpacing the underlying advances in storage technologies and compute technologies moores law taking metagenomic_studies as an example data have grown from mb million bases from termite hindgut and mb tamar wallaby to gb billion bases from cow rumen microbiome within the past_years the recently_published doe joint genome institutes jgi sequencing productivity showed that tb trillion bases of sequences were generated in alone http usa gov jgi progress data analysis at terabase scales requires state of the art parallel_computing strategies that are capable of distributing the analysis across thousands of computing elements to achieve scalability and performance most of the current bioinformatics_analysis tools however do not support parallelization as a result this exponential data growth has made most of the current bioinformatics analytic tools obsolete because they fail to scale with data either by taking too much time or too much memory for example it would take cpu years to blast the gb cow rumen metagenome data against ncbi non redundant database de_novo of the full dataset by a short_read assembler such as velvet would require computers with tb ram and take several weeks to complete re engineering the current bioinformatics_tools to fit into parallel programming models requires software engineers with expertise in high_performance and parallel algorithms take significantly_longer time to develop than serial algorithms in addition at this scale of computing hardware failures become more frequent and most bioinformatics software lack robustness so that once they fail they have to be restarted manually all these challenges contribute to the bottleneck in large_scale sequencing analysis cloud_computing has emerged recently as an effective technology to process petabytes of data per day at large internet companies mapreduce is a data parallel framework popularized by google inc to process petabytes of data using large_numbers of commodity servers and disks hadoop is an open_source implementation of the mapreduce framework and is available through the apache software foundation http wiki apache org hadoop hadoop uses a to whom correspondence should be addressed y present_address amazon_web new_york ny usa distributed file system hdfs that brings computation to the data as opposed to moving the data to the computation as is done in traditional computing paradigms in hadoop node tonode data transfers are minimized as hadoop tries its best to perform automatic co location of data and program on the same node furthermore hadoop provides robustness through a job handling system that can automatically restart failed jobs because of these advantages hadoop has been explored by the bioinformatics community in several areas including blast snp discovery short_read and transcriptome_analysis in addition there are solutions that reduce the hurdle to run hadoop based sequence_analysis applications using hadoop requires a good understanding of this framework however to break up programs into map and reduce steps skills in programming_languages such as java or c are also required to overcome the programmability limitations of hadoop several strategies have been developed for example cascading is an application framework for java developers by concurrent inc http www cascading org which simplifies the processes to build data flows on hadoop apaches pig data flow language was developed to enable non programmer data analysts to develop and run hadoop programs http pig apache org somewhat similar in nature to sql the pig language provides primitives for loading filtering and performing basic calculations over datasets pigs infrastructure layer consists of a compiler on the users client machine that turns the users pig latin programs into sequences of mapreduce programs that run in parallel on the nodes of the hadoop cluster in a similar fashion to how a database engine generates a logical and physical execution plan for sql queries in this article we describe biopig a set of extensions to the pig language to support large sequence_analysis tasks we will discuss the design principles give examples on use of this toolkit for specific sequence_analysis tasks and compare its performance with alternative solutions on different platforms there is a similar framework seqpig http seqpig sourceforge net developed in parallel to biopig which is also based on hadoop and pig we provide a detailed comparison of the two in section in this work we present a solution for improved processing of large sequence_datasets in many bioinformatics applications using only a few core modules we believe we have demonstrated the usefulness of this toolkit while its modular design should enable many similar applications that fit in the mapreduce framework biopig has several advantages over alternative parallel programming paradigm it is easy to program it is scalable to process large_datasets with gb being the largest one tested and it is generically portable to several examples of hadoop infrastructure including amazon ec without modification we also noticed several limitations of biopig most of which likely derived from hadoop itself for example it is slower than handcrafted mpi solutions this is due to both the latency of hadoops initialization and the fact that generic mapreduce algorithms are not optimized for specific problems for big datasets this limitation may not be a problem as the time spent on data analysis far exceeds the cost for the start up latency recently the hadoop community has started to address this problem certain commercial implementations of mapreduce such as ibms symphony product have been developed to reduce hadoops start up latency another promising solution is spark which can speed up hadoop applications times by using a low latency in memory cluster computing another issue is computing resource demand when dealing with huge datasets biopig shifts the need from expensive resources such as large memory tb ram machines and or parallel programming expertise to large disk_space on commodity hardware for example a kmer read index in biopig needs 
