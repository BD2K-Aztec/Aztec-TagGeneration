genome_analysis densitycut an efficient and versatile topological approach for automatic clustering of biological data motivation many biological data_processing problems can be formalized as clustering problems to partition data_points into sensible and biologically interpretable groups results this article introduces densitycut a novel density_based algorithm which is both time and space efficient and proceeds as follows densitycut first roughly estimates the densities of data_points from a k nearest_neighbour graph and then refines the densities via a random_walk a cluster consists of points falling into the basin of attraction of an estimated mode of the underlining density_function a post_processing step merges clusters and generates a hierarchical cluster tree the number of clusters is selected from the most stable clustering in the hierarchical cluster tree experimental_results on ten synthetic benchmark_datasets and two microarray gene_expression demonstrate that densitycut performs better than state of the art algorithms for clustering biological_datasets for applications we focus on the recent cancer mutation clustering and single_cell data analyses namely to cluster variant allele_frequencies of somatic_mutations to reveal clonal architectures of individual tumours to cluster single_cell to uncover cell_population compositions and to cluster single_cell mass_cytometry data to detect communities of cells of the same functional_states or types densitycut performs better than competing algorithms and is scalable to large_datasets availability_and data and the densitycut r package is available from https bit bucket org jerry densitycut dev contact clustering analysis unsupervised machine_learning which organizes data_points into sensible and meaningful groups has been increasingly used in the analysis of high_throughput biological_datasets for example the cancer_genome has generated multiple omics_data for individual patients one can cluster the omics_data of individuals into subgroups of potential clinical_relevance to study clonal_evolution in individual cancer patients we can cluster variant allele_frequencies of somatic_mutations such that mutations in the same cluster are accumulated during a specific stage of clonal_expansion emerging technologies such as single_cell have made it possible to cluster single_cell to detect rare cell_populations or to reveal lineage relationships one can cluster single_cell mass_cytometry data to study intratumour heterogeneity as measurement technology advances have drastically enhanced our abilities to generate various high_throughput datasets there is a great need to develop efficient and robust clustering_algorithms to analyze large n number of data_points large d dimensions of data datasets with the ability to detect arbitrary shape clusters and automatically_determine the number of clusters the difficulties of clustering analysis lie in part with the definition of a cluster of the numerous proposed clustering_algorithms the density_based algorithms are appealing because of the probabilistic interpretation of a cluster generated by these algorithms let d fx i g n i x i r d be drawn from an unknown density_function f x x x r d for model based_approaches such as gaussian_mixture f x p c c p c n xjl c r c a cluster is considered as the points generated from a mixture component and the clustering problem is to estimate the parameters of the density_function from d to analyze datasets consisting of complex shape clusters nonparametric methods such as kernel_density can be used to estimate b f x p n i k h x x i where k h is the kernel_function with bandwidth h here a cluster is defined as the data_points associated with a mode of the density_function f x the widely used mean shift algorithm belongs to this category and it locates the modes of the kernel density_function b f x by iteratively moving a point along the density_gradient until convergence this algorithm however is computationally_expensive having time complexity on t where t is the number of iterations typically dozens of iterations are sufficient for most cases a more efficient non iterative graph based_approach constructs trees such that each data point x i represents a node of a tree the parent of node x i is a point x j which is in the direction closest to the gradient direction r b f x i and the root of a tree corresponds to a mode of b f x then each tree constitutes a cluster this algorithm has been used to reduce the time complexity of the mean shift algorithm to on and has been extended in several ways e g constructing trees after filtering out noisy modes nonparametric clustering_methods have been generalized to produce a hierarchical cluster tree consider the k level set of a density_function f x lk f x fxjf x kg the high_level clusters at level k are the connected components of lk f x in the topological sense the maximal connected subsets of lk f x as k goes from to maxf x the high_level clusters at all levels constitute the level set tree where the leaves of the tree correspond to the modes of f x the widely used dbscan algorithm extracts the high_level clusters at just one given level k many original approaches for level set tree construction in statistics take the straightforward plug in approach to estimating the level set tree from b f x by partitioning the feature_space i e x therefore they are computationally_demanding especially for high_dimensional recently efficient algorithms have been proposed to partition the samples d directly recovering the level set tree from a finite dataset is more difficult than partitioning the dataset into separate clusters correspondingly theoretical_analyses show that for these algorithms to identify salient clusters from finite samples the number of data_points n needs to grow_exponentially in the dimension d moreover although the level set tree provides a more informative description of the structure of the data many applications still need the cluster membership of each data point which is not available directly from the level set tree the spectral_clustering works on an n by n pairwise data similarity matrix s where each element s i j measures the similarity between x i and x j the similarity matrix can be considered as the adjacency_matrix of a weighted graph g v e where vertex v i represents x i and the edge_weight e i j s i j given the number of clusters c the spectral_clustering partitions the graph g into c disjoint approximately_equal size clusters such that the points in the same cluster are similar while points in different clusters are dissimilar in contrast to density based_methods the spectral_clustering does not make assumptions on the probabilistic_model which generates data d therefore selecting the number of clusters is a challenging_problem for spectral clustering_algorithms especially in the presence of outliers or when the number of clusters is large in addition the spectral_clustering is time consuming because it needs to compute the eigenvalues and eigenvectors of the row normalized similarity matrix s requiring h n time instead of using single value decomposition to calculate the eigenvalues and eigenvectors the power iteration clustering_algorithm pic iteratively smoothes a random initial vector by the row normalized similarity matrix such that the points in the same cluster will be similar in value then the k means algorithm is used to partition the smoothed vector into c clusters although pic has a time complexity of on t where t is the number of iterations pic may encounter many difficulties in practice first the points from two quite distinct clusters may have very similar smoothed densities and therefore they may not be distinguishable by k means second the points in a non convex shape cluster can break into several clusters as the number of clusters increases these problems become more severe in this article we introduce a simple and efficient clustering_algorithm densitycut which shares some advantages of both densitybased clustering_algorithms and spectral clustering_algorithms as for spectral clustering_algorithms densitycut works on a similarity matrix thus it is computationally_efficient even for high_dimensional using a sparse k nearest_neighbour graph further reduced the time complexity besides we can use a random_walk on the k nearest_neighbour graph to estimate densities at each point as for many density_based algorithms densitycut is simple efficient and there is no need to specify the number of clusters as an input moreover densitycut inherits both methods advantage of detecting arbitrarily shaped clusters finally densitycut offers a novel way to build a hierarchical cluster tree and to select the most stable clustering we first benchmark densitycut against widely used ten simulation datasets and two microarray gene_expression to demonstrate its robustness we then use densitycut to cluster variant allele_frequencies of somatic_mutations to infer clonal architectures in tumours to cluster single_cell to uncover cell_population compositions and to cluster single_cell mass_cytometry data to detect communities of cells of the same functional_states or types 
