data_and an ensemble correlation based gene selection_algorithm for cancer classification with gene_expression data motivation gene_selection for cancer classification is one of the most important topics in the biomedical_field however microarray_data pose a severe challenge for computational_techniques we need dimension_reduction techniques that identify a small set of genes to achieve better learning performance from the perspective of machine_learning the selection of genes can be considered to be a feature_selection problem that aims to find a small subset of features that has the most discriminative information for the target results in this article we proposed an ensemble correlation based gene selection_algorithm based on symmetrical uncertainty and support_vector in our method symmetrical uncertainty was used to analyze the relevance of the genes the different starting points of the relevant subset were used to generate the gene subsets and the support_vector was used as an evaluation criterion of the wrapper the efficiency and effectiveness of our method were demonstrated through comparisons with other feature_selection techniques and the results show that our method outperformed other methods published in the literature recently there has been increasing interests in changing the emphasis of cancer classification from morphologic to molecular cancers are usually marked by a change in the expression levels of certain genes thus the selection of relevant_genes for cancer classification is an important task in most cancer gene_expression studies these discriminative genes are very useful in clinical_applications such as in recognizing disease profiles however microarray_data pose a severe computational challenge because of its high_dimensionality and small_sample from the perspective of machine_learning the selection of genes is a feature_selection problem that aims to find a small subset of features with the most discriminative information for the target feature_selection is an important pre_processing in eliminating irrelevant and redundant features for classification the growing dimensionality of recorded data demands dimension_reduction techniques that identify small sets of features that lead to better learning performance the objective of feature_selection is to provide faster and more effective models and also to avoid_overfitting and the curse of dimensionality feature_selection can be broadly categorized into three types filter wrapper and hybrid the filter methods use specific evaluation criteria that are independent of a learning algorithm to identify a feature subset from the original feature_set filter techniques are fast and scale easily to high_dimensional but they ignore interaction with the classifier wrapper methods use the classifier to evaluate the performance of each subset with a search algorithm wrapper methods tend to find the most suitable feature subset for the learning algorithm but they are very computationally_expensive hybrid methods combine the advantages of filter and wrapper techniques these algorithms aim to achieve the best learning performance with a predetermined learning algorithm and a similar time complexity to filter algorithms a feature_selection procedure can usually be divided_into two steps subset generation and subset evaluation the most important issue in generating a feature subset is how to choose the search_strategy and the starting_point complete search sequential search and random search are the typical search_methods used for subset generation complete search_methods consider every feature subset to be a potential candidate to guarantee finding the optimal result however the computational time is intractable when the dimensionality is high sequential search_methods such as sequential forward selection sfs and sequential backward selection sbs sacrifice completeness by applying the greedy hill climbing approach which adds or removes features one at a time these algorithms are computationally simpler and faster than a complete search_strategy but they can still lead to local_optima random search_methods such as random start hill climbing and simulated_annealing start with a randomly_selected subset and these algorithms help to escape local_optima in the search_space during the past_few the support_vector svm has become very popular because of its good performance on high_dimensional svm was developed byto successfully solve the problems of handwritten digit recognition object_recognition text classification cancer diagnosis and bioinformatics in this article we proposed a hybrid feature selection_algorithm named ensemble correlation based gene_selection ecbgs based on symmetrical uncertainty su and svm for gene_selection our proposed method combined a filter approach and a wrapper method to remove the redundant features and to find the relevant_features from the original feature_set for the original feature_set su was used as an evaluation criterion for the filter using the different starting points as a subset generation strategy and svm as the evaluation learning algorithm of a wrapper it was observed that the classifier combined with our proposed feature_selection method obtained promising classification_accuracy with a small gene subset on six gene_expression in this study we used svm classification_method to analyze the gene_expression data a lot of research has been shown that svm is the most effective classifier in performing accurate cancer diagnosis from gene_expression data svm is interesting because the number of parameters to be estimated essentially depends on the number of samples rather than on the number of features which is particularly relevant with very small sample to feature ratios moreover svm has many mathematical features that make them attractive for gene_expression analysis including their flexibility in choosing a similarity function sparseness of solution when dealing with large_datasets the ability to handle large feature spaces and the ability to identify outliers we have first examined the performance of our method using svm on six microarray_datasets in terms of precision tp rate fp rate and auc in ecbgs there is a parameter the relevance threshold different settings of will directly affect the number of selected genes the closer is set to the smaller the number of selected genes is from the experiments we found that the larger number of genes does not always lead to better performance therefore it is very important to choose the appropriatethe last row is the total average_accuracy of four methods on six datasets the bold values indicate the highest classification_accuracy threshold for improving classification_accuracy however choosing a proper threshold is difficult because the distinct values for each dataset are different through the experiments on six datasets we found that the threshold was closely_related with the degree of the relevance of the genes for example in the lymphoma dataset there are genes with su value that is in contrast there are only genes that satisfy the threshold in breast b dataset thus the appropriate threshold for the dataset that has higher degree of the relevance has to be larger than the one for the dataset that has lower degree h indicate the result on lymphoma dataset i and j indicate the results on mll leukemia dataset k and l indicate the results on prostate dataset we performed two experiments for each dataset one applied the sbs strategy and the other one applied the sfs strategy for three feature_selection as in the figure when the training_dataset is small the proposed method shows high_prediction and other methods primarily make poor predictions moreover the proposed_method other methods in most casesbased on this observation we can make some general recommendations based on the experiments i the threshold could be selected as the mean of all the su value of the genes respect to the class or ii decide the threshold as the following equation su max su min where su max indicates the su value of the most relevant gene respect to the class and su min refers to the lowest one we have also compared ecbgs to the fcbf algorithm the result shows that the proposed ecbgs is able to generate the most meaningful and discriminative genes in most cases however if the number of features is fcbf tends to be more effective it is because ecbgs produces the feature subsets by removing top informative features if the datasets have a small number of features with a lack of useful information removing some informative features will result in less discriminative or meaningless subsets to train the classifier furthermore we found that relevant and non redundant features are selected before repeating our method more than times we have also made a comparison between the proposed method and other feature_selection in terms of classification_accuracy and speed one interesting observation is that our method is still more powerful than other methods even when small data are given it indicates that ecbgs is more appropriate than others for analyzing small_datasets such as gene_expression data moreover our method is significantly_faster than other feature_selection additionally when the number of selected_features is the computational costs of the other three feature_selection are very expensive the selection of discriminant genes is a common task for cancer classification research in biology and medicine may benefit from the examination of the top ranking genes to confirm recent_discoveries in cancer research or suggest new avenues to be explored recently several gene_selection approaches jirapech have been proposed to solve the cancer classification_problem in contrast to these methods the prediction_accuracy of our method is competitive with a small subset of genes 
