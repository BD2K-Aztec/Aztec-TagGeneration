debwt parallel construction of burrows_wheeler transform for large collection of genomes with de_bruijn motivation with the development of high_throughput the number of assembled gen omes continues to rise it is critical to well organize and index many assembled_genomes to promote future genomics studies burrowswheeler_transform bwt is an important data_structure of genome indexing which has many fundamental applications however it is still non trivial to construct bwt for large collection of genomes especially for highly_similar or repetitive genomes moreover the state of the art approaches cannot well support scalable parallel_computing owing to their incremental nature which is a bottleneck to use modern computers to accelerate bwt construction results we propose de_bruijn branch based bwt constructor debwt a novel parallel bwt construction approach debwt innovatively represents and organizes the suffixes of input_sequence with a novel data_structure de_bruijn this data_structure takes the advantage of de_bruijn to facilitate the comparison between the suffixes with long common prefix which breaks the bottleneck of the bwt construction of repetitive genomic_sequences meanwhile debwt also uses the structure of de_bruijn for reducing unnecessary comparisons between suffixes the benchmarking suggests that debwt is efficient and scalable to construct bwt for large dataset by parallel_computing it is well suited to index many genomes such as a collection of individual human_genomes with multiple core servers or clusters availability_and debwt is implemented in c language the source_code is available at httpswith the rapid development and ubiquitous application of highthroughput_sequencing many genomes have been sequenced in cutting_edge genomics studies for example genomes and uk k projects have sequenced many thousands of individual human_genomes moreover as the cost of sequencing continuously decreases e g the cost of sequencing a human sample has already been lower than dollars the number of genomes may explosively increase in the future under this circumstance it is fundamental to well organize and index the large amount of genomes to facilitate future genomics studies burrowswheeler_transform bwt is a self indexing data_structure having many fundamental applications such as genome indexing sequence_alignment genome_compression genome_assembly and sequencing error_correction however the bwt construction of genomic_sequence s is a non trivial task mainly the core of bwt construction is to determine the lexicographical order of all the suffixes of the input_sequence s because there could be many repetitive_sequences within a genome the cost would be prohibitively high to straightforwardly compare all the suffixes to determine their lexicographical orders the problem is even more serious for constructing the bwt of many highly_similar genomes such as a large collection ofwe benchmarked debwt with three datasets mimicking various real application scenarios i a dataset consists of in silico human_genomes totally gbp each of the genomes is generated by integrating the variants of a specific sample from genomes the genomes_project into the human_reference grch hg this dataset mimics the indexing of multiple individual human_genomes which has many applications in genomic studies ii a dataset consists of a set of simulated contigs as long_read sequencing_technologies such as single molecular real_time sequencing have improved the contig n of human genome_assembly to million bp http www pacb com blog toward platinum genomes pacbio releases a newhigher quality chm assembly to ncbi we randomly extracted sequences each is about m bp_long totally gbp from the in silico human_genomes with an in house script which is revised from wgsim simulator https github com lh wgsim iii a dataset consists of eight primate genomes including gibbon gorilla orangutan rhesus baboon chimp bonobo and human downloaded from http hgdownload soe ucsc edu down loads html this dataset assessed the ability of debwt to index more diverse genomes the benchmark was implemented on a server with four intel_xeon e cpus cores in total at ghz and terabytes ram running linux ubuntu debwt uses jellyfish version for implementing the k_mer of the input_sequences the parameter k is configured as two recently published_methods ropebwt and parabwt version binary x were also performed on the same datasets for comparison at first we tested the performance of debwt with threads i e running with all the cpu cores of the server parabwt was also run with threads but ropebwt was run with its default setting as it does not support parallel_computing the elapsed time indicates that debwt and parabwt have comparable speed while ropebwt is slower likely owing to the fact that it does not support parallel_computing and the algorithm could not be suited to long_sequences we further investigate the processing of debwt and found that the speed of debwt was largely slowed down by jellyfish owing to the format of its output_file the default output of jellyfish is a binary file in an unpublished format as the details about the format is unknown for us we used the dump command of jellyfish to convert the output_file into text_file and then converted the text_file into binary file in our own format as the input of further steps this file conversion costs a couple of hours for all the three datasets i e about of the total running time the time cost would be much reduced if the output format of jellyfish was available or if other k_mer tools with similar performance and readable output format were used deducting the time of file conversion debwt is much faster than the other two methods we investigated the time cost of the various steps of debwt mainly two issues are observed first most of the core steps of debwt i e k_mer kmer sorting de_bruijn and values generation and projection suffixes sorting are efficient this is because of a couple of reasons i the de_bruijn branch code greatly_reduces the cost of sorting suffixes with long common prefixes we investigated the lengths of the generated de_bruijn branch code and found that for both of the genomes and the contigs datasets their lengths are respectively one order shorter than those of the original input_sequences under this circumstance the comparison between projection suffixes is much less expensive than that of the original suffixes furthermore the k_mer partition of bwt also helps to reduce many unnecessary comparison operations ii the designs of these steps are suitable for parallel_computing which can fully use the multiple cpu cores it is worth noting that besides the parallel implementation of the core steps of debwt the state of the art k_mer tool also has good parallelization as k_mer is still an open_problem with wide_application there are a few choices for this step we also tried a newer published tool kmc and obtained even faster speed however kmc also outputs a binary file which is hard to directly interpret it would be beneficial if the state of the art k_mer tools have an easy to interpret output_file iii besides the parallelism multiple_steps i e k_mer the radix sort of k_mers de_bruijn and values generation have quasi linear time complexity second the i o operation is the main issue slowing down the method other than the file conversion step mentioned above there are also many i o operations in the de_bruijn analysis and the additional processing_steps that is in the dbg analysis step debwt needs to merge the files recording the four ordered lists of k_mers to recognize the multiple in k_mers and in the additional processing step debwt needs to convert the large sequences to be indexed from text fasta_format into binary format and merge various bwt parts although these operations theoretically have low time complexity they also depend on the performance of the file system of the computer as well as the implementation of the program it is also an important future work for us to further optimize these operations other than the two issues mentioned above the time cost of the projection suffixes sorting step is especially critical as it is the core step to handle the long repetitions within the input_sequence s the total time cost of this step isis the time for solving the j th unsolved part of the bwt as each of the unsolved parts are handled by quicksort the time cost can be represented as follows where n j is the number of the projection suffixes involved in the unsolved part j and dp ps i is the length of the distinguishing prefix of the i th projection suffix denoted as ps i of the part here the distinguishing prefix of ps i is the shortest prefix of ps i which is necessary to determine the bwt characters of the corresponding part for the highly_similar input_sequences g g g nd the upper_bound of dp ps i is o de gm in theory owing to the existence of long repetitions where g m is the input_sequence having the longest de_bruijn and de gm is the length of the corresponding de_bruijn for example two sequences g i and g j could be almost the same thus the length of distinguishing prefix could be close to the length of the de_bruijn of the sequences as the total number of the branching characters of g m the value de gm does not only depend on how many unipaths g m has but also how many copies of the unipaths there are although the theoretical upper_bound is large however dp ps i also greatly depends on the distributions of genomic_variations as well as the repetitiveness of the input_sequences which could make it lower in practice to more precisely investigate the time cost we assessed the dp j values and the dp m j values of the most repetitive dataset in the benchmark i e the human_genomes dataset where dp j and dp m j are respectively the mean and maximal length of the distinguishing prefix of the projection suffixes within the j th unsolved part a series of quantiles of dp j and dp m j values of the human_genomes dataset are shown in these quantiles indicate that for most of the blocks the distinguishing prefixes are short e g the quantile of dp m j is indicating that for of the unsolved bwt parts the max length of the distinguishing prefixes is shorter than characters this is not expensive to determine their lexicographical orders by a straightforward comparison thus the overall cost of this step is not high although there is still a small proportion of bwt parts having long distinguishing prefixes average value is k we also run debwt with threads to investigate its scalability the results suggest that debwt can gradually speedup with the increase of threads i e it has good scalability however the speed of parabwt is nearly the same with the various settings on threads this is likely owing to the incremental nature of the parabwt method which may limit its performance on modern servers and clusters the time of the various steps of debwt with various numbers of threads is in it indicates that the two core steps de_bruijn and values generation and projection suffixes sorting steps and in the figure are most scalable steps i e they speedup with the increasing number of threads this property is beneficial for implementing the method with more computational_resources we further run debwt on the in silico human_genome dataset with various configurations on the k parameter to investigate its effect it can be observed from the result that on a large range of k parameters i e k the total running time is close but for smaller k parameter the time consumption is higher this is likely owing to the fact that the moderate long k_mers such as to mers may have similar ability to span short repeats in this situation the structure of the dbg does not change much with these k configurations i e there are similar numbers of unipaths as well as their copies in the graph however when k is smaller the unipaths will be shorter and have more copies which would make the de_bruijn longer and more projection suffixes need to be sorted k could have better ability to span repeats which may improve the overall performance however it requires much more ram space as a k_mer cannot be stored by one bits cell the memory_footprint of debwt depends on both of the method itself and the used k_mer tool the memory_usage of jellyfish and kmc is highly configurable and we set them to use relatively large memory to accomplish the k_mer step as fast as possible the major ram costs of the three phases of debwt are different in the first phase the major cost originates from the data_structure of k_mer sorting briefly debwt uses a linear table like pde s to bin all the k_mers however each cell of the table costs bytes to record the string of the k_mer as well as its number of copies the cost of the second phase is more complicated it needs to simultaneously keep the input_sequences the de_bruijn branch index and the generated de_bruijn in memory thus the memory_usage mainly depends on several issues i e the size of the input_sequence s the numbers of multiple out and in k_mers and the numbers of the copies of the multiple out and in kmers the last two items respectively determine the length of de_bruijn and the number of unsolved suffixes which need to record in memory the numbers of the multiple out and in k_mers and their copies highly relate to the repetitiveness of the input genomes we did statistics on the two human datasets as they are more repetitive and observed two issues first for both the datasets the numbers of multiple out and in k_mers are much less than s j j i e the number of characters of the input_sequences thus the cost of the hash_table is not expensive comparing with the entire input_sequences moreover it is also worth noting that for highly_similar genomes the increment of the numbers of multiple out and in k_mers would be much smaller comparing with the increment of involved genomes as there are many common sequences and they would not introduce new branches into the dbg second the numbers of the copies of multiple out and in k_mers are also an order lower than s j j although human_genomes are repetitive in this situation the de_bruijn can be seen as a dna_sequence an order shorter than s so that the space cost is not large the major cost originates from the copies of multiple in k_mers as it needs to record the value and the bwt character with a few bytes for each copy the ram cost of the third step is also similar to that of the second phase to sort the projection suffixes it needs to keep the de_bruijn and the values and the bwt characters of the copies of the multiple in k_mers in ram the well organization and indexing of many genomes will be on wide demand in future genomics studies with the rapid increase of assembled_genomes as an important genome indexing data_structure bwt may have many applications however the construction of bwt for a large collection of genomes especially highly_similar re sequenced_genomes e g many human individual_genomes is still a non trivial task moreover owing to the incremental nature of the state of the art methods it is hard to construct bwt with scalable parallel_computing this is a bottleneck to fully use the computational_resources of modern servers or clusters to handle large amount of data we propose debwt a novel parallel bwt construction approach to break the bottleneck the main contribution of debwt is its dbg based representation and organization of suffixes which facilitates the comparison of suffixes with long common prefixes and avoid_unnecessary comparisons moreover owing to its nonincremental design debwt has good scalability to various computational_resources these properties make debwt well suited to construct bwt for large collections of highly_similar or repetitive genomes with modern servers or clusters in the experiments debwt achieves a substantial improvement on the speed of indexing multiple individual human_genomes and contigs for more diverse genomes e g multiple primate genomes debwt also shows faster speed and better parallelization however the improvement is smaller likely owing to that the density of the dbg is lower that is there are more k_mers and unipaths to handle but the overall repetitiveness of the input is lower than highly_similar genomes comparing with state of the art approaches debwt has obviously larger memory_footprint there are potential_solutions to reduce the memory footprints of the various phases of debwt for phase it is feasible to bin the k_mers into several subsets and separately sort each of the subsets with limited memory the time consumption of the various steps of debwt the bars respectively indicate the elapsed time in minutes of the various steps of debwt for the human_genomes dataset a the human_genome contig dataset b and the primate genomes dataset c bars in the same color correspond to a specific number of threads i e blue red green and purple bars are respectively for and threadsdebwt indicates the elapsed time of debwt and debwt no conversion deducts the time of the format conversion of jellyfish output_file for the x y z of debwt in the memory columns the x y and z values respectively indicate the memory footprints of jellyfish phase of debwt and phases and phases of debwt results of the multiple subsets can be straightforwardly merged into the ordered list of all the k_mers with small memory space for phase it is also possible to reduce the memory_footprint by keeping only a proportion of values and bwt characters which can be implemented with the following strategy because all the multiple in k_mers and their numbers of copies are known before the second phase it can partition the whole set of multiple in k_mers into several subsets each of the subsets has a limited number of k_mer copies thus the second phase can be done with multiple times of scanning on the input_sequences instead of one time in each time of scanning only the copies of the multiple in k_mers within the corresponding subset are recognized recorded and output to a specific file with limited ram space as all the subsets are independent to each other for the third phase the files of the subsets can be separately processed to generate various parts of bwt further the bwt parts can be directly merged to accomplish the construction this strategy is feasible to limited workspace but at the expense of time owing to the fact that it needs multiple executions of phase for phase it can also keep only a proportion of unsolved of bwt partitions in memory as all such partitions are independent there are two possible improvements on debwt which are important future works for us first debwt straightforwardly sorts the projection suffixes by quick sort because the de_bruijn can be also seen as a special dna_sequence it is also possible to use other approaches to further accelerate the projection suffix sorting step for example the method proposed by karkkainen uses dcs to accelerate the sorting of the binned suffixes of the original input_sequence this method could be also used for sorting the binned projection suffixes without loss of the ability of parallel_computing as it is non incremental second for the current version of debwt the i o intensive steps are still not optimized which slowed down the speed we plan to further optimize the i o intensive steps to improve the efficiency of debwt meanwhile as k_mer is still an open_problem and advanced k_mer tools are developing we also plan to replace jellyfish by other more advanced k_mer tools or remove the file conversion step by directly accessing the default jellyfish output_file to break the practical bottleneck of the method 
