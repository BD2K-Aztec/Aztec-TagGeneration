sequence_analysis scalablast rapid and robust blast calculations on multiprocessor systems motivation blast remains one of the most widely used tools in computational_biology the rate at which new sequence_data is available continues to grow_exponentially driving the emergence of new fields of biological_research at the same time multicore systems and conventional clusters are more accessible scalablast has been designed to run on conventional multiprocessor systems with an eye to extreme parallelism enabling parallel blast calculations using processing cores with a portable robust fault resilient design that introduces little to no overhead with respect to serial blast availability scalablast source_code can be freely downloaded fromgenome and protein sequence_analysis using blast continues to be among the most used tools for computational bioinformatics the continued exponential_growth in throughput of sequencing_platforms has continued to drive the need for ever expanding capacity for blast calculations to support genome_annotation functional_predictions and a host of other foundational analysis for sequence_data parallel blast accelerators have been implemented in the past including mpiblast and scalablast parallel blast drivers accelerate large lists of blast calculations using multiprocessor systems scalablast used a hybrid parallelization scheme in which the sequence list was statically partitioned among processor pairs process groups process groups performed independent blast calculations simultaneously gaining a degree of speedup on the overall calculation in proportion to the number of process groups used in the calculation the main limitation of scalablast was the use of static data partitioning that did not have fault resilience properties by contrast the main limitation of mpiblast is the need for pre formatting datasets to achieve optimized run time sometimes requiring repeated attempts on the same dataset to find the right pre formatting configuration we have addressed these limitations in scalablast by i re implementing the task scheduling layer by introduction of a dynamic task management scheme that ii does not require preformatting this technique allows processors to obtain work units independently and at run time based on their availability this is a highly tolerant and fault resilient approach that ensures that all processors are doing as close as possible to the same amount of work throughout a calculation in addition this implementation allows for continued operation even in the presence of processor or other system failures this is critical for all large_scale calculations and is independent of the code being run because the longer the run and the larger the system the more likely one is to encounter a component failure during a calculation as the expected run time increases the likelihood of successfully completing the calculation before the next failure tends to zero we demonstrate near ideal scaling using scalablast calculations to machine capacity on a linux cluster having compute cores even during process failure events scalablast can be downloaded freely from http omics pnl gov software scalablast php output and input on globally mounted or local file systems or combinations of both after file distribution is complete the manager is responsible for tracking which tasks have been assigned and which tasks have been completed the manager is also responsible for processing the fasta input_files both query and target_database are in fasta_format eliminating the need for pre formatting database files and distributing these processed files the task groups can be controlled by the user and can span_multiple compute nodes for instance a system with eight core nodes can have a task group size of in which sets of three nodes work together as a single task group having one sub manager core and worker cores this dynamic scheduling layer ensures that when processes fail or get loaded down with tasks taking a long processing time other processes continue to do meaningful work this allows for highly_skewed input sets to be processed as much as possible in an even run time dynamic scheduling is implemented by having the manager hand out tasks to submanagers workers completing a task do not write their output until they verify from the manager via the sub manager whether the task has already been checked back in workers then request a new assignment from the manager when all the tasks have been assigned any workers reporting for new work are given a duplicate task that has not yet been completed in this way nodes that fail during a calculation are simply ignored any tasks assigned to them will be re assigned to other workers until one of them completes the calculation scalablast was run on a linux cluster at pacific_northwest national laboratory that has compute nodes each having eight cores for a total of compute elements for blastp scaling runs our query dataset contained proteins with widely varying size_distribution our query list had an average protein length of ae residues with a minimum_length of eight and a maximum_length of residues this list was compared against a version of the non redundant database from ncbi dated may and containing million reference proteins each query_sequence was compared to the reference_database using blastp with default blosum scoring_matrix and print option 
