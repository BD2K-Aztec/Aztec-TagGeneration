data dependent bucketing improves reference free compression of sequencing_reads motivation the storage and transmission of high_throughput sequencing_data consumes significant resources as our capacity to produce such data continues to increase this burden will only grow one approach to reduce storage and transmission requirements is to compress this sequencing_data results we present a novel technique to boost the compression of sequencing that is based on the concept of bucketing similar reads so that they appear nearby in the file we demonstrate that by adopting a data dependent bucketing scheme and employing a number of encoding ideas we can achieve substantially better compression_ratios than existing de_novo sequence compression tools including other bucketing and reordering schemes our method mince achieves up to a reduction in file sizes on average compared with existing state of the art de_novo compression_schemes the tremendous quantity of data generated by high_throughput sequencing_experiments poses many challenges to data storage and transmission the most common approach to reduce these space requirements is to use an off_the compression program such as gzip by gailly and adler http www gnu org software gzip or bzip by seward http www bzip org to compress the raw read files this approach can result in substantial savings during storage and transmission these programs are general_purpose well tested and highly scalable however research over the past_few has demonstrated that approaches specifically tailored to compressing genomic_data can achieve significantly better compression_rates than general_purpose tools we introduce mince a compression method specifically_designed for the compression of high_throughput sequencing_reads that achieves state of the art compression_ratios by encoding the read sequences in a manner that vastly increases the effectiveness of off_the compressors this approach known as compression boosting has been effectively applied in other contexts and is responsible for the widely observed phenomenon that bam_files become smaller when alignments are ordered by genomic location this places more similar alignments nearby in the file and results in more effective compression being possible mince is able to produce files that are smaller than those of existing compression_methods in a comparable amount of time existing work on compressing sequencing_reads falls into two main categories reference based and de_novo compression reference based_methods most often but not always attempt to compress aligned_reads e g bam format files rather than raw unaligned sequences they assume that the reference_sequence used for alignment is available at both the sender and receiver most reference based_approaches attempt to take advantage of shared_information between reads aligned to genomically close regions and to represent the aligned_reads via relatively small edits with respect to the reference_sequence these methods can in general be very effective at compressing alignments but this does not necessarily imply effective compression of the original read sequences thus if one is interested in the most efficient methods to compress the raw_reads reference based_methods can have drawbacks when compared with de_novo approaches they are generally slower since they require that reads be mapped to a reference before being compressed they assume that the sender and receiver have a copy of the reference which itself would have to be transferred and that the set of reads can be mapped with relatively high_quality to this reference such methods may perform poorly if there are many unmapped reads furthermore since different types of analysis may require different types of alignments recovering the original bam_file may not always be sufficient in which case further processing such as extracting the original sequences from the alignment file may be required conversely de_novo approaches compress the raw sequencing_reads directly and because they do not require aligning the reads to a reference are often able to compress the reads much more quickly de_novo compression_methods often work by trying to exploit redundancy within the set of reads themselves rather than between the reads and a particular reference although most approaches tend to fall into one or the other of these categories some tools expose both reference based and reference free modes notably introduced a novel approach for obtaining some of the benefits of reference based compression even when no reference is available by constructing one on the fly another similar area of research is the compression of collections of related genomes these approaches are able to achieve a very high degree of compression but generally rely on encoding a sparse and relatively small set of differences between otherwise identical_sequences unfortunately the reads of a sequencing_experiment are much more numerous and diverse than a collection of related genomes and hence these methods do not apply to the compression of raw or aligned sequencing_reads we focus on the problem of de_novo compression of raw sequencing_reads since it is the most generally_applicable mince was inspired by the approach ofof compression boosting mince only compresses the actual sequences because the compression of quality_scores and other metadata can be delegated to other approaches that are specifically_designed for compressing those types of data at the core of mince is the idea of bucketing or grouping together reads that share similar sub sequences after reads are assigned to buckets they are reordered within each bucket to further expose similarities between nearby reads and deterministically transformed in a manner that explicitly removes a shared core substring which is the label of the bucket to which they have been assigned the information encoding this reordered collection of reads is then written to a number of different output streams each of which is compressed with a general_purpose compressor depending on the type of read library being compressed we also take advantage of the ability to reverse complement reads to gain better compression in the presence of a reference placing reads in the order in which they appear when sorted by their position in the reference reveals their overlapping and shared sequences without a reference we cannot directly know the reference order the bucketing strategy described here attempts to recover an ordering that works as well as a reference based order without the advantage of being able to examine the reference we demonstrate that the bucketing scheme originally introduced by though very effective can be substantially improved on average by grouping reads in a data dependent_manner and choosing a more effective encoding scheme choosing a better downstream compressor lzip by diaz http www nongnu org lzip lzip html leads to a further reduction in size of overall mince is able to obtain significantly better compression_ratios than other de_novo sequence compressors yielding compressed sequences that are on average smaller than those of scalce we introduced mince a de_novo approach to sequence_read compression that outperforms existing de_novo compression techniques and works by boosting the already impressive lzip general_purpose compressor rather than rely on a set of pre specified core substrings like scalce mince takes a data_driven approach by considering all k_mers of a read before deciding the bucket into which it should be placed further mince improves on the heaviest bucket heuristic used by scalce and instead defines a more representative model for the marginal benefit of particular bucket assignment this model takes_into the mer composition of the read and how similar it is to the set of mers of reads that have already been placed in this bucket early on in the processing of a file when little information exists about the relative_abundance of different k_mers ties between buckets are broken consistently by preferring to bucket a read based on its minimizer this approach allows the selection of core substrings that are among the most frequent k_mers in the provided set of reads and the improved model for bucket assignment leads to more coherent buckets and better downstream compression in the rare situations where a specific order is required for the reads mince is not the most appropriate compression approach further in addition to reordering mince exploits other transformations of a read such as reverse complementing that may or may not be performed in a lossy fashion regardless of whether or not these transformations need to be reversed during decoding they lead to improvements in compression that overcome the cost of storing the sideband information necessary to reverse them 
