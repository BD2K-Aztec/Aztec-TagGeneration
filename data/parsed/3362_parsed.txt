optimal algorithms for haplotype_assembly from whole_genome sequence_data motivation haplotype inference is an important step for many types of analyses of genetic_variation in the human_genome traditional approaches for obtaining haplotypes involve collecting genotype_information from a population of individuals and then applying a haplotype inference algorithm the development_of allows for an alternative strategy to obtain haplotypes by combining sequence_fragments the problem of haplotype_assembly is the problem of assembling the two haplotypes for a chromosome given the collection of such fragments or reads and their locations in the haplotypes which are predetermined by mapping the reads to a reference_genome errors in reads significantly increase the difficulty of the problem and it has been shown that the problem is np_hard even for reads of length existing greedy and stochastic algorithms are not guaranteed to find the optimal solutions for the haplotype_assembly results in this article we proposed a dynamic_programming that is able to assemble the haplotypes optimally with time complexity o m k n where m is the number of reads k is the length of the longest read and n is the total number of snps in the haplotypes we also reduce the haplotype_assembly into the maximum satisfiability problem that can often be solved optimally even when k is large taking_advantage of the efficiency of our algorithm we perform simulation_experiments demonstrating that the assembly of haplotypes using reads of length typical of the current_sequencing is not practical however we demonstrate that the combination of this approach and the traditional haplotype_phasing approaches allow us to practically construct haplotypes containing both common and rare_variants contact obtaining haplotypes or the sequence of alleles on each chromosome is an important step for many types of analyses of genetic_variation in the human_genomes in particular haplotype inference is required for the application of many imputation algorithms which are now widely_applied in the analysis of genome_wide the standard approach for obtaining haplotype_information involves collecting genotype data from a population of individuals genotype data contains information on the set of alleles at each locus but lacks information on which chromosome a particular allele occurs on computational_methods are then applied to these genotype data to infer the haplotypes these methods take advantage of the fact that alleles at neighboring loci in to whom correspondence should be addressed the genomes are correlated or are in linkage_disequilibrium ld as well as the fact that in any given region only a few common_haplotypes account for the majority of the genetic_variations in the population due to their reliance on ld these methods have difficulty inferring haplotypes with rare_variants and have no ability to infer haplotypes for alleles that are unique to an individual recently the development of high_throughput hts technology has enabled an alternative strategy to obtain haplotypes since each sequence_read is from a single chromosome if a read covers two variant sites all of the alleles present in the read must be from the same haplotype using this insight it is possible to assemble the two haplotypes for a chromosome from the collection of such reads by joining reads that share alleles at common_variants the problem is referred to as haplotype_assembly which is challenging in the following two aspects bullet reads are sampled from either of the two haplotypes and no information is given about which one they come from the reads need to be separated for the two haplotypes in the assembly process bullet errors in reads significantly increase the difficulty of the problem and it has been shown that the problem is np_hard even for reads of length a simple greedy heuristic_method which we call the greedy_algorithm concatenates the reads with minimum conflicts and is fast but not very accurate when reads contain errors other stochastic algorithms such as hash which is a markov_chain mcmc_algorithm and hapcut which is a combinatorial_approach have been shown to be much more accurate than the greedy_algorithm on the huref diploid genome_sequence however both hash and hapcut algorithms use stochastic strategies and therefore are not guaranteed to find optimal solutions for the haplotype_assembly in this article we propose a dynamic_programming that is able to assemble the haplotypes optimally with time complexity o m k n where m is the number of reads k is the length of the longest read and n is the total number of heterozygous sites in the haplotypes since this time complexity is exponential in k we reduce the problem to the maximum satisfiability maxsat problem for cases where k is large maxsat conversion is a well known strategy for many computational_biology problem such as snp tagging the converted maxsat problem can often be solved optimally in a reasonable amount of time with an maxsat solver our experiments show that the maxsat approach can solve instances of the converted haplotype_assembly optimally we also show for the first time that the current best known solutionin this article we proposed a dynamic_programming for the haplotype_assembly which is able to assemble the haplotypes optimally with time complexity o m k n where m is the number of reads k is the length of the longest read and n is the total number of snps in the haplotypes our experiments show for the first time that the current best known solutions are very close to the optimal_solution the most difficult part of the haplotype_assembly is to handle the long_reads long_reads can span up to a few hundred positions to handle these cases we convert the problem to an maxsat problem which can be solved optimally by an maxsat solver we show that our maxsat solver is able to solve of the problem instances optimally for the remaining the maxsat solver also reports better results than hapcut does therefore the overall solution we obtained is very close to the optimal although the empirical results of our methods did not show a major advance over existing_methods we believe it is technically important and also interesting to have optimal algorithms for the haplotype_assembly our analysis on individual genotype data from hapmap shows that it is impractical to fully assemble the haplotypes as the coverage ratio needed is too high however combined with a traditional haplotype inference approach our algorithm is able to infer haplotypes containing both rare and common_snps including snps that are unique to individuals 
