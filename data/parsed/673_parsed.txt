bayesian feature_selection for high_dimensional linear_regression via the ising approximation with applications to genomics motivation feature_selection identifying a subset of variables that are relevant for predicting a response is an important and challenging component of many methods in statistics and machine_learning feature_selection is especially difficult and computationally_intensive when the number of variables approaches or exceeds the number of samples as is often the case for many genomic_datasets results here we introduce a new approachthe bayesian ising approximation bia to rapidly calculate posterior_probabilities for feature relevance in l penalized linear_regression in the regime where the regression_problem is strongly regularized by the prior we show that computing the marginal posterior_probabilities for features is equivalent to computing the magnetizations of an ising_model with weak couplings using a mean field approximation we show it is possible to rapidly compute the feature_selection path described by the posterior_probabilities as a function of the l penalty we present simulations and analytical results illustrating the accuracy of the bia on some simple regression problems finally we demonstrate the applicability of the bia to high_dimensional regression by analyzing a gene_expression dataset with nearly features these results also highlight the impact of correlations between features on bayesian feature_selection availability_and an implementation of the bia in c along with data for reproducing our gene_expression are freely_available atlinear regression is one of the most broadly and frequently used statistical_tools despite hundreds of years of research on the subject modern applications of linear_regression to large_datasets present a number of new challenges modern applications of linear_regression such as genome_wide gwas often consider datasets that have at least as many potential variables or features as there are data_points applying linear_regression to high_dimensional often involves selecting a subset of relevant_features a problem known as feature_selection in the literature on statistics and machine_learning even for classical least_squares linear_regression it turns out that the associated feature_selection problem is quite difficult the difficulties associated with feature_selection are especially pronounced in genomics and gwas in general the goal of many genomics studies is to identify a relationship between a small number of genes and a phenotype of interest such as height or body v c the author published_by all_rights for permissions please email journals permissions_oup com mass index for example many gwas seek to identify specific genetic_mutations called single nucleotide polymorphismssnps that best explain the variation of a quantitative_trait such as height or body_mass in a population using various techniques the trait is regressed against binary variables representing the presence_or of the snps in order to find a subset of snps that are highly explanatory for the trait although the number of individuals genotyped in such a study may be in the thousands or even tens_of this pales in comparison to the number of potential snps which can be in the millions moreover the presence_or of various snps tends to be correlated due to chromosome_structure and genetic processes that induce the so_called linkage_disequilibrium as a result selecting the best subset of snps for the regression involves a search for the global minimum of a landscape that is both high_dimensional due to the large number of snps and rugged due to correlations between snps the obstacles that make feature_selection difficult in gwas also occur in many other applications of linear_regression to big datasets in fact the task of finding the optimal subset of features is proven in general to be np_hard therefore it is usually computationally prohibitive to search over all possible subsets of features and one has to resort to other methods of feature_selection for example forward or backward selection adds or eliminates one feature at a time to the regression in a greedy manner alternatively one may use heuristic methods such as sure independence screening sis which selects features independently based on their correlation with the response or minimum redundancy maximum relevance which penalizes features that are correlated with each other the most popular approaches to feature_selection for linear_regression however are penalized least_squares methods that introduce a function that penalizes large regression_coefficients common choices for the penalty function include an l penalty called ridge_regression and an l penalty commonly referred to as lasso_regression penalized methods for linear_regression typically have natural interpretations as bayesian approaches with appropriately chosen prior distributions for example l penalized_regression can be derived by maximizing the posterior_distribution obtained with a gaussian prior on the regression_coefficients similarly l penalized_regression can be derived by maximizing the posterior_distribution obtained with a laplace i e double exponential prior on the regression_coefficients while penalized regression_methods essentially aim to find the features that maximize a posterior_distribution they do not allow one to actually compute posterior_probabilities which provide information about confidence in a bayesian_framework calculating these posterior_probabilities generally requires monte_carlo methods which can be very computationally_demanding in high_dimensions thus in order to apply bayesian approaches to feature_selection to highdimensional problems it is necessary to develop approximate_methods for computing posterior_probabilities that bypass the need for extensive sampling from the posterior_distribution inspired by the success of statistical physics approaches to hard problems in computer science and statistics we study high_dimensional regression with strongly regularizing prior distributions a strongly regularizing prior_distribution is one that exerts a significant influence on the posterior_distribution even when the sample_size goes to infinity the definition will be made more precise later in this strongly regularized regime we show that the marginal posterior_probabilities of feature relevance for l penalized_regression are well approximated by the magnetizations of an appropriately chosen ising modela widely_studied model from physics used to describe magnetic_materials for this reason we call our approach the bayesian ising approximation bia of the posterior_distribution using the bia the posterior_probabilities can be computed without resorting to monte_carlo using an efficient mean field approximation that facilitates the analysis of very high_dimensional we envision the bia as part of a two stage procedure where the bia is applied to rapidly screen irrelevant variables i e those that have low_rank in posterior_probability before applying a more computationally_intensive cross_validation procedure to infer the regression_coefficients for the reduced feature_set this study is especially well suited to modern feature_selection problems where the number of features p is often larger than the sample_size n our approach differs_significantly from previous methods for feature_selection traditionally penalized_regression and related bayesian approaches have focused on the weakly regularized regime where the effect of the prior is assumed to be negligible as the sample_size tends to infinity the underlying intuition for considering the weak regularization regime is that as long as the prior i e the penalty parameter is strong enough to regularize the inference problem a less influential prior_distribution should be better suited for feature_selection and prediction tasks because it allows the data to speak for themselves in the machine_learning literature the penalty parameter is usually chosen using cross_validation to maximize out of sample predictive_ability a similar esthetic is also reflected in the abundant literature on objective priors for bayesian_inference as expected these weakly regularizing approaches perform well when the sample_size exceeds the number of features n p however very strong priors may be required for high_dimensional inference where the number of features can greatly exceed the sample_size p n our bia approach exploits the large penalty parameter in this strongly regularized regime to efficiently calculate marginal posterior_probabilities using methods from statistical physics the article is organized as follows in section we review bayesian linear_regression in section we derive the bia using a series expansion of the posterior_distribution and describe the associated algorithm for variable_selection and in section we present analytical results and simulations on the performance of the bia using features with a constant correlation in section we analyze a real_dataset for predicting bodyfat percentage from different body measurements and in section we analyze a real_dataset for predicting a quantitative phenotypic trait from data on the expression of genes in soybeans to summarize we have shown that bayesian feature_selection for l penalized_regression in the strongly regularized regime corresponds to an ising_model which we call the bia mapping the posterior_distribution to an ising_model that has simple expressions for the local fields and couplings using a controlled approximation opens_the to analytical studies of bayesian feature_selection using the vast number of techniques developed in physics for studying the ising_model it will be interesting to see if our analyses can be generalized to study bayesian feature_selection for many statistical_techniques other than linear_regression as well as other prior distributions from a practical standpoint the bia provides an algorithm to efficiently compute bayesian feature_selection paths for l penalized_regression using our approach it is possible to compute posterior_probabilities of feature relevance for very high_dimensional such as those typically found in genomic studies unlike most previous work of feature_selection the bia is ideally_suited for large genomic_datasets where the number of features can be much greater than the sample_size p n the underlying reason for this is that we work in strongly regularized regime where the prior always has a large influence on the posterior_probabilities this is in contrast to previous_works on penalized_regression and related bayesian approaches that have focused on the weakly regularized regime where the effect of the prior is assumed to be small moreover we have identified a sharp threshold for the regularization_parameter k n pr where the bia is expected to break down this threshold depends on the sample_size n number of features p and root_mean correlation between features r the threshold at which the bia breaks down occurs precisely at the transition from the strongly regularized to the weakly regularized regimes where the prior and the likelihood have a comparable influence on the posterior_distribution this study also highlights the importance of accounting for correlations between features when assessing statistical_significance in large_datasets when the number of features is large even small correlations can cause a huge reduction in the posterior_probabilities of features for example our analysis of a dataset including the expression of genes demonstrates that the resulting posterior_probabilities of gene relevance may be very close to value representing random chance p k s j jy when p n and the genes are moderately correlated e g r this is likely to have important implications for assessing the results of gwas_studies where such correlations are often ignored moreover we suggest that it is generally not reasonable to choose a posterior_probability threshold for judging significance on very high_dimensional problems instead the bia can be used as part of a two stage procedure where the bia is applied to rapidly screen irrelevant variables i e those that have low_rank in posterior_probability before applying a more computationally_intensive crossvalidation procedure to infer the regression_coefficients the computational_efficiency of the bia and the existence of a natural threshold for the penalty parameter where the bia works make this procedure ideally_suited for such two stage procedures true_positive rates for feature_selection with a correlated and b uncorrelated genes features were selected by taking the q genes with highest posterior_probability at k k the uncorrelated genes were created by randomly shuffling the correlated genes the root_mean correlation among the correlated genes was r compared with r for the uncorrelated genes 
