sequence_analysis turtle identifying frequent k_mers with cache efficient algorithms motivation counting the frequencies of k_mers in read libraries is often a first step in the analysis of high_throughput sequencing_data infrequent k_mers are assumed to be a result of sequencing_errors the frequent k_mers constitute a reduced but error_free representation of the experiment which can inform read_error or serve as the input to de_novo methods ideally the memory requirement for counting should be linear in the number of frequent k_mers and not in the typically much larger total number of k_mers in the read library results we present a novel method that balances time space and accuracy requirements to efficiently extract frequent k_mers even for high coverage libraries and large_genomes such as human our method is designed to minimize cache misses in a cache efficient manner by using a pattern blocked bloom_filter to remove infrequent k_mers from consideration in combination with a novel sort and compact scheme instead of a hash for the actual counting although this increases theoretical complexity the savings in cache misses reduce the empirical running_times a variant of method can resort to a counting bloom_filter for even larger savings in memory at the expense of false_negative in addition to the false_positive common to all bloom_filter based_approaches a comparison with the state of the art shows reduced memory_requirements and running_times availability_and the tools are freely_available for download atk mers play_an in many methods in bioinformatics because they are at the core of the de_bruijn graph_structure that underlies many of todays popular de_novo assemblers they are also used in assemblers based on the overlaplayout consensus paradigm like celera and arachne as seeds to find overlap between reads several read correction tools use k_mer frequencies for error_correction their main motivation for counting k_mers is to filter out or correct sequencing_errors by relying on k_mers that appear multiple times and can thus be assumed to reflect the true sequence of the donor genome in contrast k_mers that appear only once are assumed to contain sequencing_errors melsted and pritchard and marcais and kingsford make a more detailed compelling argument about the importance of k_mer in a genome of size g we expect up to g unique k_mers this number can be smaller because of repeated regions which produce the same k_mers and small k as smaller k_mers are less likely to be unique but is usually close to g for reasonable values of k however depending on the amount of sequencing_errors the total number of k_mers in the read library can be substantially larger than g for example in the dm dataset the total number of mers is m whereas the number of mers occurring at least twice is m the size of the genome is mb megabase pairs this is not surprising because one base call error in a read can introduce up to k false kmers consequently counting the frequency of all k_mers as done by jellyfish which is limited to k requires o n space where n is the number of k_mers in the read library this makes the problem of k_mer frequency counting time and memory intensive for large read libraries like human we encounter similar problems for large_libraries while using khmer which uses a bloom_filter based_approach for counting frequencies of all k_mers ideally the frequent k_mer identifier should use o n space where n is the number of frequent k_mers n n the approach taken by bfcounter achieves something close to this optimum by ignoring the infrequent k_mers with a bloom_filter and explicitly storing only frequent k_mers this makes bfcounter more memory_efficient compared with jellyfish however the running time of bfcounter is large for two reasons first it is not multi_threaded second both the bloom_filter and the hash_table used for counting incur frequent cache misses the latter has recently been identified as a major_obstacle to achieving_high performance on modern architectures motivating the development of cache oblivious algorithms and data_structures which optimize the cache behavior without relying on information of cache layout and sizes additionally bfcounter is also limited to a count range of which will often be exceeded in single_cell experiments because of the large local coverage produced by whole_genome amplification artifacts a different approach is taken by dsk to improve memory_efficiency dsk makes many passes over the read file and uses temporary disk_space to trade_off the memory requirement althoughclaimed dsk to be faster than bfcounter on our machine to whom correspondence should be addressed using an tb raid storage system dsk required more wallclock time compared with bfcounter therefore we consider dsk without dedicated high performance disks e g solid_state and bfcounter to be too slow for practical use on large_datasets a disk based sorting and compaction approach is taken by kmc which was published very recently and it is capable of counting k_mers of large read libraries with a limited amount of memory however in our test environment we found it to be slower than the method described here we present a novel approach that reduces the memory_footprint to accommodate large_genomes and high coverage libraries one of our tools scturtle can report frequent mers with counts with a very low false_positive from a human read set with gb using gb of memory in h using worker threads like bfcounter our approach also uses a bloom_filter to screen out k_mers with frequency one with a small false_positive but in contrast to bfcounter we use a pattern blocked bloom_filter the expected number of cache misses for each inquiry update in such a bloom_filter is one the frequency of the remaining k_mers is counted with a novel sorting and compaction based algorithm our compaction step is similar to run length encoding note that this is similar to the strategy of kmc which was developed as a concurrent and independent work though the complexity of sorting in our compression step is on log n it has sequential and localized memory access that helps in avoiding cache misses and will run faster than an o n algorithm that has o n cache misses as long as log n is much smaller than the penalty issued by a cache miss for larger datasets where o n space is not available the aforementioned method will fail we show that it is possible to get a reasonable approximate solution to this problem by accepting small false_positive the method is based on a counting bloom_filter implementation the error_rates can be made arbitrarily small by making the bloom_filter larger because the count is not maintained in this method it reports only the k_mers seen more than once with a small false_positive and false_negative but not their frequency we call the first tool scturtle and the second one cturtle identifying correct k_mers out of the k_mer spectrum of a read library is an important step in many methods in bioinformatics usually this distinction is made by the frequency of the k_mers fast tools for counting k_mer frequencies exist but for large read libraries they may demand a significant amount of memory which can make the problem computationally unsolvable on machines with moderate amounts of memory resource gb or even with gb for large_datasets simple memory_efficient methods on the other hand can be timeconsuming unfortunately there is no single tool that achieves a reasonable compromise between memory and time here we present a set of tools that make some compromises and simultaneously achieve memory and time requirements that are matching the current state of the art in both aspects with our first tool scturtle we achieve memory_efficiency by filtering k_mers of frequency one with a bloom_filter our pattern blocked bloom_filter implementation is more time efficient compared with a regular bloom_filter we present a novel strategy based on sorting and compaction for storing frequent kmers and their counts because of its sequential memory access pattern our algorithm is cache efficient and achieves good running time however because of the bloom filters we incur a small false_positive the second tool cturtle is designed to be more memory_efficient at the cost of giving up the frequency values and allowing both false_positive the implementationnote the tools ran with fast mod and worker threads each reported number is an average of five runs note for the large_datasets because of memory constraints the exact counts for all k_mers could not be obtained and therefore these rates could not be computed is based on a counting bloom_filter that keeps track of whether a k_mer was observed and whether it has been stored in external media this tool does not report the frequency count of the kmers both tools allow a k_mer size of up to they also allow the user to decide how much memory should be consumed of course there is a minimum memory requirement for each dataset and the amount of memory directly influences the running time and error_rate however we believe with the proper compromises the approximate frequent k_mer extraction problem is now computationally_feasible for large read libraries within reasonable wall clock time using a moderate amount of memory 
