time and memory_efficient likelihood_based tree searches on phylogenomic alignments with missing_data motivation the current molecular data explosion poses new challenges for large_scale phylogenomic analyses that can comprise hundreds or even thousands of genes a property that characterizes phylogenomic datasets is that they tend to be gappy i e can contain taxa with many and disparate missing genes in current phylogenomic analyses this type of alignment gappyness that is induced by missing_data frequently exceeds we present and implement a generally_applicable mechanism that allows for reducing memory footprints of likelihood_based maximum_likelihood ml or bayesian phylogenomic analyses proportional to the amount of missing_data in the alignment we also introduce a set of algorithmic rules to efficiently conduct tree searches via subtree pruning and re grafting moves using this mechanism results on a large phylogenomic dna dataset with taxa genes and a gappyness of we achieve a memory_footprint reduction from gb down to gb a speedup for optimizing ml model_parameters of and accelerate the subtree pruning regrafting tree search phase by factor thus our approach can be deployed to improve_efficiency for the two most important resources cpu time and memory by up to one order of magnitude availability current open_source version of raxml v available atin this article we study the time and memory_efficient execution of subtree pruning and re grafting moves for conducting tree searches on gappy phylogenomic multi gene alignments also known as super matrices under the maximum_likelihood ml model by example of raxml while we use raxml to prove our concept the mechanisms presented here can easily be integrated into all bayesian and ml based programs that conduct tree searches and are hence predominantly limited by the time and space efficiency of likelihood computations on trees typically likelihood computations account for of overall execution time in bayesian and ml programs moreover the space required to hold the probability vectors of the likelihood model the ancestral probability vectors that are assigned to the inner nodes of the tree also largely dominates the memory_consumption of likelihood_based programs while space and time requirements can be reduced by using the cat approximation of rate heterogeneity and or single precision instead of double precision floating point arithmetics berger and stamatakis to whom correspondence should be addressed there exists an urgent need to further improve the computational_efficiency of the likelihood_function because of the bio gap i e the fact that molecular data accumulates at a faster pace than processor architectures are becoming faster seein as bioinformatics is coming off age and because the community is facing unprecedented challenges regarding the scalability and computational_efficiency of widely used bioinformatics functions we believe that work on algorithmic engineering aspects will become increasingly important to ensure the success of the field the largest published ml based phylogenomic study in terms of cpu hours and memory_requirements already required million cpu hours and gb of main_memory on an ibm bluegene l supercomputer moreover we are receiving an increasing number of reports by raxml users that intend to conduct phylogenomic analyses on datasets that require up to gb of main_memory under the standard model of rate heterogeneity and double precision arithmetics memory_consumption is therefore becoming a limiting_factor for phylogenomic analyses especially at the whole genome_scale initial work by stamatakis and ott b on methods for efficiently computing the likelihood on phylogenomic alignments with missing_data focused on computing the likelihood and optimizing branch_lengths on a single fixed tree_topology using pointer meshes here we address the conceptually more difficult extension of this approach to likelihood model parameter_optimization for parameters other than branch_lengths and tree searches that entail dynamically changing trees we describe and make available as open source_code a generally_applicable framework to efficiently compute the likelihood on dynamically changing tree_topologies during a subtree pruning regrafting spr based tree search search algorithms that rely on spr moves represent the most widely used tree search technique in state_ofthe programs for phylogenetic_inference in addition we implement full ml model parameter_optimization under the proposed mechanism and also take advantage of the memory_footprint reduction potential that was only mentioned as a theoretical possibility by stamatakis and ott b without providing an actual implementation the recursive lookups to search for gene nodes in the pruning branch and insertion branch subtrees are implemented navely by recursive descents into subtrees while this is algorithmically not very elegant the efficiency of this procedure is not critical because a profiling run using gprof on datasets d and d revealed that the recursive search procedures account for of total execution time on d the contribution may be higher but a profiling run could not be conducted because of excessive run times and the significant slowdown associated with profiling in table we indicate the execution time speedups between the standard implementation and the mesh based_approach for model parameter_optimization denoted as model optimization as well as fast lazy denoted as fast spr and slow lazy spr searches denoted as slow spr overall speedups for the fast lazy sprs tend to be higher than for the thorough lazy sprs particularly for protein data in we provide the overall execution times in seconds including file i o model optimization and spr searches for the mesh based and standard denoted as nomesh likelihood_function implementations using the fast and the more thorough lazy spr moves in table we provide the gappyness of each dataset i e the proportion of entirely missing_data per gene over the entire alignment and the memory_footprint for inferences under gtr and wag for the mesh based and standard approach the memory savings are roughly proportional to the degree of gappyness finally inwe depict the likelihood scores of the trees computed independently by optimizing the likelihood_score on the resulting spr modified trees obtained by the mesh based and standard method the scores on the trees were optimized using the mesh based_approach to save time but for the smaller datasets we also conducted a tree evaluation using the standard approach as already mentioned likelihood scores may be slightly different if model_parameters are optimized using the standard approach because of numerical deviations it is interesting to observe that for thorough lazy spr moves the mesh based_approach yields slightly better likelihood scores on dataset d and d this can be attributed to numerical error propagation because under the standard approach a significantly_larger number of computations is conducted that may introduce rounding errors 
