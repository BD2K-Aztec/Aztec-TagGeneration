de_novo meta assembly of ultra_deep data we introduce a new divide_and approach to deal with the problem of de_novo in the presence of ultra_deep data i e coverage of x or higher our proposed meta assembler slicembler partitions the input_data into optimal sized slices and uses a standard assembly tool e g velvet spades idba ud and ray to assemble each slice individually slicembler uses majority_voting among the individual assemblies to identify long contigs that can be merged to the consensus assembly to improve its efficiency slicembler uses a generalized suffix_tree to identify these frequent contigs or fraction thereof extensive experimental_results on real ultra_deep data x coverage and simulated_data show that slicembler significantly_improves the quality of the assembly compared with the performance of the base assem bler in fact most of the times slicembler generates error_free assemblies we also show that slicembler is much more resistant against high sequencing_error than the base assembler availability_and slicembler can be accessed atsince the early days of dna_sequencing the problem of de_novo has been characterized by insufficient and or uneven depth of sequencing_coverage see e g insufficient sequencing_coverage along with other shortcomings of sequencing instruments e g short read_length and sequencing_errors exacerbated the algorithmic challenges in assembling large complex genomein particular those with high repetitive content some of the third generation of sequencing_technology currently on the market e g pacific_biosciences and oxford nanopore offers very long_reads at a higher cost per base but sequencing_error is much higher as a consequence long_reads are more commonly used for scaffolding contigs created from second generation data rather than for de_novo thanks to continuous improvements in sequencing_technologies life_scientists can now easily sequence dna at depth of sequencing_coverage in excess of x especially for smaller genomes like viruses bacteria or bacterial_artificial bac yac clones ultra_deep i e x or higher has already been used in the literature for detecting rare dna_variants including mutations causing cancer for studing viruses as well as other applications as it becomes more and more common ultra_deep data are expected to create new algorithmic challenges in the analysis_pipeline in this article we focus on one of these challenges namely the problem of de_novo we showed recently that modern de_novo assemblers spades idba ud and velvet are unable to take advantage of ultra deep coverage even more surprising was the finding that the assembly_quality produced by these assemblers starts degrading when the sequencing_depth exceeds x x depending on the assembler and the sequencing_error by means of simulations on synthetic reads we also showed inthat the likely culprit is the presence of sequencing_errors the assembly_quality degradation cannot be observed with error_free reads whereas higher sequencing_error intensifies the problem the message of our study is that when the data are noisy more data are not necessarily better rather there is an error_rate dependent optimum independently from us study reached similar conclusions the authors assembled e coli mb s kudriavzevii mb and c elegans mb using soapdenovo velvet abyss meraculous and idba ud at increasing sequencing_depths up to x which is not ultra deep according to our definition their analysis showed an optimum sequencing_depth around x 
