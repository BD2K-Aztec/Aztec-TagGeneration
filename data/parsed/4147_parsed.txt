genome_analysis scoring and unfolding trimmed tree assembler concepts constructs and comparisons motivation mired by its connection to a well known n p complete combinatorial_optimization problemnamely the shortest common superstring problem scsp historically the whole_genome sequence_assembly wgsa problem has been assumed to be amenable only to greedy and heuristic methods by placing efficiency as their first priority these methods opted to rely only on local searches and are thus inherently approximate ambiguous or error_prone especially for genomes with complex structures furthermore since choice of the best heuristics depended critically on the properties of e g errors in the input_data and the available long_range information these approaches hindered designing an error_free wgsa pipeline results we dispense with the idea of limiting the solutions to just the approximated ones and instead favor an approach that could potentially lead to an exhaustive exponential time search of all possible layouts its computational_complexity thus must be tamed through a constrained search branch and bound and quick identification and pruning of implausible overlays for his purpose such a method necessarily relies on a set of score functions oracles that can combine different structural_properties e g transitivity coverage physical_maps etc we give a detailed_description of this novel assembly framework referred to as scoring and unfolding trimmed tree assembler sutta and present experimental_results on several bacterial_genomes using next_generation data we also report experimental_evidence that the assembly_quality strongly depends on the choice of the minimum overlap parameter k since the pioneering work of frederick sanger in when he developed the basic dna_sequencing sanger chemistry has continued to be employed widely in to whom correspondence should be addressed practically all large_scale genome_projects however whole_genome sequence_assembly wgsa pipelines in these projects have usually resorted to the shotgun_sequencing strategy in order to reconstruct a genome_sequence despite the limitation that sanger chemistry could only generate moderate sized reads around bp with no location information while many recent_advances in sequencing_technology has yielded_higher throughput and lower cost the limitations imposed by the read_lengths ranging between and bp still plague the genomics science forcing it to work with draft quality unfinished genotypic and misassembled genomic_data the problem is however complicated by the presence of haplotypic ambiguities base_calling errors and repetitive genomic sections recall that to obtain the input read data the dna polymer is first sheared into a large number of small_fragments and then either the entire fragment or just its ends are sequenced the resulting sequences are then combined into a consensus_sequence using a computer program dna_sequence assembler it is desired that the consensus has as small a base level discrepancy with respect to the original dna polymer as possible researchers first approximated the shotgun sequence_assembly problem as one of finding the shortest common superstring of a set of sequences although this was an elegant theoretical abstraction it was oblivious to what biology needs to make correct interpretation of genomic_data in fact it misses the correct model for the assembly problem for at least three different reasons i it does not model possible errors arising from sequencing the fragments ii it does not model fragment orientation the sequence source can be one of the two dna_strands iii most importantly it fails in the presence of repeats in the genome faced with this theoretical computational intractability n p complete most of the practical approaches for genome sequence_assembly were devised to use greedy and heuristic methods that by definition restrict themselves to find suboptimal solutions see note that if the dna was totally random then the overlap information would be sufficient to reassemble the target_sequence and greedy algorithms would perform always well however this argument is mostly irrelevant since the problem is complicated by the presence of various non random structures in particular in eukaryotic_genomes e g repeated regions rearrangements segmental_duplications in the case of human_genome initially two unfinished draft sequences were produced by different methods one by the international human_genome consortium ihgsc and another by celera genomics cg with the published ihgsc assembly constructed by the program gigassembler devised atwe have compared sutta to several well known short_read assemblers on three real_datasets from illumina next_generation data using both mated and unmated reads the following assemblers are used in the comparison edena velvet taipan abyss ssake and euler sr although these datasets do not represent the state of the art in sequencing_technology for example illumina can currently generate longer_reads up to bp they have been extensively analyzed by previously_published short_read assemblers the experimental_results show that sutta has comparable_performance to the best state of the art assemblers based on contig size comparison this comparison is to be interpreted in the context of our experimental_evidence that the choice of the minimum overlap parameter k affects both contig size and assembly_quality presented below see figs and and present the comparison based on contig size analysis for all three genomes only contigs of minimal length_bp are considered in the statistics a contig is classified as correct if it fully aligns to the genome with a minimum base similarity of for s aures and h acininychis and for e coli inspecting the results in it is evident that sutta performs comparatively well relative to these assemblers in particular sutta a thanks to its aggressive strategy assembles longer contigs but it pays in assembly_quality by generating more misassembly errors sutta c instead behaves more conservatively and generated less errors but without excessively sacrificing contig length the choice between the aggressive strategy and the conservative one is clearly based on the overall quality of the input_set of reads and the genome_structure for example in the case of an error_free dataset and a genome with few and short repeats we may opt for an aggressive strategy in the page suttacase of mate_pair data sutta produces shorter contigs compared with abyss velvet and euler sr however suttas overall assembly_quality is superior with fewer and shorter misassembled contigs sequence_assembly accuracy has now become particularly important in i genome_wide ii detecting new polymorphisms and iii finding rare and de_novo mutations new sequencing_technologies have reduced_cost and increased the throughput however they have sacrificed read_length and accuracy by allowing more single nucleotide base_calling and indel e g due to homo polymer errors overcoming these difficulties without paying for high_computational requires i better algorithmic framework not greedy ii being able to adapt to new and combined hybrid technologies allowing significantly large coverage and auxiliary long rage information and iii prudent experiment_design we have presented a novel assembly algorithm sutta that has been designed to satisfy these goals as it exploits many new algorithmic ideas challenging the popular intuition sutta enables fast global optimization of the wgsa problem by taming the complexity using the b b method because of the generality of the proposed approach sutta has the potential to adapt to future sequencing_technologies without major changes to its infrastructure technology dependent features can be encapsulated into the lookahead procedure and well chosen score functions 
