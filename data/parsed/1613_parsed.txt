error_correction of high_throughput sequencing_datasets with non uniform coverage motivation the continuing improvements to high_throughput hts platforms have begun to unfold a myriad of new applications as a result error_correction of sequencing_reads remains an important problem though several tools do an excellent job of correcting datasets where the reads are sampled close to uniformly the problem of correcting reads coming from drastically non uniform datasets such as those from single_cell remains_open results in this article we develop the method hammer for error_correction without any uniformity assumptions hammer is based on a combination of a hamming graph and a simple probabilistic_model for sequencing_errors it is a simple and adaptable algorithm that improves on other tools on non uniform single_cell data while achieving comparable results on normal multi cell data the continuing improvements to high_throughput hts platforms have begun to unfold a myriad of new and exciting applications such as transcriptome_analysis metagenomics singlecell assembly and variation detection in addition classical problems such as assembly are becoming feasible for large_scale endeavours such as the genome k project which seeks to assemble the genomes of novel species genome k community of all these projects face the same difficulty of handling the base errors that are inevitably prevalent in hts datasets error_correction of reads has thus come to the forefront as an essential problem with a slew of publications in the last years a common approach of error_correction of reads is to determine a threshold and correct k_mers whose multiplicities fall below the threshold choosing the correct threshold is crucial since a low threshold will result in too many uncorrected errors while a high threshold will result in the loss of correct k_mers the histogram of the multiplicities of k_mers will show a mixture of two distributionsthat of the error_free k_mers and that of the erroneous k_mers when the coverage is high and uniform these distributions are centered far apart and can be to whom correspondence should be addressed separated without much loss using a cutoff threshold such methods therefore achieve excellent results though hts platforms provide relatively uniform coverage in many standard sequencing_experiments in some of the more challenging applications such as transcriptome_sequencing the coverage remains drastically uneven another prominent emerging example is that of singlecell sequencing which has enabled the investigation of a diverse range of uncultivated bacteria from the surface ocean environment to the human_body though these bacteria are not amenable to normal sequencing recent_advances in dna_amplification technology have enabled genome_sequencing directly from individual cells without requiring growth in culture the read datasets thus obtained from single cells suffer from amplification_bias resulting in orders_of difference in coverage and the complete absence of coverage in some regions previous single_cell studies have used error_correction tools that assume near uniform coverage however such approaches are not as effective and result in decreased quality of assemblies when the two multiplicity distributions of erroneous and error_free k_mers are not separable using a simple threshold thus despite the initial technological difficulties the challenges_facing single_cell genomics are increasingly computational rather than experimental for applications such as these it becomes paramount to develop better error_correction algorithms that do not assume uniformity of coverage recent papers have introduced a powerful idea that has the potential to remove uniformity assumptions consider two k_mers x and y that are within a small hamming_distance and present in the read dataset if our genome does not contain many non exact repeats then it is likely that both x and y were generated by the k_mer among them that has higher multiplicity in this way we can correct the lower multiplicity k_mer to the higher multiplicity one without relying on uniformity this can be further generalized by considering the notion of the hamming graph whose vertices are the distinct k_mers annotated with their multiplicities and edges connect k_mers with a hamming_distance less than or equal to a parameter in this article we develop the method hammer for error_correction without any uniformity assumptions hammer is based on a combination of the hamming graph and a simple probabilistic_model it is a simple and adaptable algorithm that improves onpage i i i to test the effectiveness of hammer on non uniform datasets we used reads generated from a single_cell of e coli k strain using one lane of the illumina gaii pipeline submitted for publication they total coverage mostly by bp long_reads we mapped the reads to the e coli_genome using bowtie and found extremely non uniform coverage there were k positions that are not covered by any read with an additional k being covered by less than six reads in contrast in an alternate dataset that was generated on a normal multi cell sample with the same coverage and read_length there were no uncovered regions prior to working with this dataset we trimmed the first three bases and any trailing bases with a quality value of we then removed any remaining reads with ambiguous bases we use a value of k to run hammer on this dataset motivated by the fact that it was found to work best for assemblers on this data submitted for publication the distance used in building the hamming graph was set to two since we found larger values did not significantly decrease the number of singleton clusters for the singletoncutoff we used the default value of which roughly speaking means that any singleton k_mer that appears at least twice with decent quality values is kept however to demonstrate that hammer can be made super sensitive if needed we also ran hammer with a singletoncutoff of for the savecutoff we used a value of the results are shown_inand to measure the quality of a set of reads we measure its sensitivity and positive_predictive ppv with respect to the source genome let k be the set of distinct k_mers present in the reads and let s be the kspectrum of e coli i e the set of all distinct k_mers the sensitivity is measured as ks s and reflects the percentage of k_mers from the genome that are present in the reads the positive_predictive is ks k and reflects the fraction of k_mers in the dataset that do not contain errors note that we do not measure the quality of the corrected reads against the quality of uncorrected reads directly but by measuring the quality of each separately against the e coli_genome despite the success of many error_correction tools emerging areas such as single_cell fuel the need to develop better algorithms for situations of non uniform coverage recent_approaches have taken steps in the right direction by looking together i 
