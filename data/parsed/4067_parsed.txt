robust relative compression of genomes with random_access motivation storing transferring and maintaining genomic_databases becomes a major_challenge because of the rapid technology progress in dna_sequencing and correspondingly growing pace at which the sequencing_data are being produced efficient compression with support for extraction of arbitrary snippets of any sequence is the key to maintaining those huge_amounts of data results we present an lz style compression scheme for relative compression of multiple_genomes of the same species while the solution bears similarity to known algorithms it offers significantly_higher compression_ratios at compression speed over an order of magnitude greater in particular differentially encoded human_genomes are compressed over times at fast compression or even times at slower compression the reference_genome itself needs much more space adding fast random_access to text snippets decreases the ratio to availability gdc is available at http sun aei polsl pl gdc rapid development in dna_sequencing led to drastic growth of data publicly_available in sequence_databases for example genbank at ncbi or genomes_project the low_cost of acquiring an individual human_genome opens_the to personalized_medicine dna_sequences within the same species are both large and highly repetitive for example only of the gb human_genome is specific the rest is common to all humans this poses interesting challenges for efficient storage and fast access to those data most classic data compression techniques fail to recognize this tremendous redundancy simply because finding matches with e g an lz variant with a sliding_window would require a multigigabyte buffer not counting the match finding structures on the other hand using a context based statistical coding for example ppm may require maintaining a high order statistical_model otherwise the context statistics will be polluted with accidental dna matches using such a model is problematic due to its enormous memory_requirements to whom correspondence should be addressed interestingly most of the dna specialized compressors from the literature are inappropriate to handle modern genomic_databases there are a number of reasons for that i most of them care about compression_ratio rather than compression and decompression speed or the memory use during the compression process for these reasons they are not practical for sequences larger than say several megabytes ii most effort has been focused on succinctly representing a single_genome which is believed to be almost incompressible anyway hence only tiny improvements were at the stake not to be particularly efficient in detecting inter genome redundancy iii extracting a range of symbols from the middle of the compressed stream is a rarely supported feature only since around can we observe a surge of interest in practical multi sequence oriented dna compressors usually coupled with random_access capabilities and sometimes also offering indexed search the first algorithms from were soon followed by more mature proposals which will be presented below added index functionalities to compressed dna_sequences display which can also be called the random_access functionality returning the substring specified by its start and end position count telling the number of times the given pattern occurs in the text and locate listing the positions of the pattern in the text the authors noticed that the existing general solutions paying no attention to long repeats in the input are not very effective here and they proposed novel self indexes for the problem other full text indexes for repetitive_sequences were proposed inand kreft and navarro the former work presents two schemes one using an inverted index on q grams tailored to the repetitive nature of the input_data and the other being a grammar based index the latter paper introduced a self index based on the lz end which is an algorithm interesting in its own as it is an lz variant enabling efficient extraction of arbitrary phrases presented a scheme for referential compression of genomes working on the chromosome level if the pair of respective chromosomes is similar enough one of them is encoded with respect to the chromosome from the reference_sequence if not it is split into several pieces for which the best alignments are then found in the reference chromosome then longest common subsequences between matching parts are found and the differences encoded using huffman coding proposed a simple yet quite efficient compression scheme with random_access it is not an index though dubbed rlz std they choose one of the sequences as the reference_sequence and compress it with a self index in the cited work however a general_purpose compressor z with no random_access 
