fulcrum condensing redundant reads from high_throughput studies motivation ultra high_throughput produces duplicate and near duplicate_reads which can consume computational_resources in downstream applications a tool that collapses such reads should reduce storage and assembly complications and costs results we developed fulcrum to collapse identical and near identical illumina and reads such as those from pcr clones into single error corrected sequences it can process paired_end as well as single_end fulcrum is customizable and can be deployed on a single machine a local network or a commercially_available mapreduce cluster and it has been optimized to maximize ease of use cross_platform compatibility and future scalability sequence_datasets have been collapsed by up to and the reduced number and improved quality of the resulting sequences allow assemblers to produce longer contigs while using less memory availability_and source_code and a tutorial are available at http pringlelab stanford edu protocols html under a bsd like license fulcrum was written and tested in python and the single machine and local network modes depend on a modified_version of the parallel python_library provided ultra high_throughput uhts methods are being used both for resequencing projects and for de_novo sequencing and assembly of both genomes and transcriptomes during resequencing reads can generally be aligned to specific locations in the previously sequenced_genome in contrast de_novo often requires assemblers to take the input of many gigabases of sequence and return a much smaller result ranging from tens of megabases for transcriptome_assemblies to a few gigabases for many eukaryotic genome_assemblies some assemblers e g velvet store read information in ram during processing limiting the amount of raw sequence that can be processed uhts often produces reads that are exact duplicates of each other due to enzyme biases pcr_amplification or in transcriptomes the presence of highly_expressed sequences in addition genetic to whom correspondence should be addressed polymorphisms pcr errors and sequencing_errors can result in near duplicate_reads precluding the use of naive hashing algorithms to efficiently find and collapse all duplicates collapsing identical and near identical reads into single consensus reads with high quality_scores can significantly reduce the physical memory and or processing time required for assembly previously described methods for finding identical and near identical reads require large_amounts of physical memory long processing_times or both indeed if the number of reads n is large as is typically the case performing a full n n comparison on a single computer is effectively precluded by the amount of processing time required in addition these methods were designed to process single_end and cannot process the increasingly common paired_end in order to increase the number of reads that can be collapsed by the popular algorithm fastx collapse one could first preprocess the data with an error_correction program like shrec hybrid shrec or quake however shrec and hybrid shrec used several gb of ram just to process relatively small microbial datasets whereas quake required very large memories and extended times to process human datasets this need for computational_resources is expected because these programs are designed to solve the difficult problem of error_correction in non duplicated reads however substantially fewer resources are required if error_correction is only performed on near duplicate_reads and even the collapse of near duplicates can provide a significant performance advantage for subsequent assembly in the past_decade n n comparisons performed over multiple networked computers have been used to process very large_datasets such as in page ranking websites mapreduce has been helpful in solving these problems because of its power low_cost and ease of use in addition investigators without extensive computational_resources of their own can rent time from one of several commercial entities because of its wide utility we included support for mapreduce in fulcrum a pipeline for collapsing identical and nearidentical reads that can be run on a single machine a local network or a rented network of arbitrary size depending on the demands of the project 
