deliminate a fast and efficient method for loss less compression of genomic_sequences an unprecedented quantity of genome sequence_data is currently being generated using next_generation this has necessitated the development of novel bioinformatics approaches and algorithms that not only facilitate a meaningful analysis of these data but also aid in efficient compression storage retrieval and transmission of huge volumes of the generated data we present a novel compression algorithm deliminate that can rapidly compress genomic sequence_data in a loss less fashion validation results indicate relatively higher compression efficiency of deliminate when compared with popular general_purpose compression algorithms namely gzip bzip and lzma availability_and linux windows and mac implementations both and bit of deliminate are freely_available for download at http metagenomics atc tcs com compression deliminate contact the volume of sequence_data being deposited in major public sequence repositories is witnessing an exponential_rate of growth this presents an important challenge with respect to developing_efficient compression and storage methods addressing this challenge directly indirectly impacts bandwidth and cost related issues of data transmission dissemination major public sequence repositories store sequence_data either in its raw format as fastq_files or in a processed format as single multi fasta files this study pertains to compression of files containing nucleotide_sequences in single multi fasta_format such files contain information of sequences along with their corresponding headers four nucleotide_bases i e a t u g and c usually constitute the majority of text characters in the sequence portion of fasta files using a two bit encoding for these four characters is obviously the simplest way of reducing the file data size by a factor of four several studies see review by have proposed various approaches to further reduce the bits base ratio to less than while a majority of these approaches work by tracing and optimally encoding repeat patterns in genomic_data others work by capturing differences between various sequences constituting a multi fasta file a few reference based strategies have also been reported recently in spite of the availability of several specialized genome_compression algorithms sgcas which achieve reasonably high compression gains it is observed that the majority of public sequence repositories still usey general_purpose compression algorithms gpcas such as gzip and bzip for compressing and storing sequence_data the likely reasons for this observation are the following unlike gpcas most sgcas are not equipped to handle non atgc characters this limits their capability to perform a loss less compression decompression in contrast to gpcas the compression efficiency of most sgcas is generally observed to be achieved at the cost of huge time and memory_requirements the utility of reference based compression approaches is also subject to availability of closely_related reference_genomes in summary the above observations indicate the need for a compression algorithm and a practical implementation of the same that achieves loss less compression and decompression as rapidly as gpcas has significantly better compression efficiency than gpcas has low memory_requirements thus enabling it to handle files of any size and is compatible with popular software platforms unix linux windows etc and system architectures bit or bit in this article we present deliminate a combination of delta encoding and progressive elimination of nucleotide characters a novel method that implements the above_mentioned features of an ideal compression algorithm four different datasets fna ffn eukaryotic and nextgeneration_sequencing ngs dataset were used for evaluating the compression efficiency of deliminate details of these datasets are provided in supplementary_material files constituting these four datasets were compressed using deliminate both variants bzip gzip and lzma all experiments were performed on a linux workstation bit having a ghz dual core processor and gb ram the results obtained were compared in terms of percentage compression_ratio pcr and the time taken for compression and decompression pcr was calculated using the following formula pcr size of compressed dataset size of original dataset the results of both variants of deliminate in terms of pcr when compared with various gpcas are summarized in detailed results for individual files in all validation datasets for all methods are provided in supplementary tables the results indicate that both variants of deliminate achieve better compression_ratios when compared with other algorithms except for the ngs dataset all gpcas fail to achieve a compression_ratio lower than bits base in most cases in contrast for all datasets both variants of deliminate generate compression_ratios thus indicating significant compression gains files constituting the ngs dataset are observed to be highly compressible using gpcas this is expected since ngs_datasets are typically characterized by high sequencing_coverage and consequently end up having extensively repeated_sequence strings making them highly amenable to compression interestingly even for the ngs dataset both variants of deliminate are observed to significantly_outperform gpcas the percentage improvement in compression_ratio picr of delim i e the better performing deliminate variant when compared with gpcas was quantified using the following formula picr pcr of deliminate pcr of compared algorithm values of picr are summarized in supplementary the results in this table indicate that the compression_ratios obtained using delim are on an average better when compared with that obtained using gpcas in some cases the compression gains in terms of picr obtained using delim are observed to be as high as the performance efficiency of delim was also compared with two specialized genome_compression algorithms namely gencompress and xm compress details of the datasets used in this evaluation and a discussion of the results obtained are provided in supplementary_material the results with respect to time indicate that the compression time of delim is slightly_higher when compared with default gzip and bzip algorithms however it is observed that the compression speed of delim is around two to seven times_faster when compared with gzip with option i e the best compression mode and lzma algorithms although the decompression speed of delim is faster than its compression speed it is not as fast as the decompression speed of gpcas it is to be noted that the values of compression time provided for both variants of deliminate refer to the total time taken for compressing a fasta file which includes the final zip archiving step furthermore all values of compression and decompression time indicated incorrespond to the real or wall clock elapsed time not user system time although measuring real_time confers an advantage to programs such as deliminate both variants and zip which can simultaneously utilize or more available processing cores unlike single_cpu compression tools such as gzip and bzip this practice was adopted given that most of the present day workstations possess two or more processing cores in the present comparison both variants of deliminate including the piped calls to zip were run with two processing cores on another note lzmathe best performing gpca in terms of compression ratiouses a low amount of memory during compression default mode compared with deliminate it may appear that providing higher amount of memory would enable lzma to achieve better compression_ratio however results inindicate that no significant compression gains when compared with the default mode could be achieved by lzma even when gb of memory maximum possible allocation on the benchmarking system was allocated to it for compressing the validation datasets overall validation results suggest that both variants of deliminate especially delim are able to achieve significant_gains in compression_ratio as well as in processing time by providing genome_sequences represented in a unique partially compressed format as input to a general_purpose compression algorithm in order to verify as to what extent the splitting and transformation steps adopted by delim contribute to these compression gains two experiments were performed in the first experiment the f file obtained at the end of the first phase see_supplementary containing a single un split stream of a t g and c was converted to binary format bits for each nucleotide base and was provided as input to zip the results of this experiment shown in supplementary table under the head delim b demonstrate that the compression_ratio of delim and delim b are more or less comparable in the second experiment the bits corresponding to nucleotides at odd and even positions in the f file were split into two binary_data streams which were then compressed in parallel using zip the results obtained in this experiment provided inunder the head delim s also indicate that delim and delim s obtain similar compression levels however a comparison of compression time of delim delim s and delim b supplementary suggests that splitting the data streams and processing them using parallel threads as in delim and delim s positively impacts the overall time required for compression these results therefore imply that though the transformation steps adopted in delim do not help it attain any significant gain in compression_ratio compared with what could be attained by providing a homogeneous stream of four standard nucleotides to zip lzma they contribute to a faster compression process 
