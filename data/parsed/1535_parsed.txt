data_and effect of separate sampling on classification_accuracy motivation measurements are commonly taken from two phenotypes to build a classifier where the number of data_points from each class is predetermined not random in this separate sampling scenario the data cannot be used to estimate the class prior probabilities moreover predetermined class sizes can severely degrade classifier performance even for large samples results we employ simulations using both synthetic and real_data to show the detrimental_effect of separate sampling on a variety of classification rules we establish propositions related to the effect on the expected classifier error owing to a sampling ratio different from the population class ratio from these we derive a sample based mini max sampling ratio and provide an algorithm for approximating it from the data we also extend to arbitrary distributions the classical population_based anderson linear_discriminant minimax sampling ratio derived from the discriminant form of the bayes classifier availability all the codes for synthetic data and real_data examples are written in matlab a function called mmratio whose output is an approximation of the minimax sampling ratio of a given dataset is also written in matlab the medical_community is being confronted with serious problems of reproducibility in the development of biomarkers the issue has been highlighted by a recent report regarding comments by janet woodcock fda drug division head the report states based on conversations woodcock has had with genomics researchers she estimated that as much as percent of published biomarker associations are not replicable this poses a huge challenge for industry in biomarker identification and diagnostics development she said many issues affect reproducibility including the measurement platform specimen handling data_normalization and sample compatibility between the original and subsequent studies these matters concern experimental_procedures and are not our concern here rather we are interested in the methodology for designing classifiers one issue in this regard is the impact of inaccurate error estimation owing to small samples this has been previously quantified here we are interested in a different problem one that will confront us even if we have large samples and perfect error estimation the effect of having predetermined sample_sizes so that sampling is not random in classification studies it is typically a tacit assumption that sampling is random indeed it is commonplace for this assumption to be made throughout a text on classification for instance devroye et_al declare on page of their text that all sampling is random the assumption is so pervasive that it can be applied without mention with regard to the problem at hand state in typical supervised pattern classification_problems the estimation of the prior probabilities presents no serious difficulties but in fact there are often serious difficulties under the assumption of random_sampling the data_set is drawn independently from a fixed distribution of feature label pairs x y in particular this means that if a sample of size n is drawn for a binary classification_problem then the numbers of sample points n and n in classes and respectively are random_variables such that n n n an immediate consequence of the random_sampling assumption is that the prior probability c pr y can be consistently estimated by the sampling ratio namely by c n n consistency is nothing but bernoullis weak law of large_numbers namely n n c in probability thus if the sample is large we can expect the sampling ratio to be close to the prior probability suppose the sampling is not random in the sense that the ratios n n and n n are chosen prior to sampling in this separate stratified_sampling case s n s n s n where the sample points in s n and s n are selected_randomly from and but given n the individual class counts n and n are not random then in effect we have no sensible estimate of c one could let c n n but there would be no reason to do so since our aim is to use the data to train a classifier does the inability to consistently estimate c matter clearly in the case of linear_discriminant lda it does since the lda classifier is defined by n x if d samp x and n x if d samp x whereand l and l are the sample means of the class conditional populations and respectively and d is the pooled sample covariance_matrix the rationale for the lda discriminant is that the estimators converge to the population parameters to whom correspondence should be addressed as n in which case the resulting discriminant d bayes x defines the bayes optimal classifier in the two class gaussian_model with common covariance_matrix it is obvious from equation that an estimate of c is required for lda and a bad choice of c will negatively impact the classifier this fact which is a consequence of separate sampling has long been recognized the situation is less transparent with model free classification rules such as support_vector in this article we use simulation to study the effect of separate sampling on several different classification rules where the role of c does not appear explicitly in classifier learning we generate separate samples with different ratios r n n and consider the expected error e n jr of the designed classifier given r where the error of classifier n is defined by n pr n x y the probability of misclassification we will see that the penalty for separate sampling without knowledge of c can be severe with random or mixed sampling rather than being fixed prior to sampling r is a sample dependent random variable in this case e n jr denotes the expectation of the error conditioned on r and the expected classification error is given by e n e r e n jr where the outer expectation is relative to the distribution of r the classifier error is likely to be smaller when the sampling ratio r is close to c hence if one happens to fix r sufficiently close to c then e n jr e n because r c in probability as n for mixed sampling as n gets larger the distribution of r gets more tightly concentrated around c so that the distribution of e n jr as function of r gets more tightly_packed around e n which in turn means that to have e n jr e n one must choose r very close to c to illustrate this phenomenon consider d gaussian class conditional densities with means at and possessing common covariance_matrix i where i is the identity matrix and and with c for this model the bayes error is bayes shows the difference e n e n jr for different values of r and different sample_sizes when using lda if r then e n jr e n for all n if r which is fairly close to c then e n jr e n for n notice the lack of symmetry both and being equally close to c this should not be surprising because we should not expect the distribution of e n jr to be symmetric let us examinefrom the practitioners perspective suppose that cost limits the sample to a given size n if the sample is random then the expected error of the designed classifier will be e n which is unknown since the feature label distribution is unknown consider three cases i if c is accurately known from existing population statistics regarding the two classes say brca_and breast_cancer then no matter what the sample_size it is best to do separate sampling with n cn ii if c is approximately known meaning that the practitioner believes that c is close to c then for small n it may be best or at least acceptable to do separate sampling with n c n and the results will likely still be acceptable for large n though not as good as with random_sampling iii if the practitioner has no idea what c is then sampling must be random because the penalty for separate sampling can be very large while at this point these comments refer specially to which is for lda a salient point to be made in this article is that they are quite general and moreover can be extended to the commonplace separate sampling situation where one cannot choose n and n why is all of this a major_issue for bioinformatics simply put separate sampling is ubiquitous in bioinformatics in particular with genomic classification where a standard approach is to take tissue_samples from two classes say different types of cancer or different stages of cancer for which the number of specimens in each class is not chosen randomly and then to design a classifier the supplementary_material lists published_studies using separate sampling in each case we give the classification_problem sample_sizes classification rule and error estimator even if an error estimate is exact for the problem at handthat is for the sampling ratio represented by the datawhat does it mean relative to the classification error for future observations say patients that depends on the true prior probabilities which we do not know 
