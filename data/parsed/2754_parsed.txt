sequence_analysis lossy_compression of quality_scores in genomic_data motivation next_generation are revolutionizing medicine data from sequencing_technologies are typically represented as a string of bases an associated sequence of per base quality_scores and other metadata and in aggregate can require a large amount of space the quality_scores show how accurate the bases are with respect to the sequencing_process that is how confident the sequencer is of having called them correctly and are the largest component in datasets in which they are retained previous_research has examined how to store sequences of bases effectively here we add to that knowledge by examining methods for compressing quality_scores the quality values originate in a continuous domain and so if a fidelity criterion is introduced it is possible to introduce flexibility in the way these values are represented allowing lossy_compression over the quality_score data results we present existing compression options for quality_score data and then introduce two new lossy techniques experiments measuring the trade_off between compression_ratio and information_loss are reported including quantifying the effect of lossy representations on a downstream application that carries out single nucleotide poly morphism and insert deletion detection the new methods are dem onstrably superior to other techniques when assessed against the spectrum of possible trade_offs between storage required and fidelity of representation availability_and an implementation of the methods described here is available at https github com rcanovas genome_sequencing methods have evolved at the same astonishing rate as computing technology next generation devices parallelize the process producing billions of sequences reads and generating file sizes potentially in the terabyte range at costs that are decreasing on a yearly basis each read is a fragment of data extracted from the processing of a single_genome represented as a string of bases as well a number of metadata fields are associated with each read some of these fields are more expensive to store than the sequence of bases the mechanics of effectively storing and querying this information are fundamental to the field of bioinformatics several standard formats to store genome data have been adopted each aiming to be easy to manipulate and parse using text processing_tools such as perl and python the most common are the fasta and fastq formats and the sam bam formats we have described and measured the performance of a range of lossy_compression techniques for quality_scores the qualcomp approach and our two new methods offer superior trade_off options when fidelity is assessed according to the measures in our new approaches outperformed qualcomp in the max min distance criteriona measure that is well suited we believe to bioinformatics applications our experiments also quantified the extent to which use of lossy_compression affected the performance of a typical downstream application of genetic data and demonstrate that variation detection can still be reliably carried_out even with relatively compact storage of the quality_scores the approaches described here make use of sequentially greedy generation of blocks one interesting question that we have not yet examined is whether mechanisms for globally optimal block construction will make a measurable difference in overall outcomes another area that we have not yet fully explored is the coding mechanisms used for the block representatives and for the block lengths it may be that tailored codes rather than binary and gamma can offer further space savings once the particular characteristics of the data streams are taken into account we are also interested in quantifying the effect that lossy_compression has on other downstream applications of genetic sequencing_data 
