gene_expression improved mean estimation and its application to diagonal discriminant_analysis motivation high_dimensional such as microarrays have created new challenges to traditional statistical_methods one such example is on class_prediction with high_dimension low sample_size data due to the small_sample the sample mean estimates are usually unreliable as a consequence the performance of the class prediction_methods using the sample mean may also be unsatisfactory to obtain more accurate estimation of parameters some statistical_methods such as regularizations through shrinkage are often desired results in this article we investigate the family of shrinkage estimators for the mean value under the quadratic loss function the optimal shrinkage parameter is proposed under the scenario when the sample_size is fixed and the dimension is large we then construct a shrinkage based diagonal discriminant rule by replacing the sample mean by the proposed shrinkage mean finally we demonstrate via simulation_studies analysis that the proposed shrinkage based rule outperforms its original competitor in a wide_range of settings with the advent_of technologies learning highdimensional complex models is critical in many disciplines such as biology genetics epidemiology geology ecology neurology and engineering one such example is microarray_data where the expression levels of thousands of genes are measured simultaneously from each sample due to the cost and or other experimental_difficulties such as the availabilities of biological_materials it is common that high_throughput data are collected only in a limited number of samples they are referred to as high_dimensional with small_sample or large g small n data where g is the number of dimensions and n is the sample_size highdimensional data pose many challenges to traditional statistics methods specifically due to the small n there are more uncertainties associated with standard estimations of parameters such as the mean and variance estimations as a consequence statistical_analyses based on such parameter_estimation are usually unreliable to obtain to whom correspondence should be addressed more accurate estimation of parameters some statistical_methods such as regularizations through shrinkage may yield better results shrinkage based_methods have been proposed in recent_years to improve the variance estimation for large g small n data see for example storey and wright and simon smyth tong and opgen rhein and strimmer and among many others in contrast to the advances on variance estimation little attention has been paid to improving the mean estimation for high_dimensional until recently in this article we investigate the family of shrinkage estimators for the mean value which is tailored to the high_dimensional such as microarrays specifically we will propose the optimal shrinkage parameter under the quadratic loss function for the data when g tends to be infinite class_prediction with high_dimensional has been recognized as a very important problem and received much attention in different fields such as genomics proteomics brain_images medicine and machine_learning for high_dimensional with small_sample it is known that the traditional classification methods such as the linear_discriminant are not applicable as the sample covariance is going to singular when g is greater than n to overcome the singularity problem introduced two diagonalized discriminant rules the diagonal linear_discriminant dlda and the diagonal quadratic_discriminant dqda when the sample_size is small dlda performed remarkably well compared with more sophisticated classifiers in terms of both accuracy and stability in addition dlda is easy to implement and is not sensitive to the number of predictor_variables though dlda performed well for high_dimensional small_sample data there is still room to improve it in particular we notice that the mean estimation the sample mean in dlda will be unreliable when the sample_size is not sufficiently_large as a consequence the performance of dlda may also be unsatisfactory with this insight we propose in this article an improved version of dlda which replaces the sample mean by the optimal shrinkage estimator we expect that the proposed shrinkage based dlda will improve the classification_accuracy in practice the remainder of the article is organized as follows in section we investigate the family of shrinkage estimators for the mean value under the quadratic loss function with the nature of highdimensional data we assume that the variances are unequal and unknown under regularity conditions we discuss the choices of thepage shrinkage estimation of means has a long history starting with the seminal paper of james andwher is the pooled estimator of more generally when is non diagonal and unknown the james stein type estimator has the form is the pooled sample covariance_matrix to guarante non singular we require that n g note that this is not the case for high_dimensional where g can be much larger than n therefore the existing shrinkage methods for estimating break down and cannot apply to the high_dimensional directly in this article we proposed an optimal shrinkage estimator for the mean value under the large g small n scenario we then applied the proposed shrinkage estimator to high_dimensional classification_problem by constructing a shrinkage based diagonal discriminant rule its improvement over the original competitor was demonstrated through both simulations and real_data analysis though the independence_assumption in this article is popular in the literature it is unlikely to be true in practice and so certain remedy might be necessary for a further improvement when additional information is available langaas page t suggested that the clumpy dependence is a likely form of dependence where the clumpy dependence means that the genes are dependent within groups and independent among groups inspired by that one natural extension would be to propose new shrinkage estimators for the mean value under the clumpy dependence_structure to avoid the singularity problem we might need to assume that the largest group size is not larger than the number of samples another future work is to examine if the proposed optimal shrinkage estimator has any good in its own right or if it can be further improved by its positive part estimator finally we note that the proposed smdlda in this article is a shrinkage mean only based dlda whereas inthe authors proposed a shrinkage variance only based dlda as both the mean and variance estimations are crucial in the statistical_analysis further research might be needed to develop new classification rules that shrink both the mean value and the variance possible approaches can be either by plugging in the existing shrinkage estimators respectively or by proposing new shrinkage estimators for the mean value and variances simultaneously finally by minimizing the above quantity we have 
