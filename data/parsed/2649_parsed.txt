sequence_analysis kmc fast and resource frugal k_mer motivation building the histogram of occurrences of every k symbol long substring of nucleotide data is a standard step in many bioinformatics applications known under the name of k_mer its applications include developing de_bruijn genome_assemblers fast multiple_sequence and repeat detection the tremendous amounts of ngs_data require fast algorithms for k_mer preferably using moderate amounts of memory results we present a novel method for k_mer on large_datasets about twice faster than the strongest competitors jellyfish kmc using about gb or less of ram our disk based method bears some resemblance to mspkmercounter yet replacing the original minimizers with signatures a carefully_selected subset of all minimizers and using k x mers allows to significantly reduce the i o and a highly_parallel overall architecture allows to achieve unprecedented processing speeds for example kmc counts the mers of a human reads collection with fold coverage gb of compressed size in about min on a core intel i pc with an solid_state disk availability_and kmc is freely_available sun aei polsl pl kmc one of common preliminary steps in many bioinformatics algorithms is the procedure of k_mer this primitive consists in counting the frequencies of all k long strings in the given collection of sequencing_reads where k is usually more than and has applications in de_novo using de_bruijn correcting reads and repeat detection to name a few areas more applications can be found e g in with references therein k_mer is arguably one of the simplest both conceptually and programmatically tasks in computational_biology if we do not care about efficiency the number of existing papers on this problem suggests however that efficient execution of this task with reasonable memory use is far from trivial the most successful of early approaches was jellyfish maintaining a compact hash_table ht and using lock free operations to allow parallel updates the original jellyfish version as presented in marais and required more than gb of memory to handle human_genome data with fold coverage bfcounter employs the classic compact data_structure bloom_filter bf to reduce the memory_requirements due to preventing most single occurrence k_mers which are usually results of sequencing_errors and for most applications can be discarded from being added to an ht although bf is a probabilistic mechanism bfcounter applies it in a smart way which does not produce counting errors dsk and kmc are two disk based_algorithms on a high_level they are similar and partition the set of k_mers into disk buckets which are then separately processed dsk is more memory frugal and may process human_genome data in as little as gb of ram whereas kmc is faster but typically uses about gb of ram turtle bears some similarities to bfcounter the standard bf is there replaced with its cache friendly variant and the ht is replaced with a sorting and compaction algorithm which accidentally resembles a component of kmc apart from adding parallelism and a few smaller modifications finally mspkmercounter is another disk based algorithm based on the concept of minimizers described in detail in the next section in this article we present a new version of kmc one of the fastest and most memory_efficient programs the new release borrows from the efficient architecture of kmc but reduces the disk usage several times sometimes about times and improves the speed usually about twice in consequence our tests show that kmc is the fastest by a far margin algorithm for counting k_mers with even smaller memory_consumption than its predecessor there are two main ideas behind these improvements the first is the use of signatures of k_mers that are a generalization of the idea of minimizers signatures allow significant reduction of temporary disk_space the minimizers were used for the first time for the k_mer in mspkmercounter but our modification significantly_reduces the main memory_requirements up to times and disk_space about times when compared with mspkmercounter the second main novelty is the use of k x mers x for reduction of the amount of data to sort simply instead of sorting some amount of k_mers we sort a much smaller portion of k x mers and then obtain the statistics for k_mers in the post_processing phase the implementation of kmc was compared against the best in terms of speed and memory_efficiency competitors jellyfish which is significantly more efficient than the version described in dsk turtle mspcounter kanalyze and kmc each program was tested for two values of k and and in two hardware configurations using conventional hard disks hdd and using a solid_state disk ssd we used several datasets of varying_size two of them are human data with large coverage the experiments were run on a machine equipped with an intel i cpu cores clocked at ghz gb ram and hdds tb each in raid and single ssd tb some of the experiments were also run on a single hdd tb the programs were run with the number of threads equal to the number of virtual cores to achieve maximum speed in the experiments we count only k_mers with counts at least since the k_mers with a single occurrence in a read collection most likely contain erroneous base s as in some applications all k_mers may be needed we ran a preliminary kmc test in such setting with a ssd we found out that the overhead in computation time is only up to mainly caused by increased i o the comparison presented in tables and supplementary tables s and s includes total computation time in seconds maximum ram use and maximum disk use ram and disk use are given in gbs gb b time is wall clock time in seconds a test running longer than h was interrupted other reasons for not finishing a test were excessive memory_consumption limited by the total ram i e gb or excessive disk use over gb chosen for our tb ssd disk note that the largest input dataset homo_sapiens occupies gb on the same disk jellyfish was tested twice in the default and the bf based mode with exact counts unfortunately in the latter experiments the amount of memory in our machine was often not enough and this is why jellyfish bf results are shown only for two datasets several conclusions can easily be drawn from the presentedthe slowest for this reason kanalyze was tested only on the ssd kanalyze also uses a large amount of temporary disk_space which was the reason we stopped its execution on the two human datasets for k only as kanalyze does not support large values of k mspkc on the other hand theoretically allows the parameter k to exceed but in none of our datasets it finished its work for k for the smallest dataset f vesca it failed probably because of variable_length reads on the other datasets we stopped it after more than h of processing the only asset of kanalyze and mspkc we have found is their moderate memory use dsk is not very fast either still it consistently uses the smallest amount of memory gb was always reported and is quite robust as it passed all the tests jellyfish in its default mode is not very frugal in memory use and this is the reason on our machine it passed the test for k only for two datasets f vesca and m balbisiana still for k it passed all the tests being one of the fastest programs often outperforming kmc turtle is rather fast as well slower than jellyfish though but even more memory hungry we could not have run it on the two largest datasets turtle and jellyfish are memory only algorithms all the other ones are disk based this is the reason why changing hdd to a much faster ssd does not affect the performance of these two counters significantly yet it is non zero due to faster input reading from the ssd kmc on the ssd was tested three times for each k with standard memory use gb and with memory use reduced to gb suggested limit and to gb strict limit we note that reducing the memory even to gb only moderately increases the processing time it is worth to note that both kmc and dsk can be run with even lower memory limits i e about gb but it comes at a price of speed drop for experiments we however chose larger settings as gb of ram seems to fit even low end machines kmc with its standard memory use is a clear winner in processing time on the human datasets being about twice faster than jellyfish or kmc these speed differences concern the ssd experiments as on the hdd the gap diminishes but is still significant this can be explained by i o especially reading the input_data being the bottleneck in several phases of kmc processing it is worth examining how switching a conventional disk to a ssd affects the performance of disk based software it might seem natural that the biggest time reduction in absolute time not percentage gain should be seen in those programs which use more disk_space to some degree it is true e g kmc gains more than kmc but dsk is a counter example e g on h sapiens it gains as much as s which is almost seven times the reduction for kmc seemingly surprising as dsk uses less disk_space yet a probable explanation is that dsk works in several passes so its total i o is actually quite large for large_datasets interestingly for disk based_algorithms the disk use of kmc is typically reduced when switching from k to k this can be explained by a smaller number of k_mers per read and in case of kmc also by a smaller number of super k_mers per read to check if the ssd disk with about mb s read write performance may still be a bottleneck we ran kmc also in the inmemory mode the memory_consumption then grows to about the sum of memory and disk use in the standard_setting yet the processing time improves by about for g gallus and for m balbisiana this shows that even with the ssd disk the performance is somewhat hampered by i o operations we also measured how the input format raw gzipped and media one or two hdds in raid ssd affects the performance of our solution on the largest dataset h sapiens as expected using the ssd reduces the time by and reading the input from compressed form also has a visible positive impact we note in passing that replacing gzip with e g bzip results not shown here would not be a wise choice since the improvement in compression cannot offset much slower bzip s decompression compares signatures with minimizers on g gallus we can see that using our signatures diminishes the average number of super k_mers in a read by about percent also the number of k_mers in the largest disk bin is significantly_reduced sometimes more than twice these achievements directly translate to smaller ram and disk_space consumption how k x mers affect bin processing is shown infor two datasets it is easy to see that the number of strings to sort is more than halved for x yet the speedup is more moderate due to the extra split phase i e extracting k x mers from super kmers and sorting over longer strings still k mers versus plain kmers reduce the total time by more than and even for h sapiens and k the impact of k on processing time and disk_space is presented in figures and respectively longer k_mers result in even longer super k_mers which minimizes i o but makes the sorting phase longer for this reason the disk_space consumption shrinks smoothly with growing k but the effect on processing time is not so clear still counting k_mers for k is generally slower than for smaller values of k from we can see that using more memory accelerates kmc but the effect is mediocre only about speedup when raising the memory_consumption from to gb the reasons behind the speedup are basically fold i the extra ram allows to use a larger number of sorter threads which is more efficient than few sorters with more internal threads per sorter and ii occasional large bins disallow to run other sorters at the same time if memory is limited finally we analyze the scalability and cpu load of our software as expected the highest speed is achieved when the number of threads matches the number of virtual cpu cores still the time reduction between and threads is only by factor or less when the input_data are in non compressed fastq using theavg in read is the average number of super k_mers per read no k_mers largest bin is the number in millions of k_mers in the largest bin min memory is the amount of memory in gbytes necessary to process the k_mers in the largest bin i e the lower_bound of the memory_requirements the size of temporary disk_space is determined by the average number of minimizers signatures in a read for example the disk_space requirements for minimizer signature length are gb signatures k and gb minimizers k a gb ram set gzipped input sorted fraction is the ratio of the number of k x mers to the number of k_mers for h sapiens the largest bin was too large to fit the assumed amount of ram in two cases and the ram consumption of kmc was gb for mers gb for mers gb for mers and gb for mers compressed input broadens the gap to factor for k and for k the corresponding gaps between and threads i e equal to the number of physical cores are and k and k with non compressed input and and k and k with gzipped input the latter experiment tells more about the scalability of our tool since the performance boost from intel hyper threading technology can be hard to predict varying from less than to about in real code although the dominating trend in it solutions nowadays is the cloud the progress in bioinformatic algorithms shows that even home computers equipped with multi core cpus several gigabytes of ram and a few fast hard disks or one ssd disk get powerful enough to be applied for real omics tasks if their resources are loaded appropriately the presented kmc algorithm is currently the fastest k_mer counter with modest resource memory and disk requirements although the used approach is similar to the one from mspkmercounter we obtain an order of magnitude faster processing due to the following kmc features replacing the original minimizers with signatures a carefully_selected subset of all minimizers using k x mers and a highly_parallel overall architecture as opposed to most competitors kmc worked stably across a large range of datasets and test settings in real numbers we show that it is possible to count the mers of a human reads collection with fold coverage gb of compressed size in about min on a core intel core i pc with an ssd with enough amounts of available ram it is also possible to run kmc in memory only in our preliminary tests it gave rather little compared with an ssd about speedup but may be an option in datacenters with plenty of ram but possibly using network hdds with relatively low transfer in this scenario a memory only mode should be attractive after our work was ready we learned about an interesting possibility of using frequency based minimizers the idea is to select the globally least frequent m mer in a given k_mer and it dramatically_reduces the memory use in the application of enumerating the maximal simple paths of a de_bruijn in our preliminary_experiments freq based minimizers reduce the memorytimek k dependence of kmc processing time on maximal available ram and type of disk for h sapiens dataset there are results for k and gb ram these results are for set gb gb gb gb as maximal ram usage however the largest bin enforced to spend at least gb of ram 
