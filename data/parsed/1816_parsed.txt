sequence_analysis filtering duplicate_reads from pyrosequencing_data motivation throughout the recent_years pyrosequencing has emerged as an efficient alternative to traditional_sanger and is widely used in both de_novo whole_genome and metagenomics especially the latter application is extremely sensitive to sequencing_errors and artificially duplicated reads both are common in pyrosequencing and can create a strong bias in the estimation of diversity and composition of a sample to date there are several tools that aim to remove both sequencing noise and duplicates nevertheless duplicate removal is often based on nucleotide_sequences rather than on the underlying flow values which contain additional information results with the novel tool jatac we present an approach towards a more accurate duplicate removal by analysing flow values directly making use of previous_findings on flow data characteristics we combine read clustering with bayesian distance measures finally we provide a benchmark with an existing algorithm when life_sciences now roche diagnostics released the gs sequencing_platform in it was the start of a revolution in sequencing_technology it has since been followed by other platforms both subsequent generations from and competing technologies like illumina solexa and abi solid the increased throughput and decreasing per base cost of these second_generation have made high_throughput an affordable tool for many new organisms and applications the traditional_sanger is now years old and the error characteristics and artifacts intrinsic to the method are well characterized consequently there are established methods for describing sequence quality standard methods and tools for detecting and dealing with common contamination like vector sequences or genomic contamination exist some of them applicable to one or several second_generation experienced researchers will also be aware of the risk of artifacts like chimeric sequences arising through different mechanisms there are numerous approaches to the removal or correction of erroneous sequences or parts of sequences for different applications these are especially tailored to metagenomics but also to snp_detection small_rna discovery and so forth some of them using pyrosequencing flow data instead of nucleotide_sequences with good results in this article we have quantified the room for improvements when filtering pyrosequencing shotgun data for artificial duplicates we have successfully shown that by the use of flow data a higher rate of artificial duplicates can be identified than by using sequence_data only artificially duplicated reads canapart from a generally higher processing and memory requirementlead for example to incorrect conclusions about metagenomic_dataset composition or to biased quantification in digital karyotyping experiments another likely problem could be false_positive single_nucleotide calls in the presence of duplicated erroneous sequences however too stringent filtering might lead to an underestimation of abundance both jatac and cd_hit cannot distinguish natural from artificial duplicates but the percentage of natural duplicates can be estimated from sequencing_coverage by calculating the probability of multiple reads randomly starting at the same position although cd_hit s estimated duplicate rates were comparable with jatacs estimations the calculated cluster composition at similar duplication rates was of lower_quality manifested in a lower jaccard_index this is likely the result of jatac being better at handling homopolymer discrepancies and taking flow order into account whereas cd_hit is operating mostly on global similarity_scores the distance calculation in jatac is a more robust way of finding duplicates as it first identifies read_pairs with different homopolymer lengths at low distances only with higher distance thresholds reads with substitutions are taken into account this behaviour closely models the sequencing chemistry where substitution_errors are less common than indels interestingly the jaccard_index calculated from running cd_hit on the d labrax dataset degraded much faster around the true duplicate rate when compared with jatac this degradation could not be observed in the bacterial datasets and is likely due to a higher probability of matching unrelated sequences from a complex background this phenomenon could also be relevant to metagenomic experiments of highly_diverse communities where tools such as cd_hit and jatac are most useful a comprehensive_overview of comparison of jatac and cd_hit duplicate clustering at different stringency settings and estimated duplicate rates surrounding the true duplicate rate vertical grey line the range of parametrization lies between and distance threshold for jatac and between and for cd_hit identity threshold left e coli run centre e coli run right d labrax for explanation of a b and c pairs see the text applications and effects of duplicate filtering e g on genome_assembly can be found in jatacs improved duplicate identification comes at a computational price and its speed depends on the number of reads and the degree of duplication jatac takes up to several hours to filter an sff file for duplicates h for a typical gs junior run we have also evaluated jatac on iontorrent flow data as both platforms share the same data format sff although it is in principle possible to analyse ionograms using jatac the underlying flow data model has been optimized for pyrosequencing_data which is why we do not recommend jatac for iontorrent data in its present version 
