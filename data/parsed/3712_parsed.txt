gene_expression probabilistic pca of censored_data accounting for uncertainties in the visualization of high_throughput single_cell motivation high_throughput single_cell quantitative real_time poly merase chain reaction qpcr is a promising technique allowing for new insights in complex_cellular however the pcr_reaction can be detected only up to a certain detection_limit whereas failed reactions could be due to low or absent expression and the true expression_level is unknown because this censoring can occur for high proportions of the data it is one of the main challenges when dealing with single_cell principal_component pca is an important tool for visualizing the structure of high_dimensional as well as for identifying subpopulations of cells however to date it is not clear how to perform a pca of censored_data we present a probabil istic approach that accounts for the censoring and evaluate it for two typical datasets containing single_cell results we use the gaussian_process latent_variable model framework to account for censoring by introducing an appropriate noise model and allowing a different kernel for each dimension we evaluate this new approach for two typical qpcr datasets of mouse_embryonic and blood stem_progenitor respectively by performing linear_and probabilistic pca taking the censoring into account results in a d representation of the data which better reflects its known structure in both datasets our new approach results in a better separation of known cell_types and is able to reveal subpopulations in one dataset that could not be resolved using standard pca availability_and the implementation was based on the existing gaussian_process latent_variable model toolbox https github com sheffieldml gpmat extensions for noise models and kernels accounting for censoring are available at http icb helmholtzconventional approaches for pca of censored_data where values beyond the detection_limit are substituted with the detection_limit can yield strongly biased results we have proposed a novel approach for performing dual pca of censored_data our new approach resulted in a mapping between low_dimensional and high dimensional_space such that more censored data_points were mapped correctly to values greater than the detection_limit it was previously shown that for single_cell it is crucial to explicitly_model the population of non detects when performing a statistical_test of univariate differential_expression to date no approaches for dealing with this issue for multivariate_analyses such as pca have been proposed we evaluated our new approach for two different real_world datasets comprising measurements of single_cell for both datasets the pca representations better reflected the known structure of the data when the censoring was explicitly considered we evaluated using a linear as well as a non linear kernel and for both datasets accounting for non linearities resulted in better visualizations in contrast to using a linear kernel i e pca this comes at the price of losing interpretabilityalthough in the linear case loadings can be easily visualized in a bi plot in the non linear case this is more difficult because loadings change across the d plot whether trading off interpretability for complexity is beneficial depends highly on the dataset under consideration and any non linearities present in the context of single_cell our analyses suggest that a non linear kernel is necessary to capture the typical complex dependency_structure of such data for linear kernels corresponding to standard pca as well as for non linear kernels allowing for interactions our new approach yielded considerably_lower nearest_neighbour error_rates with reductions of up to in the linear case furthermore in the case of mesc data the structure of subpopulations was reflected better in the case when censoring was taken into account in the non linear case in contrast to nonlinear probabilistic pca with the substitution approach two subpopulations corresponding to cells from the cell stage with high id expression and cells in the icm with high fgf_expression could be identified these known subpopulations were previously_identified in a univariate_analysis of cells from the same cell stage however this standard approach has several drawbacks because it can become unfeasible when too many genes are measured simultaneously furthermore only univariate patterns can be identified whereas important information may lie in multivariate patterns which could be defined by the differential_expression of several genes finally when analysing univariate distributions or correlations between two genes for cells from the same cell stage the identified subpopulations cannot be put in context with other cells from other cell stages in contrast when performing a probabilistic kernel pca of all cell stages it is possible to identify complex multivariate subpopulations and by simultaneously displaying all cells the pca plot provides an intuitive illustration of the relation between all cell_populations this was achieved by implementing a gplvm with different kernels for each dimension censoring was accounted for by a probit noise_level the steepness parameter of the probit function was learnt together with other kernel parameters resulting in a parameter free approach for pca of censored_data although our approach was designed for accounting for uncertainties in single_cell related issues can be found in single_cell in contrast to single_cell qpcr however high levels of technical noise are present in all commonly used protocols for single_cell this technical noise is particularly strong for low levels of expression and dominates all other uncertainties like censoring although these uncertainties are inherently different from the censoring found in single_cell qpcr the flexible framework of gaussian processes allows us to account for these uncertainties in a straightforward manner by using an additional term in the gaussian_noise model reflecting the technical noise which can be estimated using the approach suggested by although single_cell are generated in the form of read_counts it is crucial to perform normalization steps accounting for different cell_sizes different sequencing_depth and depending on the protocol different transcript lengths such normalization can be performed by calculating reads per kilo base per million rpkm and fragments per kilobase of exon per million fragments mapped fpkm or using deseq inspired normalization procedures after normalization gene_expression is measured on a continuous scale such thatafter an appropriate variancestabilizing transformation e g log transformation gplvm can be applied without modification because efficient implementations allow fast processing of datasets with tens_of of genes and hundreds of cells without overfitting it is a promising tool for analysing such datasets the main_drawback of our proposed approach is that it scales cubically with the number of cells which may be prohibitive when the number of analysed cells is very large although standard gplvms are time consuming too significant speed ups can be achieved because of sharing the kernel across all dimensions and using a spherical noise model however if necessary approximations resulting in sparse covariance_matrices commonly used in gaussian_process literature could be applied for our framework too for the application to single_cell we found that this was not necessary because computation times were in the order of only a few hours on a standard laptop we acknowledge that this is a considerable increase of time compared with standard pca which can be performed when using the substitution approach to deal with censored_data in applications with only a small fraction of taking censoring into account with probit noise model and fixed e and probit noise model with learnt from data f gj gplvm with rbf kernel for blood data standard gplvm with substitution approach g taking censoring into account with probit noise model and fixed h and probit noise model with learnt from data i the background intensity indicates the relative uncertainty of the mapping with black pixels corresponding to the highest uncertainty of the mapping censored data_points this rather large increase in runtime may result in only minor changes of the pca representation and simpler approaches such as the substitution approach or treating the data as missing may be a valid alternative if runtime is an issue however in the case of single_cell we have shown that taking censoring into account avoids a potential_bias in low_dimensional representations due to the censoring this in turn can result in better biological_insights first our approach can yield a better separation of different cell_types second it may even reveal new biologically_meaningful subpopulations that may be obscured because of a bias introduced by the censoring when designing single_cell qpcr experiments the quantification of heterogeneities and the reliable identification of new subpopulations are often key goals that is why we believe that our approach will be of interest for many practitioners working with censored_data especially in the field of high_throughput singlecell qpcr we have presented a new approach for performing probabilistic pca for censored_data within the framework of gplvms therefore we implemented an appropriate noise model and allowed different kernels for each dimension we showed that for single_cell with a high fraction of censored data_points the resulting probabilistic kernel pca representations reflected the true structure of the data better than conventional approaches in two real_world datasets known cell_types could be better separated when censoring was taken into account and in one dataset several distinct_subpopulations could be revealed that could not be resolved with standard pca 
