hapcol accurate and memory_efficient haplotype_assembly from long_reads motivation haplotype_assembly is the computational_problem of reconstructing haplotypes in dip loid organisms and is of fundamental_importance for characterizing the effects of single_nucleotide on the expression of phenotypic_traits haplotype_assembly highly benefits from the advent of future generation sequencing_technologies and their capability to produce long_reads at increasing_coverage existing_methods are not able to deal with such data in a fully satisfactory way either because accuracy or performances degrade as read_length and sequencing_coverage increase or because they are based on restrictive assumptions results by exploiting a feature of future generation technologiesthe uniform_distribution of sequencing errorswe designed an exact algorithm called hapcol that is exponential in the maximum number of corrections for each single_nucleotide position and that minimizes the overall error_correction score we performed an experimental analysis comparing hapcol with the current state of the art combinatorial methods both on real and simulated_data on a standard benchmark of real_data we show that hapcol is competitive with state of the art methods improving the accuracy and the number of phased positions furthermore experiments on realistically simulated_data revealed that hapcol requires significantly less computing_resources especially memory thanks to its computational_efficiency hapcol can overcome the limits of previous_approaches allowing to phase datasets with higher_coverage and without the traditional all heterozygous assumption diploid organisms such as humans contain two sets of chromosomes one from each parent reconstructing the two distinct copies of each chromosome called haplotypes is crucial for characterizing the genome of an individual the process is known as phasing or haplotyping and the provided information may be of fundamental_importance for many applications such as analyzing the relationships between genetic_variation and gene function or between genetic_variation and disease_susceptibility in diploid species haplotyping requires v c the author published_by all_rights for permissions please_e journals permissions_oup com assigning the variants to the two parental copies of each chromosome which exhibit differences in terms of single_nucleotide snps since a large_scale direct experimental reconstruction of the haplotypes from the collected_samples is not yet cost_effective a computational approachcalled haplotype assemblythat considers a set of reads each one sequenced from a chromosome copy has been proposed reads also called fragments have to be assigned to the unknown haplotypes using a reference_genome in a preliminary mapping phase if available this involves dealing in some way with sequencing and mapping errors and leads to a computational task that is generally modelled as an optimization_problem minimum error_correction mec is one of the prominent combinatorial approaches for haplotype_assembly it aims at correcting the input_data with the minimum number of corrections to the snp values such that the resulting reads can be unambiguously partitioned into two sets each one identifying a haplotype wmec is the weighted variant of the problem where each possible correction is associated with a weight that represents the confidence degree assigned to that snp value at the corresponding position this confidence degree is a combination of the probability that an error occurred during sequencing phred based error probability for that base call and of the confidence of the read_mapping to that genome position the usage of such weights has been experimentally_validated as a powerful way to improve accuracy haplotype_assembly benefits from technological_developments in genome_sequencing in fact the advent_of ngs_technologies provided a cost_effective way of assembling the genome of diploid organisms however to assemble accurate haplotypes it is necessary to have reads that are long enough to span several different heterozygous positions this kind of data is becoming increasingly available with the advent of future generation sequencing_technologies such as single_molecule real_time technologies like pacbio rs ii http www pacificbiosciences com products and oxford nanopore flow cell technologies like minion https www nanoporetech com these technologies thanks to their ability of producing single_end longer than bases eliminate the need of paired_end data and have already been used for tasks like genome finishing and haplotype_assembly besides read_length the future generation sequencing_technologies produce fragments with novel features such as the uniform_distribution of sequencing_errors that are not properly addressed or exploited in most of the existing_methods that instead are tailored to the characteristics of traditional ngs_technologies recently mec and wmec approaches have been used in the context of long_reads confirming that long fragments allow to assemble haplotypes more accurately than traditional short_reads since mec is np_hard exact solutions have exponential complexity different approaches tackling the computational hardness of the problem have been proposed in literature integer_linear techniques have been recently used but the approach failed to optimally solve some difficult blocks there were also proposed fixedparameter tractable fpt algorithms that take time exponential in the number of variants per read and hence are well suited for short_reads but become unfeasible for long_reads for this kind of data heuristic approaches have been proposed to respond to the lack of exact solutions most of the proposed heuristics such as refhap make use of the traditional all heterozygous assumption that forces the heterozygosity of all the phased positions these heuristics have good performances but do not offer guarantees on the optimality of the returned solution two recent_articles aim at processing future generation long_reads by introducing algorithms exponential in the sequencing_coverage a parameter which is not expected to grow as fast as read_length with the advent of future generation technologies the first algorithm called probhap is a probabilistic dynamic_programming that optimizes a likelihood_function generalizing the objective_function of mec albeit probhap is significantly_slower than the previous heuristics it obtained a noticeable improvement in accuracy the second approach called whatshap is the first exact algorithm for wmec that is able to process long_reads it was shown to be able to obtain a good accuracy on simulated_data of long_reads at coverages up to and to outperforms all the previous exact approaches however it cannot handle coverages higher than and its performance evidently decreases when approaching that limit in this article we exploit a characteristic of future generation technologies namely the uniform_distribution of sequencing_errors for introducing section an exact fpt algorithm for a new variant called k cmec of the wmec problem where the parameters are i the maximum number k of corrections that are allowed on each snp position and ii the coverage the new algorithm called hapcol is based on a characterization of feasible solutions given inand its time complexity is ocov k lm albeit it is possible to prove a stricter bound where cov is the maximum coverage l is the read_length and m is the number of snp positions hapcol is able to work without the all heterozygous assumption in section we experimentally compare accuracy and performance of hapcol on real and realistically simulated_datasets with three state of the art approaches for haplotype assemblyrefhap probhap and whatshap on a real standard benchmark of long_reads we executed each tool under the all heterozygous assumption since this dataset has low coverage on average and since the covered positions are heterozygous with high_confidence hapcol turns out to be competitive with the considered methods improving the accuracy and the number of phased positions we also assessed accuracy and performance of hapcol on a large collection of realistically simulated_datasets reflecting the characteristics of future generation sequencing_technologies that are currently or soon available coverage up to read_length from to bases substitution error_rate up to and indel rate equal to when considering higher coverages interesting applications such as snp_calling or heterozygous snps validation become feasible and reliable since these applications require that haplotypes are reconstructed without the all heterozygous assumption on the simulated_datasets we only considered the tools that do not rely on this assumptionwhatshap and hapcol results on the simulated_datasets with coverage show that hapcol while being as accurate as whatshap they achieve an average_error of is faster and significantly more memory_efficient times_faster and times less memory the efficiency of hapcol allows to further improve accuracy indeed the experimental_results show that hapcol is able to process datasets with coverage on standard workstations small servers whereas whatshap exhausted all the available memory gb and that since the number of ambiguous uncalled positions decreases the haplotypes reconstructed by hapcol at coverage are more accurate than those reconstructed at coverage 
