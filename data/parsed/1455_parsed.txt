data_and dnorm disease name normalization with pairwise learning to rank motivation despite the central role of diseases in biomedical_research there have been much fewer attempts to automatically_determine which diseases are mentioned in a textthe task of disease name normalization dnorm compared with other normalization tasks in biomedical_text research methods in this article we introduce the first machine_learning approach for dnorm using the ncbi disease corpus and the medic vocabulary which combines mesh and omim our method is a high_performing and mathematically principled framework for learning similarities between mentions and concept names directly from training data the technique is based on pairwise learning to rank which has not previously been applied to the normalization task but has proven successful in large optimization_problems for information_retrieval results we compare our method with several techniques based on lexical normalization and matching metamap and lucene our algorithm achieves micro averaged f measure and macro averaged f measure an increase over the highest performing baseline method of and respectively availability the source_code for dnorm is available at http www ncbi nlm nih gov cbbresearch lu demo dnorm along with a web_based demonstration and links to the ncbi disease corpus results on pubmed_abstracts are available in pubtator http www ncbi nlm nih gov cbbresearch lu demo pubtatordiseases are central to many lines of biomedical_research and enabling access to disease information is the goal of many information_extraction and text_mining efforts islamaj dog an and the task of disease normalization consists of finding disease mentions and assigning a unique_identifier to each this task is important in many lines of inquiry involving disease including etiology e g genedisease relationships and clinical aspects e g diagnosis prevention and treatment disease may be defined broadly as any impairment of normal biological_function given the wide_range of concepts that may thus be categorized as diseasestheir respective etiologies clinical_presentations and their various histories of diagnosis and treatmentdisease names naturally exhibit considerable variation this variation presents not only in synonymous terms for the same disease but also in the diverse logic used to create the disease names themselves disease names are often created by combining roots and affixes from greek or latin e g hemochromatosis a particularly flexible way to create disease names is to combine a disease category with a short descriptive modifier which may take many forms including anatomical locations breast_cancer symptoms cat eye syndrome treatment dopa_responsive causative_agent staph infection biomolecular etiology g pd deficiency heredity x_linked or eponyms schwartz jampel syndrome modifiers are also frequently used to provide description not part of the name e g severe_malaria when diseases are mentioned in text they are frequently also abbreviated exhibit morphological or orthographical variations use different word orderings or use synonyms these variations may involve more than single_word substitutions for example because affixes are often composed a single_word oculocerebrorenal may correspond to multiple words eye brain and kidney in another form the disease normalization task is further complicated by the overlap between disease concepts forcing systems that locate and normalize diseases in natural_language text to balance handling name variations with differentiating between concepts to achieve good performance previous_works addressing disease name normalization dnorm typically use a hybrid of lexical and linguistic approaches islamaj dog an and lu b while string normalization techniques e g case folding stemming do allow some generalization the name variations in the lexicon always impose some limitation machine_learning may enable higher performance by modeling the language that authors use to describe diseases in text however there have been relatively few attempts to use machine_learning in normalization and none for disease names in this work we use the ncbi disease corpus islamaj dog an and which has recently been updated to include concept annotations islamaj dogan et_al unpublished_data to consider the task of disease normalization we describe the task as follows given an abstract return the set of disease concepts mentioned our current purpose is to support entityspecific semantic search of the biomedical_literature and computer_assisted biocuration especially document triage in this article we introduce dnorm the first machine_learning to normalize disease names in biomedical_text our technique learns the similarity between mentions and concept names directly from the training data thereby focusing on the candidate generation phase of normalization our technique can learn arbitrary mappings between mentions and names including synonymy polysemy and relationships that are not to moreover our method specifically handles abbreviations and word order variations our method is based on pairwise learning to rank pltr which has been successfully_applied to large optimization_problems in information_retrieval but to the best of our knowledge has not previously been used for concept normalization during development all techniques were evaluated using the development subset of the ncbi disease corpus varying the learning_rate demonstrated to provide the highest performance on the development set and this is the setting used for all experiments reported in this section final_evaluation was performed using the test subset of the ncbi disease corpus our evaluation considers only the set of disease concepts found within each abstract ignoring the exact location s where each concept was found thus the number of true_positives in an abstract is the size of the intersection between the set of concepts annotated in the gold_standard and the set of concepts returned by the system the number of false_negatives and false_positives are defined analogously our result measures are precision recall and f measure which were calculated as follows micro averaged results were calculated by summing the number of true_positives false_positives over the entire evaluation set macro averaged results were determined from the number of true_positives false_positives for each abstract and the mean result was calculated across all abstracts reports the evaluation results for dnorm and all baseline methods using micro averaged performance reports the results for the same experiments using macroaveraged performance reports the recall for the banner lucene banner cosine similarity and dnorm banner pltr experiments if we return more than the highest scoring result from the candidate generation we created our own implementation of pltr using the colt matrix library http acs lbl gov software colt the implementation enables high performance by taking_advantage of the sparsity of the mention and name vectors training on the ncbi disease corpus training subset in h using a single ghz intel_xeon processor limited to gb_memory our implementation scores one mention against the nearly names in the lexicon in ms using the same equipment we have applied dnorm to all pubmed_abstracts and made the results publicly_available in pubtator though the nlm lexical normalization method has higher recall than any method besides dnorm the precision remainsthe remaining methods use separate stages for ner and normalization because all use banner for ner the errors caused by the ner component are the same the remaining methods also use abbreviation resolution significantly_reducing the number_of caused by ambiguous abbreviations the inference_method handles term variations by using string similarity and lucene search though it tends to select highly_specific concepts such as mapping inherited_disorders to blood_coagulation disorders inherited mesh d analyzing the errors made by banner lucene but not by banner cosine similarity shows that most are due to the lucene scoring_function insufficiently penalizing lexicon names containing tokens not present in the mention the majority of the errors made by banner cosine similarity but not by dnorm are due to term variation because banner lucene banner cosine similarity and dnorm banner pltr use the same processing pipeline the performance difference between these methods is solely due to the normalization methodology in addition because the scoring_function for cosine similarity is equivalent to the one used by dnorm before training the performance difference between these methods is solely due to the weights learned during training to further isolate the effect of pltr training on performance we performed a normalization experiment comparing lucene cosine similarity and pltr using the gold_standard mentions from the ncbi disease corpus test subset as input instead of the mentions found by banner we again used the pltr model trained using in this comparison we count a result as correct if the concept associated with the lexicon name scored highest by dnorm matched the annotated concept for the mention out of the mentions lucene found cosine similarity found and pltr found this experiment confirms the effectiveness of the novel learning procedure used by dnorm we performed an experiment to demonstrate the effect that varying the learning_rate has on training time and performance we varied exponentially between and and report the results in the best performance was achieved with which required a training time of min and resulted in a micro averaged f measure of while the final performance is similar over a wide_range of values for the training time varied_widely ranging from min to h with smaller values requiring longer training times we have shown that pltr successfully learns a mapping from disease name mentions to disease concept names resulting in a significant_improvement in normalization performance we have also shown that the training time requirements are modest and that inference time is fast enough for use online our approach models many kinds of term variations learning the patterns directly from training data our error analysis showed that ner is a continued concern and the analysis of the learned weight_matrix showed that morphological_analysis is important for this problem our technique primarily addresses the candidate generation step in normalization and could be paired with more sophisticated techniques for disambiguation we believe that pltr may prove to be sufficiently useful and flexible to be applicable to normalization problems in general while general applicability should be verified in future work the present article represents an attempt to move toward a unified_framework for normalizing biomedical entity mentions with machine_learning 
