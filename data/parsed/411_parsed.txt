systems_biology an unbiased evaluation of gene prioritization_tools motivation gene_prioritization aims at identifying the most promising candidate_genes among a large_pool of candidatesso as to maximize the yield and biological_relevance of further downstream validation experiments and functional studies during the past_few several gene prioritization_tools have been defined and some of them have been implemented and made available through freely_available web_tools in this study we aim at comparing the predictive_performance of eight publicly_available prioritization_tools on novel data we have performed an analysis in which recently reported disease gene_associations from literature are used to benchmark these tools before the underlying databases are updated results cross_validation on retrospective data provides performance estimate likely to be overoptimistic because some of the data_sources are contaminated with knowledge from disease gene association our approach mimics a novel discovery more closely and thus provides more realistic performance_estimates there are however marked differences and tools that rely on more advanced data_integration schemes appear more powerful a major_challenge in human_genetics is to discover novel diseasecausing genes both for mendelian and complex_disorders identifying disease genes is a crucial first step in unraveling molecular_networks underlying_diseases and thus understanding disease mechanisms also toward the development of effective therapies the discovery of a novel disease gene often starts with a cytogenetic_study a linkage_analysis a high_throughput omics experiment or a genome_wide gwas however these studies do not always pinpoint the disease gene uniquely but often result in large lists of candidate_genes that are potentially relevant moreover recent_advances offer promising opportunities to explore the genomic_alterations of patients however thousands of mutations in hundreds of genes are often detected among which only a few are in fact linked to the genetic condition of interest the experimental_validation of these candidate_genes for instance through resequencing pathway or expression analysis is still expensive_and an efficient way to reduce the validation cost is to narrow down the large list of candidate_genes to a small and manageable set of highly_promising genes a process called gene_prioritization prioritization in the past was achieved manually by geneticists and biologists and was mainly based on their own expertise nowadays biologists and geneticists can use computational_approaches that can handle and analyze the large amount of genomic_data currently available in the past_few many gene_prioritization methods have been proposed some of which have been implemented into publicly_available tools that users can freely access and use information about these tools is summarized in our gene_prioritization portal http www esat kuleuven be gpp that currently describes prioritization_tools this web_site has been designed to help researchers to carefully select the tools that best correspond to their needs for instance only few tools can prioritize the whole_genome which can be necessary when no positive regions can be identified beforehand or when selecting candidates for a medium throughput screen instead of low_throughput validation another example is the study of a poorly_characterized disorder for which a prioritization tool not relying on a set of known disease genes might be more suited recently several studies have demonstrated that gene prioritization_tools can help geneticists to discover novel disease genes for instance a kif a mutation was discovered in hereditary spastic_paraparesis patients after kif a was predicted to be the best candidate_gene from the locus using multiple prioritization_tools another study discovered homozygous mutations in the ptrf cavin gene in patients with congenital generalized lipodystrophy with muscle rippling after ptrf cavin was predicted as the most probable candidate_gene for high expression in muscle and adipose_tissue a third study identified the hhex gene to be associated with type to whom correspondence should be addressed ythe authors wish it to be known that in their opinion the first three authors should be regarded_as diabetes t d in a dutch cohort after investigating the t dsusceptibility loci using candidate_gene however beyond these conceptual differences one essential parameter to consider when selecting gene prioritization_tools is their respective performancethat is their ability to identify the true_positive genes as promising candidate_genes to maximize the yield of the follow_up experimental_validation a common standard in bioinformatics is to estimate the performance with a benchmark analysis several publications that introduce a novel prioritization approach also describe a comparative benchmark with several existing_methods however these benchmarks are most of the time cross_validations of gold_standard disease datasets e g known data therefore the estimation of the performance is likely an overestimate of the real performance i e on novel data because different types of data are dependent on each other e g gene_ontology go annotation kyoto_encyclopedia kegg_pathway membership and medline_abstracts it becomes impossible to remove all cross_talk effects between data_sources e g removing medline data does not remove all information from the biomedical_literature since much of it is present in go and kegg to prevent contamination of the prediction of the disease gene by actual retrospective knowledge of this association this makes it challenging to create benchmarks on retrospective data that are indicative of the performance of the method in an actual research_setting next to benchmarking some studies use several prioritization methods to analyze disease associated loci mostly for type_diabetes and obesity however the results have not been experimentally_validated which means that it is not possible to identify which methods made better predictions also a few studies combine computational and experimental analysis in silico generated hypothesis are then validated in vivo we have for instance performed a computationally supported genetic_screen in drosophila that led to the identification of novel atonal genetic interactors although useful such studies often rely on the use of a single tool and therefore cannot be used to compare different approaches they also give no indication of the performance of the method in general but only illustrate it on a single well validated case in this study we aim at comparing the performance of several freely_accessible web_based gene prioritization_tools on novel data which to our knowledge has never been performed before to this aim we selected recently reported disease gene_associations from literature and use several gene prioritization_tools to make predictions immediately after publication typically within days our approach relies on the fact that when the prioritization_tools are used the novel disease gene association of interest is not yet included in the databases that underlie these tools as a consequence our approach mimics a novel discovery and therefore the estimation of the performance is more accurate it has to be mentioned that we compare tools and not the underlying algorithms we see a tool as an algorithm plus some data_sources because this is what is most relevant to geneticists the overall ranking results of all gene prioritization_tools are summarized in the complete results are presented in supplementary tables s and s these results have also been added to the gene_prioritization portal http www esat kuleuven be gpp right among the genome_wide tools endeavourgw performs slightly better than pinta gw and candid left when considering the response_rate endeavour both modes candid and pinta both modes performed the best study with closely followed by toppgene genedistiller and genewanderer rw with more than meaning that only one or two associations are missing at the other hand of the spectrum posmed ks and posmed dn only work for about half of the experiments in our benchmark and respectively when we compare the tools based on the global auc see we observe that genedistiller appears as the best performing tool overall with an auc of it is followed by endeavour cs endeavour gw pinta gw suspects pinta cs candid genewanderer rw genewanderer dk toppgene posmed ks and posmed dn however the roc_curves are in general intertwined meaning that none of the approaches is clearly performing better than the other however we postulate that in our case the most important section of the roc_curve is the beginning and therefore usethree other measures the true_positive rates at and respectively these measures indicate how efficient the tools would be if only the top candidate_genes would be assayed considering the tpr in top and we can observe a similar trend indeed at genedistiller is first with a rate of associations found over followed by both toppgene and endeavour cs with associations however at the best tool is endeavour cs associations followed by genedistiller associations the other tools show smaller tpr at both levels pinta cs we aim at assessing the usefulness of eight gene prioritization_tools that are freely_available via web_applications we have built a validation based on recently_discovered disease gene_associations from literature containing novel genes for both monogenic conditions and complex_disorders we have selected novel disease gene_associations regardless of their strength and of the underlying methodology to mimic a real discovery we have run the tools as soon as the article appeared online so that all databases used for gene_prioritization are still not contaminated by the knowledge of the novel disease gene association this also means that we had to exclude tools that query medline online because their results would be biased we want to compare the performance of the tools even if the inputs are different genes versus keywords genome_wide versus candidate set among the eight gene prioritization_tools that we have analyzed in this study only endeavour candid and pinta have been used for genome_wide prioritization the input_data for endeavour and pinta are training genes whereas candid requires keywords the gene prioritization_tools that we have used to prioritize_candidate within a region of interest are suspects toppgene genewanderer posmed genedistiller and again endeavour and pinta suspects and posmed are trained with keywords and the other tools require training genes we have extensively searched through literature and dedicated databases to identify as many reliable training genes as possible for the disease of interest as well as a set of appropriate keywords to derive fair and meaningful comparisons however different and possibly better results might be obtained by refining the inputs our validation is too small to claim that the differences among tools are significant however a trend can still be observed genedistiller and endeavour cs consistently appear as the best tools when looking at all performance_measures it is interesting to notice that the best results are in general obtained with tools that use many data types in conjunction up to eight for endeavour when compared with the three data_sources used by posmed but there is no perfect correlation this is in agreement with the conclusion of the recent review by who indicate that successful computational applications will be facilitated by improved data_integration all tools except posmed have a high response_rate ranging from to meaning that at least of the novel disease genes are prioritized or of for suspects however the response_rates for posmed ks and posmed dn are and respectively which can be explained by the fact that posmed also acts as a filter on the candidate_genes to obtain a reduced list of genes in the end there are therefore cases for which the novel disease gene has been removed by the filter this is different from the other tools for which missing genes basically correspond to genes that are not recognized by the tool it happens most of the time with poorly_characterized genes such as c orf another special_case is suspects that went offline during the validation and therefore could only be validated with the first associations we therefore calculated the response_rate only on the first associations two types of tools can be distinguished the ones that are trained with already known genes and the ones that are trained with descriptive keywords it appears that gene based tools seem to work better than keyword based tools the average of medians is for gene based tools and for keyword based tools similar results are obtained with the other measures see supplementary this could be because we use ingeneral more genes than keywords for training genes on an average for six keywords this also indicates that more keywords might be needed to model a disease and that a small text such as an omim entry might even be necessary van there is in general an agreement between the five performance_measures we use throughout our study one notable_exception exists for toppgene whose auc is and corresponds to rank th out of the prioritization_tools in contrast its associated tpr in top is which corresponds to rank second this apparent contradiction can be explained by observing in which the roc_curve exhibits a non convex shape this is because toppgene either ranks the novel disease gene on top or at the bottom i e the disease genes are rarely ranked in the middle therefore the tpr in top will be high because it only takes_into the top of the list while the auc will be lower because it basically behaves like an average over all cases another important point is that our observations are in line with the no free lunch theorem indeed each tool can perform better than all the others for some cases or in other words none of the tools outperforms another on the complete dataset if we do not consider the special_case of posmed that also acts as a filter posmed ks has been trained with the complete keyword set whereas posmed dn has been trained only with the disease name the median rank ratio is when the complete keyword set is used and drops to when only the disease name is inputted if we only compare the results over the associations for which both tools are able to prioritize the novel disease gene the difference becomes even larger and respectively for posmed ks and posmed dn altogether these results indicate that posmed does not rely on the use of the single disease name and that the extra keywords are indeed important it can be observed that the performance_measures for posmed are worse than for the other tools in our benchmark study however when looking at the individual ranks it can be observed that posmed returns far fewer genes than the other tools because it also acts as a filter as a result the rank ratios are in general larger and the performance_measures are therefore worse as such it becomes difficult to fairly compare posmed with the other tools because our measures of performance naturally penalize the fact that posmed returns prioritizations for a limited set of candidates changing our performance_measures to counterbalance this effect would then give an unfair advantage to posmed because it returns prioritizations only for the safer bets genewanderer has also been run twice with different network algorithms random_walk and diffusion kernel the respective performances are very similar although the random_walk approach is performing a little bit better than the diffusion kernel albeit non significant for median rank ratio similar differences are observed with the other measures the heat map indicates a strong correlation see supplementary between the two modes which was expected since applying diffusion to a kernel can be interpreted as equivalent to applying a random_walk on the underlying_network altogether this indicates that these two algorithms are similar endeavour and pinta are used to prioritize both the whole_genome endeavour gw and pinta gw and the defined chromosomal_region endeavour cs and pinta cs allowing us to identify the influence of the size of the gene_list to prioritize the median rank ratio is better for endeavour cs than for endeavour gw in our benchmark the difference remains albeit smaller when considering the auc and the tpr in top and the same training genes are used and therefore the observed difference is only caused by extending the small candidate gene_set to the whole_genome this confirms previous_findings that prioritizing the whole_genome is more difficult than prioritizing a rather small positive locus the heat map indicates that the two endeavour modes are strongly correlated as expected since the core algorithm is the same in both modes see supplementary in contrary the results for both pinta modes are similar correlation of and seem to indicate that the size of the candidate set does not influence this algorithm in this study we consider the tools as off_the solutions and use them as recommended by the developers without fine_tuning of the parameters however an important feature that might influence the results is the date of the last data update the latest genomic_data still prior to discoveries considered in this study is likely to give the best results because it will model more accurately what is currently known when compared with data that are years old in our setup we have no control over the genomic_data used and cannot identify whether variation in performance among tools can be explained by this in addition the quality of both the data_sources and the integration methodologies are also influencing the outcome of the prioritization process however we aim at estimating the usefulness of some prioritization_tools for geneticists therefore an in depth comparison of the implementation of the tools is beyond the scope of this study it is important to notice that the novel disease gene_associations do not represent a homogeneous set indeed the median of the rank ratios over the tools show that some associations seem to be easier to predict than others this also explains why all tools are moderately correlated on the heat map a plausible explanation is the disparity in the available data between the novel disease genes since only little data can be gathered for poorly_characterized genes such as c orf they are more difficult to prioritize however we also hypothesize that the nature of the underlying genetic_disorder as well as the quality of the reported association might influence the ability of the tools to correctly predict that association we have therefore divided the associations between confirmed intermediate and unconfirmed among the associations are confirmed are intermediate and are unconfirmed see supplementary we hypothesize that this might influence our validation since some unconfirmed associations might in fact be spurious we observe that suspects and toppgene perform better for the confirmed associations than for the unconfirmed ones see supplementary tables s and s however this trend is not always shared as the situation is opposite for genedistiller and genewanderer although informative these comparisons are not significant due to the small number of associations in our validation dataset there are monogenic diseases and multifactorial disorders see supplementary tables s and s it has been shown that it is more difficult to make predictions for multifactorial_diseases than for monogenic diseases our results however seem to indicate that not all tools are influenced by the intrinsic complexity of multifactorial_diseases for instance endeavour and toppgene seem to perform better for monogenic conditions while genewanderer and suspects perform better for complex_disorders however the size of our validation dataset does not allow for a complete statistical_analysis larger validation datasets and real predictive studies will be pursued to complement our preliminary study we are aware of the limited coverage of available literature in human_genetics in our study that report novel disease gene_associations however we aimed at estimating the real performance of gene prioritization_tools and therefore have decided to keep under strict_control all the factors that could potentially bias the benchmark we were further interested in finding novel disease gene_associations for defining a proper benchmark and there is no guarantee that these associations are uniformly_distributed over the whole literature we have used journals about genetic_disorders in general and favor journals that report novel associations and have avoided specialized journals that focus on few diseases to avoid introducing bias toward one disease class our choice of the six selected journals may not be perfect but they allowed us to cover most disease types and most situations several studies have shown that combining predictions of several tools lead to even better predictions however no performance_criteria were used to select the tools to be combined with this comparison of tools we ease the selection of the most efficient tools whose combination may lead to more accurate_predictions in addition we report that the meta predictors that integrate the predictions made by several tools perform better than the best individual tools as already reported our results indicate that cross_validation based benchmarks tend to overestimate the real predictive_performance indeed all the tools for which such a benchmark exists have lower auc than anticipated using our dataset see supplementary we therefore believe that developers should take extra care when benchmarking their tools as to avoid these pitfalls also some hard constraints have made this study small enough not to reach significance e g only few tools have a programmatically queryable interface as already discussed in this field needs to consolidate through improved benchmarking efforts due to the lack of a ground_truth for evaluating the performance of prioritization methods therefore we see a need for a large_scale community effort to compare multiple tools across common prospective benchmarks we hope our work represents the first step toward a collaborative_effort to tackle this problem at a larger scale 
