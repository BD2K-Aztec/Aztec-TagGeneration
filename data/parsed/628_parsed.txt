genome_analysis a variable_selection method for genome_wide genome_wide gwas have become increasingly popular for studying complex human_diseases within the last several years the number of single_nucleotide snps per dna_array has grown from to million despite the very large number of snps that are genotyped in a study gwas data are commonly analyzed one snp at a time indeed the armitage trend_test att is used almost exclusively to whom correspondence should be addressed there are at least two strong reasons for considering all the snps or at least a large subset of them simultaneously first the marginal_effects of snps i e the effect of each snp on disease when it is considered alone may be quite different from their joint effects i a snp that is not related to disease but is correlated with a causal snp will be marginally associated with disease ii some snps may have weak marginal_effects but strong joint effects conditional on causal_snps that are already in the model false_positive signals tend to be weakened while marginally uncorrelated causal_snps have a better chance of being selected second the predictive_power of a single snp tends to be very low the accuracy of prediction can be improved_substantially by utilizing a large number of relevant snps it is extremely_challenging to decide which set of snps should be included in the joint analysis because the number of snps in a gwas is much larger than the sample_size this is commonly referred to as the small n large p problem a major_difficulty in this problem is that the number and extent of spurious_associations between predictors and response increase rapidly with increasing p weak effects of causal_variants and strong linkage_disequilibrium ld among snps present additional challenges there is a large body of literature on variable_selection methods including bridge regression least absolute shrinkage and selection operator lasso smoothly clipped absolute deviation scad elastic_net and adaptive lasso however these methods were designed for a moderate number of predictors i e tens or hundreds for ultra high p these methods may be computationally infeasible and statistically inaccurate recently fan and lv developed the so_called sure independence screening sis strategy for high_dimensional statistical_modeling the idea is to first reduce the dimension from a very large_scale to a moderate scale that is below sample_size by univariate correlation learning and then select important predictors by a moderate scale variable_selection method such as the lasso or scad in a similar spirit reduced the dimension of snps in a gwas to several hundreds using a simple score criterion and applied the lasso to the reduced set of snps a drawback of this approach is that important features that are marginally uncorrelated with response are bound to be missed because the univariate screening step is based entirely on marginal correlations suggested the iterative sure independence screening isis procedure which iterates the sis procedure conditional on the previously selected_features so as to capture important features that are marginally uncorrelated with response fan and lvs work is confined to linear_regression of a continuous response and the number of features they considered is merely thousands in this article we extend fan and lvs isis idea to logistic_regression of casecontrol gwas data this extension is challenging for several reasons first the isis performs linear_regression of residuals but residuals cannot be used as response variables in logistic_regression second prediction_errors tend to be much higher for binary outcomes than continuous outcomes third the number of snps can be extremely_large typically more than half a million fourth the effects of causal_snps on complex_diseases tend to be small to modest so the signal_to in gwas data is low fifth the ld among snps is extensive and can be extremely_high in certain regions a separate challenge is that the false_discovery fdr associated with the isis and indeed with any existing variable_selection method tends to be high recently meinshausen andproposed the stability selection_strategy to reduce the fdr the idea is to repeatedly subsample the original data and perform variable_selection on each subsample the features selected frequently among the subsamples tend to be truly associated with outcome and thus should be included in the final model in this article we integrate stability_selection into our isis procedure to develop a new approach gwaselect for genome_wide variable_selection we describe our approach in the next section in section we demonstrate through simulation_studies that gwaselect has robust performance under a variety of ld structures and can substantially_increase the power and reduce the fdr compared with existing_methods in addition the regression_models generated by gwaselect significantly improve prediction_accuracy in section we apply gwaselect to the gwas data from the wellcome_trust and show that it yields several novel discoveries and improves prediction_accuracy we have developed a new tool gwaselect for variable_selection at the genome_wide level this regression_based method has the ability to capture both marginally correlated and marginally uncorrelated causal_snps and has low fdr the advantages over the existing_methods have been demonstrated through simulated_and our method has two versions the first version requires the specification of the model size d for which we suggest to choose a number that is consistent with the current biological_knowledge of the studied disease the second version d gwaselect does not require the specification of the model size and this is the version we recommend for general use the correlation structures for causal_variants used in our simulation_studies have biological_relevance scheme mimics a scenario in which the causal_variants form a gene_cluster that contributes synergistically to the disease outcome while scheme reflects a scenario in which several biological_pathways or networks affect the disease development we did not include least angle regression lars in our studies because it has been shown to have highly_similar performance to lasso indeed lasso can be implemented by lars with a small modification demonstrated that ccd is considerably faster and more robust than lars and is more successful than lars in model_selection the hlasso adopts a concave penalty function but the ccd algorithm may not converge for non convex penalty functions a valid algorithm to implement concave penalty functions is local linear approximation which amounts to multiple rounds of ccd and would make the hlasso computation prohibitively_expensive for the wtccc t d data running the ccd version of the hlasso with iterations on an intel quadcore nehalem processor ghz gb_memory requires to h depending on the value of the tuning parameter in contrast we have been running the gwaselect in a parallel_computing environment and the same analysis can be completed within several hours on processors in an independent effort developed an isis method for generalized_linear in the context of microarray_data analysis in their method the conditional screening procedure requires fitting a separate regression_model for each feature which would create heavy computational_burden for gwas data in addition their method tends to have high fdr they observed that cross_validation tends to yield large models for logistic_regression resonating our findings we can extend our methods to select interactions instead of considering all possible interaction_terms we may incorporate known biological_network information into our selection procedure another approach is to first extend the existing genetic network identification tools such as the liquid association and bounded mode stochastic_search to infer snp interactions and then incorporate such information into our gwaselect procedure recently proposed a markov blanket based method to evaluate epistatic_interactions for gwas data it will be interesting to compare to that method when we extend our work to interaction effects 
