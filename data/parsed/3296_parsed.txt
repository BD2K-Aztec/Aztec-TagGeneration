nrgc a novel referential genome_compression algorithm motivation next_generation techniques produce millions to billions of short_reads the procedure is not only very cost_effective but also can be done in laboratory_environment the state of the art sequence assemblers then construct the whole genomic_sequence from these reads current cutting_edge computing technology makes it possible to build genomic_sequences from the billions of reads within a minimal cost and time as a consequence we see an explosion of biological_sequences in recent times in turn the cost of storing the sequences in physical memory or transmitting them over the internet is becoming a major_bottleneck for research and future medical_applications data compression techniques are one of the most important remedies in this context we are in need of suitable data compression algorithms that can exploit the inherent structure of biological_sequences although standard data compression algorithms are prevalent they are not suitable to compress biological sequencing_data effectively in this article we propose a novel referential gen ome compression algorithm nrgc to effectively and efficiently compress the genomic_sequences results we have done rigorous experiments to evaluate nrgc by taking a set of real human gen omes the simulation_results show that our algorithm is indeed an effective genome_compression algorithm that performs better than the best known algorithms in most of the cases compression and decompression times are also very impressive availability_and the implementations are freely_available purposes they can be downloaded from next_generation ngs techniques reflect a major breakthrough in the domain of sequence_analysis some of the sequencing_technologies available today are massively_parallel signature sequencing pyrosequencing illumina solexa sequencing solid sequencing ion semiconductor sequencing etc any ngs technique produces abundant overlapping reads from a dna_molecule ranging from tiny bacterium to human species modern sequence assemblers construct the whole_genome by exploiting overlap information among the reads as the procedure is very cheap and can be done in standard_laboratory environments we see an explosion of biological_sequences that have to be analysed but before analysis the most important prerequisite is storing the data in a permanent memory as a consequence we need to increase physical memory to cope up with this increasing amount of data by between million and billion human_genomes are expected to have been sequenced according to the storage requirement for this data alone could be as much as exabytes one exabyte being bytes although the recent engineering innovation has sharply decelerated the cost to produce physical memory the abundance of data has already outpaced it besides this the most reliable mechanism to send data instantly around the globe is using the internet if the size of the data is huge it will certainly create a burden over the internet network congestion and higher transmission costs are some of the side_effects data compression techniques could help alleviate these problems a number of techniques can be found in the literature for compressing general_purpose data they are not suitable for special purpose data like biological sequencing_data as a result the standard compression tools often fail to effectively compress biological data in this context we need specialized algorithms for compressing biological sequencing_data in this article we offer a novel algorithm to compress genomic_sequences effectively and efficiently our algorithm achieves compression_ratios that are better than the currently best performing algorithms in this domain by compression_ratio we mean the ratio of the uncompressed data size to the compressed data size the following two versions of the genome_compression problem have been identified in the literature i referential genome_compression the idea is to utilize the fact that genomic_sequences from the same species exhibit a very high_level of similarity recording variations with respect to a reference_genome greatly_reduces the disk_space needed for storing any particular genomic_sequence the computation complexity is also improved quite a bit so the goal of this problem is to compress all the sequences from the same or related species using one of them as the reference the reference is then compressed using either a general_purpose compression algorithm or a reference free genome_compression algorithm ii reference free genome_compression this is the same as problem i stated above except that there is no reference_sequence each sequence has to be compressed independently in this article we focus on problem i we propose an algorithm called nrgc novel referential genome compressor based on a novel placement scheme we divide the entire target_genome into some nonoverlapping segments each segment is then placed onto a reference_genome to find the best placement after computing the best possible placements each segment is then compressed using the corresponding segment of the reference simulation_results show that nrgc is indeed an effective compression tool the rest of this article is organized as follows section has a literature_survey section_describes the proposed algorithm and analyzes its time complexity our experimental_platform is explained in section this section also contains the experimental_results section_presents some discussions section concludes the study our proposed algorithm is able to work with any alphabet used in the genomic_sequences of interest other notable algorithms existing in the domain of referential genome_compression can perform compression only with a restricted set of alphabets used for genomic_sequences e g p fa a c c g g t t n ng these characters are most commonly seen in biological_sequences but there are several other valid characters frequently used in clones to indicate ambiguity about the identity of certain bases in sequences in this context our algorithm is not restricted with the limited set of characters found in p nrgc also differentiates between lower case and upper case letters gdc green and idocomp can identify the difference between upper case and lower case characters defined in p but algorithms such as grs or rlz opt can only handle upper case alphabet from p idocomp replaces all the character in the genomic_sequence with n that does not belong to p specifically nrgc compresses the target_genome file regardless of the alphabets used and decompresses the compressed file that is exactly identical to the target file gdc idocomp and ergc perform the similar job but green does not include the metadata information and outputs the sequence as a single_line instead of multiple lines i e it does not encode the line break information the difference between two genomic_sequences can be computed by globally aligning them as the sequences in the query set coming from the same species are similar and of roughly_equal size let r and t denote the reference and target_sequences respectively as stated above the time complexity of a global_alignment algorithm is typically ojrjjtj i e quadratic in terms of the reference and target lengths global_alignment is solved by employing dynamic_programming and thus is a very time and space intensive procedure specifically if the sequences are very large in fact it is not possible to compute the difference between two human_genomes using global_alignment in current_technology instead if we divide the reference and target into smaller segments and globally align the corresponding segments the time and space complexities seem to be improved but there are two shortcoming in this approach i it still is quadratic with respect to segment lengths and ii because of large_insertions and or deletions in the reference and or target the corresponding segments may come from different regions i e dissimilar to quantify this issue we propose a placement scheme which efficiently finds the most suitable place for a segment in the reference the segment is then compressed by our greedy variation detection algorithm from the experimental_evaluations please see it is evident that ergc performs better than gdc idocomp and nrgc in out of datasets it is also not restricted to the alphabets defined in p but the main limitation of ergc is that it performs better only when the variations between the reference and the target genomes are small if the variations i e insertions and or deletions are high between the reference and the target its performance degrades dramatically as hg contains large_insertions and or deletions ergc fails to perform a meaningful compression while using this genome as the reference or the target on the contrary nrgc performs better than ergc and other notable algorithms on an average please see this is due to the fact that nrgc can handle large variations between the reference and target genomes the main difference between nrgc and ergc is that nrgc at first finds a near optimal placement of non overlapping segments of target onto the reference_genome and then records the variations on the other hand ergc tries to align the segments contiguously and due to its look ahead greedy nature it fails to align the segments when there are large_insertions and or deletions in the reference and or the target genomes in this scenario ergc concludes that the segments could not be aligned and stores them as raw sequences as discussed previously our proposed algorithm nrgc runs in three phases at first it computes a score for each of the nonoverlapping segments these segments are then aligned onto thereference genome in the second phase using the scores computed in the first phase after finding the best possible alignment nrgc records the variations in the final_phase we provide the time elapsed in each phase in computing scores takes less time compared to alignment and record variation phases this is due to the fact that the placement procedure performs sorting twice and searches for a nonoverlapping placement for each segment the execution time can be reduced by restricting the search within certain regions of the reference_genome the third phase performs k_mer production hash_table generation and recording variations this is why it also consumes higher cpu cycles than the first phase 
