data_and classification with correlated features unreliability of feature ranking and solutions motivation classification and feature_selection of genomics or transcriptomics_data is often hampered by the large number of features as compared with the small number of samples available moreover features represented by probes that either have similar molecular_functions gene_expression analysis or genomic_locations dna_copy analysis are highly_correlated classical model_selection methods such as penalized logistic_regression or random_forest become unstable in the presence of high feature correlations sophisticated penalties such as group_lasso or fused_lasso can force the models to assign similar weights to correlated features and thus improve model stability and interpretability in this article we show that the measures of feature relevance corresponding to the above_mentioned methods are biased such that the weights of the features belonging to groups of correlated features decrease as the sizes of the groups increase which leads to incorrect model interpretation and misleading feature ranking results with simulation_experiments we demonstrate that lasso logistic_regression fused support_vector group_lasso and random_forest models suffer from correlation bias using simulations we show that two related methods for group selection based on feature clustering can be used for correcting the correlation bias these techniques also improve the stability and the accuracy of the baseline models we apply all methods investigated to a breast_cancer and a bladder_cancer arraycgh dataset and in order to identify copy_number predictive of tumor_phenotype availability r code can be found at the accelerated development of microarrays and more recently of high_throughput sequencing_techniques affords genome_wide measurements of molecular changes in the cell that have an impact on cancer onset and progression high_resolution experiments targeting gene_expression dna_copy or dna_methylation in tumors can be the basis for discovering patterns predictive of diagnosis prognosis and therapy selection to whom correspondence should be addressed machine_learning for classification and feature_selection are often used for automated identification of variables associated with particular tumor phenotypes in this article we are concerned with two widely_discussed aspects of microarray classification handling high_dimensionality and ill conditioning the high_dimensionality of microarray_based experiments contrasting to the small number of samples easily leads to overfitting regularized linear_models such as logistic_regression with ridge or lasso penalty are popular solutions to fitting sparse models in which only a small subset of features plays a role more sophisticated penalties for sparse model_selection are discussed by the problem of ill conditioning refers to the existence of groups of highly_correlated features the high correlations often have a biological_basis for example if the correlated features relate to the same molecular_pathway coregulated genes in expression data are in close_proximity in the genome_sequence neighboring genes in copy_number data or share similar methylation profile consecutive cpg_dinucleotides in cpg islands methods using simple penalties like lasso typically discard most of the correlated features only one or a few arbitrary representatives from every group of correlated features enter the model provided they are relevant for the outcome as a consequence the models become unstable small changes in the training_set result in dramatic changes in the selected subset of features if the purpose of feature_selection includes biological_interpretation of the model then stability must be ensured a successful approach used in many recent_articles is that of selection of groups of features for example the group_lasso model consists of lasso selection of predefined groups of features the fused support_vector combines a lasso and a fused penalty for enforcing similar weights on correlated features this way performing group discovery and group selection simultaneously another approach to group selection adopted in a large class of methods uses clustering procedures to discover feature groups compute super features to summarize every cluster and apply feature_selection on the set of super features for example in the features are grouped with a hierarchical_clustering procedure and the cluster centroids are used for training linear_models the metagene method consists of k_means of the features followed by computing the principal_components of the clusters called metagenes which are used for model training use fuzzy clustering to determine groups of features and then select a limited number of representatives from each cluster for training svm models search for dense groups ofwe have shown that several widely used classification algorithms can generate misleading feature rankings when the training datasetscontain large groups of correlated features this can confound model interpretation since large groups of predictive features can be masked and falsely appear irrelevant such an effect is likely to occur because variables relating to a biological_process or genomic location of high interest w r t a phenotype are overrepresented in the probes set of microarray_based experiments in this article we have described the correlation bias and have shown that it affects random_forest lasso logistic_regression group_lasso and fused svm models we used two artificial datasets based on linear_models to show that the expected importance of the features in a correlated group decreases as the size of the group increases we also illustrated the correlation bias caused by the combination of fused and lasso penalties by means of a theoretical example which considers the particular case of two groups of correlated features we showed that correlation bias can be reduced using a group selection_algorithm which combines feature clustering with any classification_method we tested two methods for estimating the number of clusters based on a unsupervised fc and supervised approach fc sup respectively we showed using simulated_data experiments that fc and fcsup successfully remove the correlation bias improve the stability of feature importance and increase the accuracy of the baseline methods fc sup outperforms fc in terms of accuracy but fc is faster and has higher stability the classification of the real_data shows that fc dramatically increases the model interpretability and stability of feature importance moreover in five out of eight classification tasks fc improved the accuracy of the baseline models fc sup improves the accuracy of the baseline models in six out of eight classification tasks fc sup used in combination with lasso logistic_regression yields highest_accuracy in three out of four cases using hierarchical_clustering of the features and then computing cluster centroids using the average is certainly not the only solution for identifying and summarizing groups of correlated features depending on the distribution of the features in the sample space methods using principal component as cluster centroid as inor even several representatives may yield better performance 
