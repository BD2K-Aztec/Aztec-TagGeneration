ls_gkm a new gkm svm for large_scale datasets gkm svm is a sequence_based method for predicting and detecting the regulatory vocabulary encoded in functional dna elements and is a commonly used tool for studying gene regulatory_mechanisms here we introduce new software ls_gkm which removes several limitations of our previous releases enabling training on much larger scale ls datasets ls_gkm also provides additional advanced gapped k_mer based kernel functions with these improvements ls_gkm achieves considerably_higher accuracy than the original gkm svm availability_and c c source codes and related scripts are freely_available from http github com dongwon lee lsgkm and supported on linux and mac os x contact dwlee jhu edu supplementary_information supplementary data are available at bioinformatics online we have previously_introduced a sequence_based method kmer svm fletez brant et_al lee et_al to predict regulatory_elements from dna_sequence and epigenetic data using support_vector svm vapnik it has been successfully_applied to studies of regulatory_elements in different cellular contexts gorkin et_al pimkin et_al and further improved by using gapped k_mers as new features gkm svm ghandi et_al we have also recently demonstrated its ability to predict regulatory sequence_variants lee et_al since then gkm svm has gained_increasing setty and leslie zhou and troyanskaya our general_strategy is to build an svm classifier that distinguishes regulatory_sequences from non regulatory genomic_sequences in the k_mer or gapped k_mer frequency feature_vector space training of svm involves evaluation of a kernel_matrix or gram matrix defined as an n by n matrix of all possible inner products or kernel functions between a set of vectors of n training examples however as n increases direct computation of the kernel_matrix quickly becomes impractical with gapped k_mers as features to resolve this issue gkm svm employs an efficient algorithm that calculates a full kernel_matrix with a runtime that linearly scales with n instead of n an svm classifier is then trained using the pre_calculated kernel_matrix and standard svm training methods yet the full kernel_matrix evaluation required in the original implementation has hindered optimal training of gkm svm on larger datasets because it needs substantial memory resources proportional to n sub sampling strategies to circumvent this issue can be helpful ghandi et_al but training on smaller datasets may yield sub optimal svm classifiers to tackle this problem i have developed an improved software ls_gkm by implementing gapped k_mer kernel gkm kernel functions within the libsvm framework chang and lin most svm tools such as libsvm utilize decomposition methods for svm training it iteratively finds and solves a small subset svm problem that only needs a partial kernel_matrix for example libsvm evaluates just two columns of the kernel_matrix in each of the problem_solving steps in its sequential minimal optimization_algorithm fan et_al therefore replacing the libsvm kernel routines with the gkm kernel functions can essentially solve the memory resource issue and consequently allows us to train svm on much larger datasets to this end i adopted and modified the original gkm kernel algorithm substituting for the original libsvm kernels so that it can efficiently evaluate one column of the kernel_matrix in the same manner as the original gkm svm does for the full matrix multi thread functionality is also implemented in ls_gkm for further speed up supplementary methods for more details i first compared runtime and memory_usage of the new software to the original gkm svm by varying the training_set size n supplementary methods fig s as expected our previous method exhibits quadratic growth of memory_usage as n increases gkmtrain the new svm training module of ls_gkm with the large downloaded from cache gb also exhibits quadratic memory expansion when n however the overall memory_usage is much less than the original method moreover once the cache is full the memory only linearly increases regarding runtime the original method initially shows better computational_efficiency than gkmtrain with the default setting thread mb however with either the large cache or the four threads the new program can run faster than the original one furthermore it can run almost faster when both options are used most notably we can now regularly train ls_gkm on much larger datasets with reasonable time and memory in addition to the integration of kernel functions into libsvm three new kernel functions have been developed analogous to the basic rbf kernel gkmrbf kernel is defined as the radial_basis in the space of gapped k_mer frequency vectors supplementary methods the second option denoted as center weighted gkm kernel or wgkm kernel is inspired by the observation that most chip_seq and dnasei seq signals are concentrated in the central_regions within peaks in this new kernel the gapped k_mers are differentially weighted based on their distances from the center of the peak supplementary methods fig s the last kernel wgkmrbf kernel is the combination of the previous two to demonstrate the utility of ls_gkm i assessed how much classification_accuracy can be improved by ls_gkm for predicting regulatory_elements uniformly processed encode chip_seq the encode_project consortium containing at least regions were considered and standard training and test procedures developed in the previous_studies ghandi et_al lee et_al were applied with some modifications supplementary methods first the new models trained on the whole datasets exhibit considerably better auc than the models trained on the sub sampled sets n especially when the training_set is large n supplementary methods fig s a a grid_search of the c parameter on selected datasets confirms that this result is not an artifact caused by a sub optimal choice of c supplementary table s in fact our default value c was optimal or near optimal in almost all cases we tested second the model trained with gkmrbf kernel further increases the auc as compared to the original gkm kernel in every case but the improvement is marginal supplementary methods fig s b this result implies that the advantage of using non linear decision boundaries is limited with gkm kernel third wgkm kernel can also significantly improve the auc when the datasets already exhibit auc values supplementary methods fig s c closer investigation reveals that most of the less predictive datasets auc are chip_seq on pol and its related factors taf taf and tbp this suggests that many of these peaks may represent transient binding of the factors and thus contain less predictive sequence_features fourth similar to the gkmrbf kernel wgkmrbf kernel marginally improves auc when compared to wgkm kernel supplementary methods fig s d note that in some cases such as pol chip_seq the best aucs are achieved by gkmrbf kernel not by wgkmrbf kernel therefore for the final comparison the better kernels were chosen based on classification_performance with independent training and evaluation supplementary methods fig s supplementary figure s compares the baseline aucs trained on regions with gkm kernel from the original gkm svm to the best aucs achieved by ls_gkm the average gain of auc is significant if pol and the related chip_seq are removed the average of the best aucs is remarkably_high to determine_whether ls_gkm can also improve deltasvm a major application of gkm svm for predicting regulatory sequence_variants lee et_al the dsqtl test_set was reanalyzed using the new ls_gkm models trained on a larger gm dhs data_set supplementary methods fig s the precision_recall curves show that the new models consistently outperform the original model however no further improvement is achieved with new kernels suggesting that larger datasets primarily contribute to the improvement of deltasvm accuracy in this study i have presented new and improved software ls_gkm which offers several new functionalities and considerably improves the classification_accuracy on predicting regulatory_elements we strongly encourage all users of our software to train models on the largest datasets available which can produce significantly more accurate_predictions in this regard ls_gkm should improve the performance considerably and in combination with its enhanced functions ls_gkm is expected to significantly contribute to our understanding of gene_regulation 
