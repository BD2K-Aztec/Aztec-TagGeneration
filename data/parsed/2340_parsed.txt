idocomp a compression scheme for assembled_genomes motivation with the release of the latest next_generation ngs machine the hiseq x by illumina the cost of sequencing a human has dropped to a mere thus we are approaching a milestone in the sequencing history known as the genome era where the sequencing of individuals is affordable opening the doors to effective personalized_medicine massive generation of genomic_data including assembled_genomes is expected in the following years there is crucial need for compression of genomes guaranteed of performing well simultaneously on different species from simple bacteria to humans which will ease their transmission dissemination and analysis further most of the new genomes to be compressed will correspond to individuals of a species from which a reference already exists on the database thus it is natural to propose compression_schemes that assume and exploit the availability of such references results we propose idocomp a compressor of assembled_genomes presented in fasta_format that compresses an individual genome using a reference_genome for both the compression and the decompression in terms of compression efficiency idocomp outperforms previously_proposed algorithms in most of the studied cases with comparable or better running time for example we observe compression gains of up to in several cases including h sapiens data when comparing with the best compression performance among the previously_proposed algorithms availability idocomp is written in c and can be downloaded from http www stanford edu iochoa idocomp html we also provide a full explanation on how to run the program and an example with all the necessary files to run it in us president bill clinton declared the success of the human_genome calling it the most important scientific discovery of the th century although it was not until that the human genome_assembly was completed it was the end of a project that took almost years to complete and cost billion dollars per base_pair fortunately sequencing cost has drastically decreased in recent_years while in the cost of sequencing a full human_genome was around million in it dropped to a million and in to a mere www genome gov sequencingcosts thanks to illuminas latest ngs machine the hiseq x we are approaching the human_genome milestone the rate of this price drop is surpassing moores law which suggests that efficient compression will be crucial for sustaining this growth as an example the sequencing_data generated by the genomes_project www genoms org in the first months exceeded the sequence_data accumulated during years in the ncbi genbank database the compression algorithms proposed previously in the literature can be classified into two main categories i compression of raw ngs_data namely fastq and sam bam_files and ii compression of assembled data i e the compression of fasta files containing assembled_genomes see the articles byand deorowicz and grabowski a for an extended review moreover within each of these categories the compression can be made either with or without a reference we focus here on what will likely quickly become a prevalent mode compression of assembled_genomes with a reference specifically we consider pair_wise compression i e compression of a target_genome given a reference available both for the compression and the decompression although there exists a need for compression of raw_sequencing fastq sam bam compression of assembled_genomes presented in fasta_format is also important for example whereas an uncompressed human_genome occupies around gb its equivalent compressed form is in general smaller than mb thus easing the transfer and download of genomes e g it can be attached to an email moreover with the improvements in the sequencing_technology increasing amounts of assembled_genomes are expected in the near future next we show the performance of the proposed algorithm idocomp in terms of both compression and running time and compare the results with the previously_proposed compression algorithms as mentioned we consider pair_wise compression for assessing the performance of the proposed algorithm specifically we consider the compression of a single_genome target_genome given a reference_genome available both at the compression and the decompression we use the target and reference pairs introduced into assess the performance of the algorithm although in all the simulations the target and the reference belong to the same species note that this is not a requirement of idocomp which also works for the case where the genomes are from different species to evaluate the performance of the different algorithms we look at the compression_ratio as well as at the running time of both thenote each row specifies the species the number of chromosomes they contain and the target and the reference assemblies with the corresponding locations from which they were retrieved t and r stand for the target and the reference respectively compression and the decompression we compare the performance of idocomp with those of gdc green and grs when performing the simulations we run both green and grs with the default_parameters the results presented for gdc correspond to the best compression among the advanced and the normal variant configurations as specified in the supplementary data presented in note that the parameter configuration for the h sapiens differs from that of the other species we modify it accordingly for the different datasets regarding idocomp all the simulations are performed with the same parameters default_parameters which are hard coded in the code please refer to the supplementary data section iv for more information regarding the values of the default_parameters used for the simulations in idocomp as well as for the versions and options used for the other algorithms for ease of exhibition for each simulation we only show the results of idocomp gdc and the best among green and grs the results are summarized in for each species the target and the reference are as specified in to be fair across the different algorithms especially when comparing the results obtained in the small_datasets we do not include the cost of storing the headers in the computation of the final_size for any of the algorithms the last two columns show the gain obtained by our algorithm idocomp with respect to the performance of green grs and gdc for example a reduction from kb to kb represents a gain improvement note that with this metric a gain means the file size remains the same a improvement is not possible as this will mean the new file is of size and a negative gain means that the new file is of bigger size as seen in the proposed algorithm outperforms in compression_ratio the previously_proposed algorithms in all cases moreover we observe that whereas green grs seem to achieve better compression in those datasets that are small and gdc in the h sapiens datasets idocomp achieves good compression_ratios in all the datasets regardless of their size the alphabet and or the species under consideration in cases of bacteria small_size dna the proposed algorithm obtains compression gains that vary from against green grs to up to when compared with gdc for the s cerevisae dataset also of small_size idocomp does better in compression_ratio than grs gdc for the case of medium_size dna c elegans a thaliana o sativa and d melanogaster we observe similar results idocomp again outperforms the other algorithms in terms of compression with gains up to finally for the h sapiens datasets we observe that idocomp consistently performs better than green with gains above in four out of the six datasets considered and up to with respect to gdc we also observe that idocomp produces better compression results with gains that vary from to based on these results we can conclude that gdc and the proposed algorithm idocomp are the ones that produce better compression results on the h sapiens genomes in order to get more insight into the compression capabilities of both algorithms when dealing with human_genomes in the supplementary data section vii we provide more simulation_results specifically we simulate twenty pair_wise compressions and show that on average idocomp employs mb per genome whereas gdc employs mb moreover the gain of idocomp with respect to gdc for the considered cases is on average regarding the running time we observe that the compression and the decompression time employed by idocomp is linearly dependent on the size of the target to be compressed this is not the case of green for example whose running_times are highly_variable in gdc we also observe some variability in the time needed for compression however the decompression time is more consistent among the different datasets in terms of the size and it is in general the smallest among all the algorithms we considered idocomp and green take approximately the same time to compress and decompress overall idocomps running time is seen to be highly_competitive with that of the existing_methods however note that the time needed to generate the suffix_array is not counted in the compression time of idocomp whereas the compression time of the other algorithms may include construction of index structures like in the case of gdc see the supplementary data section v for details on the construction of the suffix_arrays finally we briefly discuss the memory_consumption of the different algorithms we focus on the compression and decompression of the h sapiens datasets as they represent the larger files and thus the memory_consumption in those cases is the most significant idocomp employs around gb for compression and around gb for decompression green consumes around gb both for compression and decompression finally the algorithm gdc employs gb for compression and gb for decompression inspection of the empirical results of the previous section shows the superior_performance of the proposed scheme across a wide_range of datasets from simple bacteria to the more complex humans without the need of adjusting any parameters this is a clear advantage over algorithms like gdc where the configuration must be modified depending on the species being compressed although idocomp has some internal parameters namely l d k and q the default values that are hard coded in the code perform very well for all the datasets as we have shown in the previous section see the post_processing step in section for more details however the user could modify these parameters data dependently and achieve better compression_ratios future work will explore the extent of the performance_gain which we believe will be substantial due to optimizing for these parameters we believe that the improved compression_ratios achieved by idocomp are due largely to the post_processing step of the algorithm which modifies the set of instructions in a way that is beneficial to the entropy encoder in other words we modify the elements contained in the sets so as to facilitate their compression by the arithmetic encoder moreover the proposed scheme is universal in the sense that it works regardless of the alphabet used by the fasta files containing the genomic_data this is also the case with gdc and green but not with previous_algorithms like grs or rlz opt which only work with a c g t and n as the alphabet it is also worth mentioning that the reconstructed files of both idocomp and gdc are exactly the original files whereas the reconstructed files under green do not include the header and the sequence is expressed in a single_line instead of several lines another advantage of the proposed algorithm is that the scheme employed for compression is very intuitive in the sense that the compression consists mainly of generating instructions composed of the sequence of matches m and the two sets s and i that suffice to reconstruct the target_genome given the reference_genome this information by itself can be beneficial for researchers and gives insight into how two genomes are related to each other moreover the list of snps generated by our algorithm could be compared with available datasets of known snps for example the ncbi dbsnp database contains known snps of the h sapiens species finally regarding idocomp note that we have not included inthe time needed to generate the suffix_array of the reference only that needed to load it into memory which is already included in the total compression time we refer the reader to the supplementary data section v for information on the time needed to generate the suffix_arrays the reason is that we devise these algorithms based on pair_wise compression as the perfect tool for compressing several individuals of the same species in this scenario one can always use the same reference for compression and thus the suffix_array can be reused as many times as the number of new genomes that need to be compressed 
