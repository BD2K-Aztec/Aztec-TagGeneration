sequence_analysis genome_compression a novel approach for large collections motivation genomic repositories are rapidly_growing as witnessed by the genomes or the uk k projects hence compression of multiple_genomes of the same species has become an active research area in the past_years the well known large redundancy in human sequences is not easy to exploit because of huge memory_requirements from traditional compression algorithms results we show how to obtain several times_higher compression_ratio than of the best reported results on two large genome collections human and plant_genomes our inputs are variant_call files restricted to their essential fields more precisely our novel ziv lempel style compression algorithm squeezes a single human_genome to kb the key to high compression is to look for similarities across the whole collection not just against one reference_sequence what is typical for existing solutions the dna_sequencing has become so affordable that there are several large_scale projects in which at least hundreds of individuals of some species are sequenced from many perspectives including the advent of personalized_medicine the homo_sapiens data belong to the most interesting and this is the reason why large projects like the genomes_project gp and the uk k project http www uk k org with thousands of human_genomes sequenced so_far were initiated among such projects the most ambitious perhaps is the personal genome_project pgp with genomes of individuals as the anticipated outcome large repositories are built not only for human_genomes to mention genomes_project gp with arabidopsis_thaliana genetic_variation http www genomes org about html it is a well known fact that two organisms of the same species are highly_similar it was estimated that the genomes of two persons are identical in the huge amount of data obtained in the large_scale projects demands efficient ways of storing them taking_into the high similarity of organisms it becomes obvious that some compression method may be effectively applied compression of single genomic_sequences is hardly efficient as the best obtained compression_ratios achieve a factor of or only when realized that instead of the complete_genomic storing only differences between it and some referential sequence is enough the task became easier in their seminal paper showed how to store the description of variations between james watsons jw genome and a referential genome in only mb however the authors prior_knowledge was not only the reference_sequence but also the single_nucleotide snp map as the input they took the information about the snps and insertion or deletion indels variations between jw genome and referential genome and compressed these data using some clever but simple techniques comparing with gbases of human_genome this means fold compression in the following years a number of articles on relative compression of genomes were published in all the articles the input_sequences were complete_genomes not differences between genomes and a reference_genome this complicates the compression problem as it is necessary to find the differences between genomes without any prior_knowledge and without a database of variants i e snp and indel database the most successful of the algorithms seems to be gdc which differentially compressed a collection of human_genomes from complete genomics inc to mb mb per individual it is based on the zivlempel paradigm and finds approximate matches between the genome_sequences recently showed how to improve the technique from compressing the jw genome to mb with very similar average results on multiple gp genomes the introduced novelties are partly biologically_inspired e g making use of tag_snps characterizing haplotypes another line of research concerns indexing genomic collections or more generally repetitive_sequences i e building data_structures enabling fast pattern search in the genomes such indexes are efficient when the index resides in the main computer memory which is challenging considering the sheer volume of indexed data some of the listed works are rather theoretical and their to whom correspondence should be addressed implementations are not yet available whereas the works that have been implemented are tested on relatively small collections not exceeding gb in this article we try to answer the question how well a collection of genomes of the same species can be compressed when knowledge of the possible variants is given the cited works ofare so_far the only attempts to compress a single genome_sequence with a variant database in this work we take two large collections of genomes h sapiens and a thaliana and try to exploit cross sequence correlations in the variant loci our solution is a specialized ziv lempel style compressor where the input_sequences are basically formed by binary flags denoting if successive variants from the database were found in the individuals this approach appears highly_successful allowing to store the human collection in mb kb per individual and the plant collection in mb kb per individual we point out that the general idea of exploiting common features for improved compression is known for some other ngs tasks including compression of both mapped and unmapped reads in the next section we present the input_data and the general idea of our approach then we show some details of the proposed compression algorithm finally we evaluate the compressor the last section concludes the article as mentioned earlier for the evaluation of the proposed compression algorithm we use two datasets of h sapiens and a thaliana genomes in all experiments the data are processed chromosome by chromosome this approach typical in the genomic compression literature see e g deorowicz and reduces the memory_footprint speeds up computations and improves the compression_ratio for the generic algorithms there are several tools for compressing genomic_sequences in fasta_format unfortunately the amount of our test data tb of raw sequences if converted to fasta is so huge that the running_times of some of those compressors would be counted in months thus we started from a preliminary test in which we evaluated the most powerful as well as the most recent tools for only two human_chromosomes and and also two plant chromosomes and the results are presented in this and all further experiments were performed on a computer equipped with four core ghz amd opteron cpus with gb ram running red hat linux two of the presented compressors may be considered fast with regard to the compression speed abrc with mb s run with threads and gdc normal with mb s speed only a serial implementation exists while the others are by about one gdc ultra and rlz or about two z orders_of interestingly the only generic compressor in the tests z is the second best in the compression_ratio after gdc ultra but its compression speed is low mb s the memory available for the compression has a major impact on the compression_ratio for example z was run with its maximum setting gb for its lz buffer translating to gb of total memory use yet it fit only a small part of the input for h sapiens data individuals each of size mb for chromosome dataset and individuals each of size mb for chromosome dataset this supposedly was the main_reason for which its compression_ratio in the latter case is significantly_higher the hypothesis is indirectly confirmed by the results for much shorter a thaliana chromosomes for which more individuals fit the gb lz buffer and the compression_ratios are close to gdc ultra in the next experiment we compared a few well known generic compressors gzip bzip z on vcfmin input_files compressed sizes in megabytes and compression times are reported for selected chromosomes and the collections in total surprisingly perhaps the best compression was obtained by bzip compressor is similar but now the inputs are in our temporary dense representation vdbv here the generic compressors are compared against our proposal tgc two simple observations can be made i the more compact of these two input representations vdbv is clearly more appropriate for the best generic compressor z both from the point of compression_ratio and from speed ii tgc is significantly better than vdbv z in both measured aspects which demonstrates that designing a specialized compression algorithm was a worthy goal in this case the results are summarized in for comparison purposes we also show the sizes of the raw sequences as well as the compression results of the best compressor working on such representation gdc ultra we resigned however from presenting the compression and decompression times of gdc ultra as it works on a completely different representation than vcfmin which we use in the article the compression_ratios of gdc can be treated as a reference_point the most important numbers from this summary are the average sizes of genomes in the most compact tgc representation the obtained kb for the human data is more than six times smaller than offered by the best so_far genome_sequence gdc ultra compressor also the very recent paper by working on a representation similar to our vcfmin reports more than six times larger files when expressing the compression_ratios in relation to the raw genome_sequence sizes it means that our algorithm squeezes h sapiens genomes times and a thaliana times in the last experiment we compared tgc against speedgene an algorithm for efficient storage of snp datasets for an honest comparison we restricted the gp set of variants to snps only speedgene requires the input_data to be in linkage format in which there is no distinction between chromosomes in each pair i e for each snp it describes only whether no snp is found in a genome one snp on any chromosome is found two snps are found on both chromosomes or the status of snp is unknown thus we changed our algorithm slightly and instead of processing each single chromosome we joined chromosomes of each pair and obtained vectors of dibits i e bit pairs these vectors are then transformed intonote there are two columns containing ratios ratio to raw tells how many times the compressed file is smaller than the genome_sequences in fasta_format ratio to vcfmin is the compression_ratio according to the size of vcfmin files for gdc ultra compression and decompression times are not given as this compressor uses a different input form than others and such a comparison would be irrelevant the values marked in bold indicate best compression note all sizes are in megabytes and times are in seconds the vdbv c time column contains the conversion times from vcfmin format to vdbv format in the remaining columns titled c time total compression time is given i e vdbv z c time denotes the sum vcfmin to vdbv conversion time and z compression time tgc c time is the total tgc processing time comprising the conversion to vdbv and the actual compression note that the variant database part of the vdbv representation is of size mb for h sapiens and mb for a thaliana after compressing by tgc their sizes included in tgc size column are mb and mb respectively the values marked in bold indicate best compression the extended version of this table with results for all chromosomes can be found in supplementary table s byte vectors each byte contains four consecutive dibits in this way our tool can compress these byte vectors without any change presents the compressed sizes obtained by speedgene and tgc we show the results for three chromosomes as well as for the complete genome as one can see tgc reduces the dataset size more than four times better than speedgene we examined the possibility of obtaining much better compression_ratios of genomic collections than from existing_tools when additional knowledge is given the knowledge was the information about the possible variants in genomes and the occurrence of these variants in specific genomes this helps a lot in compression of genomic_sequences as all input_sequences are perfectly aligned and the task of finding repetitions in data usually the most important and time consuming task handled by data compression algorithms becomes rather simple we should mention that in theory such perfect alignments can be found by compression algorithms but the computational_burden would be enormous thus compression tools usually make some heuristic decisions when comparing the sequences in the hope that they do not lose too much the success of our algorithm was possible not only because of the variant database but also because we searched for crosscorrelations between individuals in other words for each individual similarities to any other previously processed individual i e runs of repeating variants can be found in principle the available memory may be a limiting_factor but processing the collection on the chromosome level resulted in gb_memory use for the larger human of the tested datasets in the future when much more genomes are available we may need to re address the memory issue though possibly via working on blocks smaller than whole chromosomes or trying to re order the sequences in a way to maximize local similarities in the compression method design we sometimes traded compression_ratio for reduced memory_requirements e g some rather minor improvements in compression would be possible owing to higher_order contextual modeling probably a more practical approach is to make use of more biological_knowledge the very recent work ofgives new insight which might be possible to use in our scheme but we leave it for future work why such experiments can be interesting although accurate and efficient analyses of such huge several terabytes in raw format genomic collections remain a major_challenge we believe that the mere compressibility of human_genomes e g as a lower_bound for memory_requirements of future algorithms and tools is a question worth investigating for example our compressed collection takes mb so including also a compressed reference_genome at most mb requires gb of space which seems quite modest naturally running efficient queries over such data is another matter clearly with some overhead in space use but our results suggest this is not impossible the information kept in vcf or genome_variation format gvf files is often more detailed e g may include quality_scores than what our tool preserves although clearly efficient compression_methods for such data are also needed we do not anticipate a possibility to obtain similar compression_ratios to tgc unless a strongly lossy mode is used unfortunately we cannot see a way to easily adapt our compression techniques for such data tgc allows extracting an arbitrary chromosome or a whole_genome from the compressed collection yet this solution is simple and rather slow making this extraction faster or even better allowing for quick access to position restricted arbitrary snippets of the genomes in the collection is an important task left for future work clearly there must be some space time tradeoffs for such functionalities a somewhat related functionality will be to add or remove an individual genome to from the collection currently changing the archive content requires recompressing the collection from scratch the performed experiments showed that even the best genomic_sequence compressor gdc ultra is significantly up to seven times poorer in compression_ratio than what can be obtained with extra knowledge the main conclusions from our work are note vcfmin means a simplified vcf with snp calls only that spends only bytes for each genotype all sizes are in megabytes the values marked in bold indicate best compression 
