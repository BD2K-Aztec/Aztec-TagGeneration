sequence_analysis scalce boosting sequence compression algorithms using locally consistent encoding motivation the high_throughput hts platforms generate unprecedented amounts of data that introduce challenges for the computational infrastructure data management storage and analysis have become major logistical obstacles for those adopting the new platforms the requirement for large investment for this purpose almost signalled the end of the sequence_read hosted at the national_center ncbi which holds most of the sequence_data generated world_wide currently most hts_data are compressed through general_purpose algorithms such as gzip these algorithms are not designed for compressing data generated by the hts platforms for example they do not take advantage of the specific nature of genomic sequence_data that is limited alphabet size and high similarity among reads fast and efficient compression algorithms designed specifically for hts_data should be able to address some of the issues in data management storage and communication such algorithms would also help with analysis provided they offer additional capabilities such as random_access to any read and indexing for efficient sequence similarity_search here we present scalce a boosting scheme based on locally consistent parsing technique which reorganizes the reads in a way that results in a higher compression speed and compression_rate independent of the compression algorithm in use and without using a reference_genome results our tests indicate that scalce can improve the compression_rate achieved through gzip by a factor of when the goal is to compress the reads alone in fact on scalce reordered reads gzip running time can improve by a factor of on a standard pc with a single core and gb_memory interestingly even the running time of scalce gzip improves that of gzip alone by a factor of when compared with the recently_published beetl which aims to sort the inverted reads in lexicographic order for improving bzip scalce gzip provides up to times better compression while improving the running time by a factor of scalce also provides the option to compress the quality_scores as well as the read names in addition to the reads themselves this is achieved by compressing the quality_scores through order arithmetic coding ac and the read names through gzip through the reordering scalce provides on the reads this way in comparison with gzip compression of the unordered fastq_files including reads read names and quality_scores scalce together with gzip and arithmetic encoding can provide up to improvement in the compression_rate and improvement in running time availability our algorithm scalce sequence compression algorithm using locally consistent encoding is implemented in c with both gzip and bzip compression options it also supports multithreading when gzip option is selected and the pigz binary is available it is available at http scalce sourceforge net contact although the vast_majority of high_throughput hts_data are compressed through general_purpose methods in particular gzip and its variants the need for improved_performance has recently lead to the development of a number of techniques specifically for hts_data available compression techniques for hts_data either exploit i the similarity between the reads and a reference_genome or ii the similarity between the reads themselves once such similarities are established each read is encoded by the use of techniques derived from classical lossless_compression algorithms such as lempel ziv which is the basis of gzip and all other zip formats or lempel ziv compression_methods that exploit the similarity between individual_reads and the reference_genome use the reference_genome as a dictionary and represent individual_reads with a pointer to one mapping position in the reference_genome together with additional information about whether the read has some differences with the mapping loci as a result these methods require i the availability of a reference_genome and ii mapping of the reads to the reference_genome unfortunately genome_mapping is a time wise costly step especially when compared with the actual execution of compression i e encoding the reads itself furthermore these methods necessitate the availability of a reference_genome both for compression and decompression finally many large_scale sequencing_projects such as the genome k project focus on species without reference_genomes compression_methods that exploit the similarity between the reads themselves simply concatenate the reads to obtain a single_sequence apply modification of and to whom correspondence should be addressed we evaluated the performance of the scalce algorithm for boosting gzip on a single core ghz intel_xeon x pc with network storage and gb of memory we used four different datasets in our tests i pseudomonas_aeruginosa rna_seq library bp single lane ii p aeruginosa genomic_sequence library bp single lane iii whole_genome wgs library generated from the genome of the hapmap individual na bp reads at genome_coverage and iv a single lane from the same human wgs dataset corresponding to genome_coverage sequence_read id srr we removed any comments from name section any string that appears after the first space also the third row should contain a single character separator character the reads from each dataset were reordered through scalce and three separate files were obtained for i the reads themselves ii the quality_scores and iii the read names each maintaining the same order note that lcp reordering is useful primarily for compressing the reads themselves through gzip the quality_scores were compressed via the scheme described above finally the read names were compressed through gzip as well the compression_rate and run time achieved by gzip software alone only on the reads from the p aeruginosa rna_seq library dataset is compared against those achieved by scalce followed by gzip in the compression_rates achieved by the gzip software alone in comparison with gzip following scalce on the combination of reads quality_scores and read names are presented in the run times for the two schemes again on reads quality_scores and read names all together are presented in when scalce is used with arithmetical coding of order with lossless qualities it boosts the compression_rate of gzip between and fold when applied to reads quality_scores and read names significantly_reducing the storage requirements for hts_data when arithmetical coding of order is used with loss without reducing the mapping accuracyimprovements in compression_rate are between and in fact the boosting factor can go up to when compressing the reads only moreover the speed of the gzip compression step can be improved by a factor of interestingly the total run time for scalce gzip is less than the run time of gzip by a factor of furthermore users can tune the memory available to scalce through a parameter to improve the run time when a large main_memory is available in our tests we limited the memory_usage to gb note that our goal here is to devise a fast boosting method scalce which in combination with gzip gives compression original left and transformed right quality_scores for two random reads that are chosen from na individual the original scores show much variance where the transformed quality_scores are smoothened except for the peaks at local maxima that help to improve the compression ratiorates much better than gzip alone it is possible to get better compression_rates through mapping based strategies but these methods are several orders_of than scalce gzip we tested the effects of the lossy compression_schemes for the quality_scores used by scalce as well as cram tools to single_nucleotide snp discovery for that we first mapped the na wgs dataset with the original quality values to the human_reference grch using the bwa aligner and called snps using the gatk software we repeated the same exercise with the reads after lossy transformation of the base_pair qualities with scalce note that the parameters for bwa and gatk we used in these experiments were exactly the same we observed almost perfect correspondence between two experiments in fact of the discovered snps were the same not surprisingly most of the difference was due to snps in mapping to common repeats or segmental_duplications we then compared the differences of both snp callsets with dbsnp release in in addition we carried_out the same experiment with compressing decompressing of the alignments with cram tools as shown in quality transformation of the cram tools introduced about errors in snp_calling accuracy with respect to the calls made for the original data_set as the gold_standard one interesting observation is that of the new calls after scalce processing matched to entries in dbsnp where this ratio was only for the new calls after cram tools quality transformation moreover of the snps that scalce lost are found in dbsnp and cram tools processing caused removal of times more potentially real snps than scalce as a final benchmark we compared the performance of scalce with mapping based reordering before gzip compression we first mapped one lane of sequence_data from the genome of na same as above to human_reference grch using bwa and sorted the mapped_reads using samtools and reconverted the map sorted bam_file back to fastq using combined with bzip by a factor between and where running time is improved by a factor between and see scalce on full fastq_files also outperforms dsrc compression_ratio on complete fastq_files by a factor between and see 
