genome_analysis bias from removing read_duplication in ultra_deep experiments motivation identifying subclonal mutations and their implications requires accurate estimation of mutant_allele fractions from possibly duplicated sequencing_reads removing duplicate_reads assumes that polymerase_chain from library constructions is the primary source the alternativesampling coincidence from dna fragmentationhas not been systematically investigated results with sufficiently_high sequencing_depth sampling_induced is non negligible and removing duplicate_reads can overcorrect read_counts causing systemic biases in variant allele fraction and copy_number estimations minimal overcorrection occurs when duplicate_reads are identified accounting for their mate reads inserts are of a variety of lengths and samples are sequenced in separate batches we investigate sampling_induced in deep sequencing_data with to duplicates removed sequence_coverage we provide a quantitative solution to overcorrec tion and guidance for effective designs of deep sequencing_platforms that facilitate accurate estimation of variant allele fraction and copy_number availability_and a python implementation is freely_available at https bitbucket org wanding duprecover overview contact many somatic_mutations including known driver_mutations are found in only a subset of tumor_cells detecting the presence of these subclonal mutations and estimating their population_size can critically affect the clinical_diagnosis and therapeutic_intervention of individual cancer patients this realization has led to the rapid development of deep_sequencing as a molecular_diagnostic platform in cancer clinics estimating the variant allele fraction vaf from somatic samples sheds light on the intrinsic sample_heterogeneity that originates from somatic_mutations and hence the etiology of many diseases particularly cancer in addition genomic_regions such as genes or exons may exist in different numbers of copies due to mutational_events such as duplication and deletion this is referred to as copy_number in oncology comparisons between copy_numbers of different genes or between copy_numbers of the same gene from different samples normal versus tumor_tissue for instance disclose signs of any selective_pressure driving tumorigenesis both tasks can be approached by counting reads from next_generation ngs experiments in practice read counting is complicated by amplification_bias namely the bias as a result of the preference of the polymerase_chain pcr in reproducing reads of different lengths and compositions removing duplicate readsreads of the same length and sequence identityis a widely used practice to correct this bias when analyzing ngs_data the underlying assumption of this approach is that pcr_amplification is responsible for most of the read_duplication extending from this assumption a long_standing recognition has been held in the community that removing duplicate_reads at least does not harm the data an alternative source of read_duplication is sampling coincidence whereby inserts are fragmented at identical genomic positions during library_construction the practice of removing duplicate_reads is well justified only when the sequencing_depth is low and sampling coincidence is unlikely this was true when most ngs applications were of low sequencing_depths and were oriented toward uncovering germline_mutations from monoclonal samples however as recent_studies that aim to detect rare somatic_mutations from heterogeneous_samples have pushed sequencing_depth to a high magnitude the validity of this assumption requires serious re evaluation this article provides a quantitative_understanding of the source of read_duplication by quantifying the read_duplication that is induced by sampling coincidence by providing a statistical formulation for the bias of the allele fraction estimator based on de duplicated reads we are led to conclude that at a high sequencing_depth the practice of duplicate_read can overcorrect amplification_bias from simulations we show that the extent of overcorrection is jointly determined by the sequencing_depth the variance of the insert_size the strategy to whom correspondence should be addressed used for marking duplicate_reads and intrinsic sequence properties such as the existence of segregating sites in the neighboring region and the linkage_disequilibrium ld pattern among sites to quantify the amount of sampling_induced we applied our model and overcorrection amendment method to data from a clinical cancer sequencing_platform that produces to sequence_coverage to exons in targeted cancer_genes consistent with the currently applied assumption behind duplicate_read we found that pcr_amplification rather than sampling coincidence is responsible for most read_duplication when duplicate_reads are removed the read_depth is not as high as originally designed from the experiment reflecting an insufficient sample complexity in the experiment however for reads that are treated as single_end because the corresponding mates cannot be identified one tenth of reads sampling_induced is not rare further when we artificially mixed different deep_sequencing samples to a much higher read_depth we observed more sampling_induced as expected hence we predict that further increases in sequencing_depth or reduction in insert_size variation may lead to non negligible biases that require a method of correction such as what we provide in this article in the field of rna_seq where read_count is used to estimate transcription_level two recent_studies have taken into account natural duplication this concept is analogous to what we study in this article albeit studied without systematic investigation of segregating sites or vaf bias with that in mind the contribution of this article is fold first we call attention to the potential_bias in estimating vaf and copy_number due to overcorrecting read_counts in deep dna_sequencing particularly whole_exome for clinical_applications although duplicate_read does not lead to substantial overcorrection on the datasets we studied our simulations_demonstrate that overcorrection from duplicate_read could be substantial at smaller insert_size variances and higher read_depths second we provide insights into the design of ultra_deep experiments such that duplicate_read is most effective and overcorrection is minimal third we propose a practical computational_method for estimating the amount of sampling_induced for evaluating whether a dataset is amenable to de duplication and for amending the overcorrection through simulations we show that our methods can recover the true vaf or copy_number up to the extent permitted by the data removing read duplicates while correcting for pcr amplification_bias could introduce another bias owing to overcorrection of read_counts as a result of sampling_induced this bias is of particular concern when the sequencing is deep e g and the insert_size is short and non variant a maximum_likelihood amendment can be applied to the number of de duplicated reads to account for sampling_induced sampling_induced in most current ultra_deep experiments is not prevalent due to the presence of a substantial amount of pcr amplificationoriginated duplicate_reads nevertheless attention must be paid to duplicate_read in ultra_deep experiments that perform fewer rounds of pcr_amplification and use tightly selected insert_sizes it has been generally perceived that the accuracy of allele fraction estimation can be infinitely improved by increasing sequencing_depth the practice of duplicate_read may be a limiting_factor for this improvement this is because when read_depth is high sampling_induced becomes more common and their removal can disproportionately distort the read_count the overcorrection amendment method introduced in this article alleviates this problem yet still fails to obtain an accurate estimation of the original read_count when the number of unique reads is close to or at saturation the variance of the maximum_likelihood tends to infinity in fact as coverage increases more and more combinations of insert_size and cover are saturated the variance of the haplotype frequency_estimation is not reduced as compared with the clear enhancement of the accuracy of the estimation made from raw_reads or de duplicated reads supplementary another reason is that the number of unique reads m is constrained to integers the variance cannot be further reduced as coverage approaches the point where m l saturation see green dots and boxes in supplementary in fact as m reaches a value close to l em can never be reached thus one cannot accurately amend the vaf from the estimator p based on one single sequencing_experiment this is again in contrast to the estimator keeping duplicate_reads b p i c i n where varb p i p i p i n where the higher the coverage n the smaller the variance and hence the more accurate the estimation the choice of performing duplicate_read depends on whether the target quantity e g vaf or copy_number is distorted disproportionately amplified by read_duplication the potential cause for distortion is pcr amplification_bias and the bias introduced from removing sampling_induced insert_size variance is a key_player in determining the extent of both forms of biases data with large variances in insert_size are more susceptible to pcr amplification_bias but less susceptible to sampling_induced for such datasets duplicate_read is more appropriate on the other hand for data with sharply selected insert_sizes pcr amplification_bias is smaller whereas sampling_induced is more frequent in practice the reported sequencing_depth may be misleading as it contains a large proportion typically of amplification induced duplicate_reads these duplicate_reads do not help improve the measurement of the sample dna more specifically the number of ligated fragments that can eventually be captured by emulsion beads abi solid and roche sequencing or by forming clusters on the flow cell lawn illumina solexa sequencing is smaller than that of the sequenced_reads for most applications that aim to decipher the vaf or copy_number the more appropriate definition of read_depth should exclude pcr_amplification induced duplication based on such definition some ultra_deep datasets may not be as deep as they seem an important limitation to the true read_depth is the amount of sample dna fragmented in the initial_stage of the sequencing_experiment or the so_called sample complexity assuming that all sites from the whole_genome have equal_amounts of dna and no molecule is lost from fragmentation and size_selection the theoretical coverage limit can be calculated by w ml n a where w is the weight of the dna to start a sequencing_experiment m ng mol bp is the average molecular_weight of dna_molecules per base_pair l bp is the length of the genome and n a is avogadros constant in deep sequencing_experiments w typically ranges from to ng meaning that the upperbound of the coverage is from to this calculation assumes no loss from size_selection and ligation which is unrealistic if the procedure that includes fragmentation ligation size_selection and single_molecule capturing loses of these molecules which is not unlikely considering the chance of obtaining the desired insert_size from random fragmentation 
