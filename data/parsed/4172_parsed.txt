sequence_analysis sek sparsity exploiting k_mer based estimation of bacterial_community motivation estimation of bacterial_community from a high_throughput sequenced sample is an important task in metage nomics applications as the sample sequence_data typically harbors reads of variable lengths and different levels of biological and technical noise accurate statistical_analysis of such data is challenging currently popular estimation methods are typically time consuming in a desktop computing environment results using sparsity enforcing methods from the general sparse signal_processing field such as compressed_sensing we derive a solution to the community_composition estimation problem by a simultaneous assignment of all sample reads to a pre processed reference_database a general statistical_model based on kernel_density techniques is introduced for the assignment task and the model solution is obtained using convex_optimization tools further we design a greedy_algorithm solution for a fast solution our approach offers a reasonably fast community_composition estimation method which is shown to be more robust to input_data variation than a recently_introduced related method availability_and a platform independent matlab_implementation of the method is freely_available www ee kth se ctsoftware source_code that does not require access to matlab is currently being tested and will be made available later through the above web_site high_throughput have recently enabled detection of bacterial_community at an unprecedented level of detail the high_throughput approach focuses on producing for each sample a large number of reads covering certain variable part of the s rrna gene which enables an identification and comparison of the relative_frequencies of different taxonomic units present across samples depending on the characteristics of the samples the bacteria involved and the quality of the acquired sequences the taxonomic units may correspond to species genera or even higher levels of hierarchical classification of the variation existing in the bacterial kingdom however at the same time the rapidly increasing sizes of read sets produced per sample in a typical project call for fast inference methods to assign meaningful labels to the sequence_data a problem that has_attracted many approaches to the bacterial_community estimation problem use s rrna amplicon_sequencing where thousands to hundreds of thousands of moderate length around bp reads are produced from each sample and then either clustered or classified to obtain estimates of the prevalence of any particular taxonomic unit in the clustering approach the reads are grouped into taxonomic units by either distance based or probabilistic methods such that the actual taxonomic labels are assigned to the clusters afterward by matching their consensus_sequences to a reference_database recently the bayesian bebac method was shown to provide high biological fidelity in clustering however this accuracy comes with a substantial computational_cost such that a running time of several days in a computing cluster environment may be required for large read sets in contrast to the clustering_methods the classification approach is based on using a reference_database directly to assign reads to meaningful units representing biological_variations methods for the classification of reads have been based either on homology using sequence_similarity or on genomic_signatures in terms of oligonucleotide composition examples of homology based_methods include megan and phylogenetic_analysis von a popular_approach is the ribosomal database projects rdp classifier which is based on a na ve bayesian_classifier nbc that assigns a label explicitly to each read produced for a particular sample despite the computational simplicity of nbc the rdp classifier to whom correspondence should be addressed the author published_by all_rights for permissions please_e journals permissions_oup com may still require several days to process a dataset in a desktop environment given this challenge considerably faster methods based on different convex_optimization strategies have been recently_proposed in particular sparsity based techniques mainly compressive_sensing based_algorithms are used for estimation of bacterial_community in however used sparsity promoting algorithms to analyze mixtures of dye terminator reads resulting from sanger_sequencing with the sparsity assumption that each bacterial_community comprises a small subset of known bacterial_species the scope of the work thus being different from methods intended for high_throughput sequence_data the quikr method of uses a k_mer based_approach on s rrna sequence_reads and has a considerable similarity to the method sek sparsity exploiting k_mers based algorithm introduced here explained briefly the quikr setup is based on the following core theoretical formulation given a reference_database d fd d m g of sequences and a set s fs s t g of sample sequences the reads to be classified it is assumed that there exists a unique d j for each s l such that s l d j in general all reference_databases and sample_sets consist of sequences with highly_variable lengths in particular the lengths of reference_sequences and samples reads are often different violation of the assumption leads to sensitivity in quikr performance according to our experiments another example of fast estimation is called taxy which addresses the effect of varying sequence_lengths taxy uses a mixture_model for the system setting and convex_optimization for a solution the method referred to as compass is another convex_optimization approach similar to the quikr method that uses large k_mers and a divide andconquer technique to handle large resulting training matrices the currently available version of the matlab based compass software does not allow for training with custom databases so a direct comparison with sek is not yet possible to enable fast estimation we adopt an approach where the estimation of the bacterial_community is performed jointly in contrast to the read by read analysis used in the rdp classifier our model is based on kernel density estimators and mixture density models and it leads to solving an under determined system of linear equations under a particular sparsity assumption in summary the sek approach is implemented in three separate steps off line computation of k_mers using a reference_database of s rrna_genes with known taxonomic_classification online computation of k_mers for a given sample and then final online estimation of the relative_frequencies of taxonomic units in the sample by solving an under determined system of linear equations 
