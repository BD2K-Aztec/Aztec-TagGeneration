genome_analysis swiftlink parallel mcmc linkage_analysis using multicore cpu and gpu motivation linkage_analysis remains an important tool in elucidating the genetic_component of disease and has become even more important with the advent of whole_exome enabling the user to focus on only those genomic_regions co segregating with mendelian traits unfortunately methods to perform multipoint_linkage scale poorly with either the number of markers or with the size of the pedigree large_pedigrees with many markers can only be evaluated with markov_chain mcmc methods that are slow to converge and as no attempts have been made to exploit parallelism massively underuse available processing power here we describe swiftlink a novel application that performs mcmc linkage_analysis by spreading the computational_burden between multiple processor cores and a graphics_processing gpu simultaneously swiftlink was designed around the concept of explicitly matching the characteristics of an algorithm with the underlying computer architecture to maximize performance results we implement our approach using existing gibbs samplers redesigned for parallel hardware we applied swiftlink to a real_world dataset performing parametric multipoint_linkage on a highly consanguineous pedigree with east syndrome containing members where a subset of individuals were genotyped with single_nucleotide snps in our experiments with a four core cpu and gpu swiftlink achieves a speed up over the single threaded version and a speed up over the popular linkage_analysis program simwalk availability swiftlink is available at https github com ajm swiftlink all source_code is licensed under gplv exact linkage_analysis algorithms scale exponentially with the size of the input beyond a critical_point the amount of work that needs to be done exceeds both available time and memory the elstonstewart algorithm calculates successive conditional likelihoods between family_members permitting the algorithm to scale linearly with the size of the pedigree more precisely the number of meioses but can only analyse a handful of markers jointly the landergreen hidden_markov hmm approach on the other hand can analyse many markers but only in pedigrees of limited size in between these two extremes approaches using bayesian_networks for example superlink scale to greater numbers of markers than the elstonstewart algorithm and to larger pedigrees than the landergreen algorithm but still cannot handle both large_pedigrees and many markers in the situation where we have a large perhaps consanguineous pedigree typed with many markers we are forced to either abbreviate the input or else use an approximation like markov_chain mcmc although mcmc helps make the problem tractable it can take a long time to converge the problem of slow running time is compounded by the fact that existing software is single threaded and therefore not designed to take advantage of all the processing power available in even a single modern pc this underuse will become more acute as multicore processors feature increasing numbers of cores and graphics_processing gpus become more general_purpose parallelism has previously been applied to accelerate exact linkage algorithms in the applications fastlink across networks of workstations parallel genehunter on computer clusters and superlink online on grid middleware although parallel implementations of exact linkage algorithms perform analyses faster and can scale to larger problems owing to accessing memory in multiple machines they still have the same fundamental scaling properties as their underlying algorithms problems outside of the scope of exact algorithms must be analysed with mcmc based linkage programs such as to whom correspondence should be addressed the author published_by all_rights for permissions please_e journals permissions_oup com simwalk and morgan which have both been shown where possible to produce results of comparable accuracy to exact algorithms to the best of our knowledge there have been no published attempts to parallelize mcmc based linkage_analysis in general parallelizing mcmc is considered a hard problem because the states of the markov_chain form a strict sequential ordering successful parallel mcmc implementations tend to focus either on parallelizing expensive likelihood calculations for example or on multiple chain schemes for example the approach we took with swiftlink maximizes the use of hardware resources by combining both previous_approaches allowing different parts of the calculation to be distributed across both cpu and gpu the markov_chain is run in parallel on the cpu where each step in the chain is performed by one of two multithreaded block gibbs samplers based on the locus sampler and the meiosis sampler expensive likelihood calculations required to estimate lod logarithm_of scores are performed on the gpu if one is available high performance is achieved on the gpu by maximizing hardware use requiring us to identify substantial independent calculations this article is organized as follows we elaborate on the details of the gibbs samplers used to form the markov_chain what aspects of each sampler were changed to facilitate execution on parallel hardware and how swiftlink orchestrates these actions to maximize hardware use we report experimental_results comparing swiftlink with other mcmc based linkage_analysis software on a highly consanguineous pedigree and end with discussion and an outline of future work the single threaded variants of both locus and meiosis samplers have been shown previously to perform well on simulated_datasets that can be analysed using exact linkage algorithms in this article we focus on a real_world case study to gain an appreciation for the runtimes inherent in linkage_studies that require mcmc our case study shown in is a highly consanguineous six generation pedigree with east syndrome east syndrome is an autosomal_recessive monogenic disorder related to improper renal_tubular salt handling where patients additionally present with the following symptoms infantile onset seizures ataxia and sensorineural_deafness genotyping with snps single_nucleotide was performed on six members of the pedigree using affymetrix_genechip human mapping k snp_chips including all affected_individuals and the parents of the three affected_siblings individuals in in this article we only consider chromosome which included snps cm average spacing the disease trait iscontaining members over six generations assumed to have an allele_frequency of and to have complete penetrance we compare the runtime performance of simwalk version morgan version lm linkage program and swiftlink in several configurations simwalk was always run with default_parameters and as the runtime can be prohibitively slow for large_numbers of markers each dataset was run in windows of markers this size was chosen based on experience with past projects even with this advantage simwalk still had to be run on a computer cluster whereas morgan and swiftlink were run on a single desktop pc we report simwalks total runtime assuming sequential execution morgan by default is compiled at optimization level causing unnecessary slowdown to ensure a fair comparison we recompiled morgan at optimization level which we refer to as morgan fast following the parameters used in a previous study both morgan and swiftlink were run for a total of iterations comprising iterations of burn in and iterations of simulation plocus sampler was set to the markov_chain was initialized with runs of sequential imputation during simulation we scored every th sample and calculated lod_scores at equidistant points between each consecutive pair of markers swiftlink and morgan were both run on the same computer running ubuntu lts edition with linux kernel for bit the computer has an amd phenom ii with four processor cores clocked at ghz and gb ram clocked at ghz gpu performance was tested with cuda release candidate on an nvidia gtx each experiment was repeated_times all results are averages over all runs shows typical results for the region of interest for the east pedigree on chromosome from each of the four parallel versions of swiftlink tested the most likely linked region found by all programs corresponded to the kilobase region identified in the original study containing the kcnj gene with similar maximum lod_scores for the region of interest shows the different performance characteristics of simwalk morgan and swiftlink in addition to these results we anecdotally ran simwalk once with all markers which took days making simwalk by far the slowest even with the advantage of running in marker windows simwalk had the worst runtime of the three programs tested all versions of swiftlink were faster than both morgan and morgan fast with single threaded swiftlink showing a speed up over morgan fast this difference in runtime between single threaded programs can probably be attributed to differences in the internal representation used for pedigree peeling swiftlink peels a genotype network whereas morgan appears to peel an allele network while allele networks ultimately scale better than genotype networks in our experience a majority of pedigrees will be smaller than is necessary for this to make a difference at its fastest using four cpu cores and gpu swiftlink is faster than morgan fast and faster than simwalk analysing the east pedigree swiftlink achieves a high degree of parallelism as of the programs total runtime was spent on serial tasks that could not be parallelized using all four cores of the cpu and the gpu we achieve up to almost an order of magnitude speed up over the single threaded implementation compared with four cpu cores the addition of a gpu provides a speed up we had expected higher performance but the cpu is a bottleneck which does not run the markov_chain fast enough to keep up with the gpu we expect the situation to improve with cpus with more cores and with improvements to our cpu code so_far we have not used simd instructions to demonstrate how underused the gpu is the lod scoring code in isolation performs faster than a single threaded cpu implementation swiftlink cpux swiftlink cpux swiftlink cpux swiftlink cpux gpu results from all four tested versions of swiftlink showing region of interest from chromosome of the east pedigreethe methods presented in this article produce almost an order of magnitude speed up of mcmc linkage_analysis compared with an optimized single threaded implementation on fairly minimal hardware compared with existing mcmc linkage software swiftlink achieves up to two orders_of speed up when using four cpu cores and a gpu concurrently we believe swiftlink will be especially useful to researchers in for example clinical research settings where access and or experience with computer clusters is limited in addition swiftlink has been specifically_designed to fit into existing workflows by supporting the standard linkage file_formats ped map and data_files as other popular mulitpoint linkage_analysis programs e g allegro and genehunter a majority of linkage projects include only small to medium_size pedigrees and can therefore be analysed using exact algorithms many large_pedigrees can be successfully abbreviated to run using an exact algorithm however for cases where there are few genotyped individuals or in cases like east syndrome where one of the branches only contains a single genotyped individual any abbreviation will dramatically reduce the power of the study this is a clear niche where swiftlink can be used to great effect unlike many other articles detailing gpu applications we did not show results for both and bit as performance on the gpu is affected so strongly by register usage and therefore occupancy bit code tends to run faster as pointers take up half the space and more blocks can be run concurrently however cpu code in general performs faster in bit thanks to wider instructions in the current version of swiftlink the gpu is underused because the cpu is a bottleneck therefore it only makes sense for us to run in bit mode in the future we will be aiming to improve swiftlink by removing the bottlenecks from the slower execution of the markov_chain on the cpu additional speed ups could be made by using simd instructions as well as multithreading or by offloading some of the excess work to the gpu it is not clear how to balance the load dynamically between both platforms and whether the additional complexity is warranted for a speed up that can be provided more simply with additional cpus e g with more cores locally or in a computer cluster 
