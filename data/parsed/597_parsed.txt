automated benchmarking of peptide mhc_class binding predictions motivation numerous in silico methods predicting peptide binding to major_histocompatibility mhc_class have been developed over the last decades however the multitude of available prediction_tools makes it non trivial for the end_user to select which tool to use for a given task to provide a solid basis on which to compare different prediction_tools we here describe a framework for the automated benchmarking of peptide mhc_class binding prediction_tools the framework runs weekly benchmarks on data that are newly entered into the immune epitope database iedb giving the public access to frequent up to date performance_evaluations of all participating tools to overcome potential selection_bias in the data included in the iedb a strategy was implemented that suggests a set of peptides for which different prediction_methods give divergent predictions as to their binding_capability upon experimental binding validation these peptides entered the benchmark study results the benchmark has run for weeks and includes evaluation of datasets covering mhc alleles and more than peptide mhc binding measurements inspection of the results allows the end_user to make educated selections between participating tools of the four participating servers netmhcpan performed the best followed by ann smm and finally arb availability_and up to date performance_evaluations of each server can be found online at http tools iedb org auto bench mhci weekly all prediction_tool developers are invited to participate in the benchmark sign up instructions are available at http tools iedb org auto bench mhci join cytotoxic t cell lymphocytes ctls play_a in the immune control in vertebrates ctls scan the surface of cells and are able to recognize and destroy cells harboring intracellular threats they do this by interacting with complexes of peptides and major_histocompatibility mhc_class presented on the cell_surface many events influence which peptides from a given non self protein will become epitopes including processing by the proteasome and tap peptide trimming and t cell precursor frequencies however the single most selective event is binding to the mhc_class mhc i molecule given this large efforts have been dedicated over the last decades to the development of prediction_methods capable of accurately predicting peptide binding to mhc i molecules the large number of different methods poses a significant challenge for the end_user in terms of selecting which method is most suitable to solve a given question several articles have been published with the aim of dealing with this using different strategies such as conducting a large_scale benchmark of prediction_tools benchmarks where prediction_methods are trained and evaluated on identical datasets making large static benchmark_datasets available or by hosting a machine_learning competition that serves as a benchmark itself such large_scale benchmarks of prediction_tools are essential for researchers looking to make use of the predictions as well as for tool developers as it allows them to evaluate how novel prediction_algorithms and training strategies increase predictive_performance however performing such benchmarks in an optimal manner where all participating methods are trained and evaluated on identical datasets is a highly computationally complex task limiting participation to expert users another issue is the time lag between when the benchmark is performed and when the manuscript describing the results is published during this time developers may have updated or improved their prediction_tools meaning some of the benchmark results are instantly outdated finally when it comes to static benchmark_datasets a risk of overfitting exists leading to development of sub optimal methods lacking generalizability to novel data this is simply due to the fact that the same data are used repeatedly to evaluate and select the most optimal methods another critical issue of benchmark studies relates to the transparency of both the data used in the study and the evaluation measures the machine_learning competition in immunology mli hosted bywas a well supported competition gathering a total of participating prediction_tools likewise the mli competition attracted_significant from the community with submissions for the competition bio dfci harvard edu dfrmli html natural php being the first of their kind these benchmarks have been of high relevance for both users and developers of mhc i binding prediction_tools however for both endusers and tool developers certain aspects of the competitions were sub optimal for instance the benchmark data for the competition of mhc i binding prediction_methods were generated using a commercial assay used in few academic settings with a criterion for binding that could not readily be compared with more commonly used kd ic half_life data likewise the mli competition of ligands eluted from mhc i molecules did not clarify up front how negative peptides would be chosen how peptides for different lengths would be dealt with nor how the performance would be scored as participants in these competitions we felt that it was unfortunate that this information was not provided up front and that the best way to reduce such uncertainties was to completely automate the benchmarking process to make it completely transparent here we seek to provide a complimentary approach to benchmarking prediction_tools that addresses some of the issues listed above our approach consists of two steps first we have developed a framework for the automated benchmarking of mhc i binding prediction_methods earlier similar approaches have been taken to evaluate prediction of protein_structure the participating methods are run via a restful web_service henceforth referred to as servers hosted locally for each participating method making the effort involved in joining the benchmark minimal for tool developers the benchmark is run weekly on data newly submitted to the immune epitope database iedb thus making the source and nature of the evaluation data fully transparent furthermore to achieve the maximum degree of transparency the benchmark evaluation criteria are outlined explicitly the results of all benchmark evaluations are made publicly_available giving the public access to frequent up to date performance_evaluations of all participating methods second to overcome the problem of selection_bias in the data that are included in the iedb which is often pre selected based on certain prediction_algorithms we have developed an approach that selects a set of peptides that is highly informative in the sense that different prediction_methods disagree on how well the peptides bind we plan to run this approach once a year and test a set of the resulting peptides to provide complete transparency the script selecting the peptides in the benchmark will be made publically available the script takes a list of peptides and returns a subset of the peptides that should be measured_experimentally the resulting peptides and measurements can then be submitted to the iedb where they will automatically be identified and included in the benchmark every step from peptide selection to comparison of predicted and experimental values is performed without manual intervention 
