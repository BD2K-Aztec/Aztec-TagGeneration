bayesian evolutionary model testing in the phylogenomics era matching model_complexity with computational_efficiency motivation the advent of new sequencing_technologies has led to increasing amounts of data being available to perform phylogenetic_analyses with genomic_data giving rise to the field of phylogenomics high_performance is becoming an indispensable research tool to fit complex evolutionary_models which take into account specific genomic properties to large_datasets here we perform an extensive bayesian phylogenetic model_selection study comparing codon and nucleotide_substitution models including codon position partitioning for nucleotide data as well gene specific substitution models for both data types for the best fitting partitioned models we also compare independent partitioning with standard diffuse prior specification to conditional partitioning via hierarchical prior specification to compare the different models we use state of the art marginal likelihood estimation_techniques including path sampling and stepping_stone sampling results we show that a full codon model best describes the features of a whole mitochondrial_genome dataset consisting of protein_coding but only when each gene is allowed to evolve under a separate codon model however when using hierarchical prior specification for the partition specific parameters instead of independent diffuse priors codon_position models can still outperform standard codon models we demonstrate the feasibility of fitting such a combination of complex models using the beagle library for beast in combination with recent graphics cards we argue that development and use of such models needs to be accompanied by state of the art marginal likelihood estimators because the more traditional and computationally less demanding estimators do not offer adequate accuracy contact the startling advances in sequencing_technology have led to a dramatic_increase in the scale and ambition of phylogenetic_analyses as more and more complete_genomes are sequenced phylogenetics is entering a new erathat of phylogenomics which uses phylogenetic principles to extract information from genomic_data until recently molecular phylogenies based on a single or few orthologous_genes often yielded contradictory results one branch of the expanding field of phylogenomics aims to reconstruct the evolutionary_history of organisms on the basis of their genomes as opposed to performing single gene studies which have long dominated the field by expanding the number of characters that can be used in phylogenetic_reconstruction from a few thousand to tens_of access to genomic_data could potentially alleviate sampling problems that hampered previous phylogenetic_analyses phylogenomic analyses involve estimating the underlying evolutionary_history of sequences either as an intermediate goal or as an end_point statistical phylogenetics provides a framework for estimating historical patterns inferring intrinsic parameters of evolutionary_processes and testing hypotheses under the auspices of the neutral theory of molecular_evolution in contrast to the few genes that were previously available the complete_genomes of many species that are now accessible for inferring evolutionary_relationships constitute large_quantities of data that lead to reduced estimation errors associated with site sampling to very high power in the rejection of simple evolutionary hypotheses and to high_confidence in estimated phylogenetic patterns traditionally phylogenetic_analyses based on many genes combined data into a contiguous block a practice that is still commonly used today under this concatenated model all genes are not only assumed to evolve under the same tree but also to evolve at the same rate although multigene datasets have the advantage of providing greater resolutionwith more information it is likely to find trees that more accurately_reflect evolutionary historyit may prove challenging to account for the heterogeneous nature of the data different genes undergo different selective_pressures and the degree of site rate heterogeneity may vary from gene to gene likelihood calculations on trees have been shown to clearly benefit from accommodating different evolutionary pressures as observed in different codon positions or different genes in heterogeneous data selection is a key evolutionary_process in shaping genetic_diversity and a major focus of phylogenomics investigations using statistical_methods in evolutionary_genetics researchers frequently evaluate the strength of selection operating on genes or even individual codons in the entire phylogeny or in a subset of branches codon_substitution have been particularly useful for this purpose because they allow estimating the ratio of non synonymous and synonymous_substitution dn_ds in a phylogenetic framework two different versions of codon_substitution were simultaneously introduced that both allowed estimating a single dn_ds ratio across all sites and branches various extensions have since been proposed such as codon models that model variation in dn_ds among sites complemented with empirical_bayes approaches to identify the sites under specific selection regimes codon_substitution can to some extent be approximated by partitioning nucleotide models according to codon positions which accommodates differences in evolutionary_dynamics at the three codon positions for example yang a b takes_into the nucleotide frequency bias the substitution_rate bias and the difference in the extent of rate variation among the three codon positions and shows that incorporating these features can yield drastically different divergence time estimates compared with models not incorporating this complexity although full codon models approximate biological reality more closely codon_position models have the advantage to be far more computationally_efficient while yang a b only used the hky evolutionary model in his analyses also included the gtr model tavare tavare in a comprehensive model evaluation study see also showed that codon_position models are biologically_motivated computationally practical alternatives to codon models for the analysis of protein_coding it is therefore not surprising that such approaches are also being exploited for detecting positively_selected the large state_space of full codon_substitution renders these models computationally_expensive compared with standard nucleotide_substitution models which explains why they are frequently fit to trees to scrutinize selection_processes but generally not used to reconstruct phylogenies however as computational power has increased phylogenetic_inference using codon based models is becoming more and more realistic here we examine whether the use of full codon models is feasible in the phylogenomics era by fitting several codon models to a full genome mitochondrial dataset in a bayesian_framework we use state of the art model_selection approaches to assess whether increased biological realism as obtained by modelling gene specific properties goes hand in hand with increased model performance finally we provide estimates of computation time required on both multi core cpu systems and a system equipped with one of the latest graphics cards available we first compared the standard hky and gtr nucleotide models assuming varying_rates across sites modelled using a discrete gamma_distribution with four rate categories yang a b with the hky based and gtr based codon_position models analysed in the work ofand the codon model of goldman and yang also accommodating varying substitution_rates across codons for each of the models the marginal likelihoods were calculated assuming both a strict clock and a uncorrelated relaxed clock with an underlying lognormal distribution ucld the results of this model comparison are listed in we consider the common codon partition cp models where all three codon positions are considered separately denoted cp and where the first and second codon position are grouped together denoted cp as the third codon positions generally evolve much faster than the first and the second codon positions none of the marginal likelihood estimators inselects the gy codon model as the best fitting mode indicating that increased biological realism offered by explicitly_model substitution in codon space is not reflected in an increased model_fit the model that seems to most adequately capture the substitution complexity in the data is a full fledged codon_position model which assumes a separate gtr model for each codon position as well as different rate heterogeneity patterns and nucleotide frequency compositions across codon positions the partitioning also specifies different relative rates for the codon positions the various estimators consistently select that model as the best model with apparently only small differences in the ranking of the models we note however that the hme and shme yield drastically different estimates for the log marginal likelihood compared with ps and ss which supports earlier claims that such posterior based estimators overestimate the marginal likelihood the gy codon model ranks among the various codon_position models albeit in different parts of the overall ranking depending on the estimator used standard nucleotide models with or without varying_rates across lineages appear to be too simplistic for this dataset as established by all the model_selection approaches for each of the models tested a relaxed molecular_clock with underlying lognormal distribution ucld is consistently shown to outperform a strict clock next we introduce gene specific partitioning for the models tested in to this purpose we assume one model instance for each of the genes present in the dataset resulting for example in gy codon models being estimated and gtr models being estimated for the most parameter rich codon position partitioned models along with gamma distributions being estimated and sets of empirical frequencies being used for each of these models we have calculated marginal likelihoods assuming both a strict and a relaxed clock note posterior based model_selection approaches i e hme and shme were run for million iterations of which million serve as burn in million for the codon models while ps and ss were run for path steps ratios with iterations iterations burn in per path step ratio after an initial million iterations that serve as burn in million for the codon models rescaling in beagle was set to default delayed the gene specific partitioning tested inyields a different model ranking compared with whereas a gene independent codon model was outperformed by its codon_position approximation this is no longer the case when gene specific evolutionary_patterns are taken into account the gene specific codon model that allows for different relative rates between the different genes with a different gamma_distribution to model varying_rates across sites within each gene achieves the best performance as assessed by both ps and ss however the posterior based model_selection approaches hme and shme fail to detect this and also other subtle differences can be observed for these estimators when comparing various versions of the gene specific codon_position models in the strong tendency to overestimate marginal likelihoods by the hme is also reflected inreveals that each of the models analysed benefits from taking_into gene specific properties of the dataset studied i e that it is composed of protein_coding seefor gene specific estimates of the key_parameters of the codon model one gene that stands out from all the others is the atp gene exhibiting a dn_ds ratio that is between to times_higher than that of the other genes and the lowest transition transversion ratio of the genes considered further the atp gene is the only gene with a rate heterogeneity parameter lower than implying that most sites have very low substitution_rates yang a b but few sites also have high rates whereas all the other genes have on average more intermediate rates across sites the cox cox cox cytb and nd genes have clearly lower dn_ds ratios and the cytb nd and nd genes have a much higher transition transversion ratio finally allowing for different relative rates between the different genes shows that certain genes such as atp and cytb may evolve twice as fast as some of the other genes e g atp cox and cox gene partitioning allows capturing variation in the substitution process but considerably increases the number of parameters that need to be estimated in particular for the gtr based codon_position substitution models because standard diffuse priors offer little protection againstnote posterior based model_selection approaches hme and shme were run for million iterations of which million serve as burn in million for the codon models while ps and ss were run for path steps ratios with iterations iterations burn in per path step ratio after an initial million iterations of burn in million for codon models rescaling in beagle was set to default delayed over parameterization and prior specification impacts marginal likelihood estimates we also explore hpm approaches allowing to share_information between different genes through hierarchical prior specification we put hierarchical priors on the gene specific and parameters of the gy codon model on the parameters and the five free evolutionary_parameters of the gtr model log marginal likelihoods shown in were calculated using the same settings as in tables and the hpm approach offers marked improvements of the marginal likelihoods and as expected this is more pronounced for the parameter rich codon_position models about log_units higher compared with the codon model about log_units higher as a consequence a gtr based nucleotide model partitioned according to gene and codon position now yields the highest marginal likelihood similar to the partitioning with independent prior specification we can notice discrepancies in model_selection outcome between the hme and shme on the one hand and ps and ss on the other in the past_decades codon models have largely been avoided for phylogenetic_reconstruction because they are computationally prohibitive recent development of dedicated apis and high_performance libraries have however made it possible to harness the large_numbers of computing cores available in graphics cards which is particularly useful in the case of datasets with many unique site patterns such as the one analysed here at the time of the introduction of these techniques in statistical phylogenetics the dataset analysed here could not be analysed in double precision on what were state of the art graphics cards at that time because they were limited in the amount of memory hence to analyse this dataset using codon models three graphics cards were combined in the analysis of this dataset amounting to a considerable cost we compare the performance of a gtx graphics card and a core xeon r e ghz system using a auto sizing thread pool two different beagle rescaling schemes and different numbers of beagle instances both for a single codon model and a codon model for each gene we demonstrate that for a single codon model and using a delayed rescaling scheme the default in beagle with a single core instance a state of the art graphics card offers a speed increase with a factor over compared with a singlecore instance on a modern cpu running beast i e without beagle because the number of beagle instances divides the likelihood calculations into two or more parts thereby allowing each core to calculate part of the likelihoods corresponding to unique site patterns multi core or multi gpu systems may benefit from using two or more beagle instances the speedup further increases to a factor when two such beagle instances are used which allows for both gpus on the graphics card to be used and half of the site patterns being calculated on each gpu the speed increase provided by a multi core cpu system levels off at a factor of with the maximum performance being reached when using approximately cores illustrating that a costly multi core cpu architecture cannot achieve the same degree of speed ups as a graphics card no further increases in performance by using additional cores could be obtained although we cannot exclude communication latency as a possible cause for this observation the most likely explanation is the considerable difference in memory bandwidth between both systems with the gtx sporting a theoretical total memory bandwidth of gb s much higher than the performance of a multi core cpu system such as ours yielding a typical memory bandwidth of gb s benchmarked using a multithreaded version of stream the situation is different when attempting to fit independent codon models to each of the genes and each with its own gamma_distribution to model site heterogeneity the partitioning strategy implies a change in the meaning of beagle instances the specification of partitions translates into likelihoods that need to be evaluated which are naturally distributed over multiple cores as instances the specification of more beagle instances partitions the likelihood calculations even more by splitting further each partition e g two instances would yield sets of likelihood this implies that there is little benefit in using instances on the gtx graphics card as each of the gpus already handles six likelihood sets however this approach may still profit from a core cpu system where initial increases in the number of beagle instances yield the highest speedups but there is a diminishingthe bayesian phylogenetic model comparison we present here consistently shows that partitioning by gene yields an increased model_fit using standard diffuse priors a separate codon model for each gene accompanied with gene specific among codon rate variation and gene specific relative substitution_rates offers the best performance followed by codon partition models and trailed by standard nucleotide models however when substituting the independent diffuse priors by a hierarchical prior specification over the gene specific parameters a more parameter rich gtr based nucleotide_substitution model partitioned according to gene and codon position emerges again as the best fitting model these results can only be uncovered using recent model_selection approaches such as path sampling and stepping_stone sampling by demonstrating an increased model_fit for gene partitioning we corroborate the results of earlier_studies e g by who combined morphology and nucleotide data from four genes in a study on model heterogeneity across data partitions through bayes_factor comparisons the authors showed a dramatic_increase in model_fit when extending twopartition models one partition for the morphology data and one joint partition for the four genes to five partition equivalents one partition for each of the four genes emphasizing the importance of accommodating across partition heterogeneity the authors also showed that within partition rate variation was by far the most important model component i e much more than across partition heterogeneity but that the difference in fit between substitution models was only pronounced when comparing jc and gtr tavare tavare it is important to note that the bayes_factor comparisons reported in are calculated using the hme because convincing_evidence has been presented for the poor_performance of the hme in recent_years b we advocate for caution when interpreting such results and encourage the use of ps and ss over hme and shme consistent with the increased model_fit for the gene partitioned models we observed considerable variation in evolutionary_parameters across the mitochondrial_genes we examined whether this parameter variation observations could be associated with the asymmetrical mutation bias gradients in vertebrate mitochondrial_genomes faith and pollock andobtained the same relative gene_order with respect to the duration of the single_strand state of the parental h strand cox cox atp atp cox nd nd l nd nd nd nd cytb when comparing the relative orders of and the relative rate inagainst this relative gene_order only ranking according to the relative rate results in a similar order with the exception of atp and the nd genes cox cox atp cox nd l nd nd nd nd nd atp cytb our comparison of independent diffuse priors and hierarchical priors for the gene partitioned models illustrates the impact of prior specification on marginal likelihood estimation and model_selection we have previously highlighted that the outcome of bayesian model_selection is dependent on prior choices through hierarchical prior specification hpms offer a middle ground between the extreme scenarios of independently fitting different models across genes and fitting a single model to all genes while accommodating parameter variation among genes will be appropriate in most cases there is less information available in each gene to inform the gene specific parameters hpms allow borrowing of strength of information from one partition by another providing more precise gene specific parameter_estimates and resulting in further model improvements in our comparisons beast supports a generic implementation of hierarchical prior specification and hpm approaches can therefore be applied to different problems such as hiv within host evolution for different groups of patients and phylogeographic problems by adopting hierarchical prior specification across gene specific parameters in gtr nucleotide_substitution models with both gene and codon position partitions we notice a better model_fit compared with gene partitioning with a standard codon_substitution however we are essentially comparing the most complex parametrization among conventional nucleotide_substitution models with the simplest codon substitution model and many assumptions can still be relaxed to make codon models more realistic to illustrate this point we observe a marginal likelihood improvement of about log_units or more between the hky based and gtr based codon_position models so we also expect model_fit improvements for a codon model that would consider different substitution_rates for the different types of nucleotide_substitutions within a codon instead of merely distinguishing between transitions and transversions as is done in the gy model such a gtr type of model applied at the nucleotide level but with the constraint that the nucleotide_sequence must encode some fulllength amino_acid is the rationale of the codon_substitution in the style of the codon model of muse and gaut the codon model of muse and gaut may offer a more realistic parameterization than the gy model which has no natural mechanistic interpretation at the nucleotide level resulting in a possible increase in model_fit over the gy model more importantly codon models can model varying selective_pressure but a gene specific dn_ds is a very coarse approximation of this variation and many realistic codon codons now accommodate among site variation in dn_ds in dn and ds separately kosakovsky and or among lineage variation in dn_ds further research is needed to implement such models in beast and to assess their model_fit for each of the models tested in this manuscript be it with or without gene partitioning a relaxed molecular_clock with underlying lognormal distribution ucld is shown to outperform a strict clock using all of the estimators given that mammalian datasets of mitochondrial_dna exhibit a wide variation in substitution_rate across lineages additional clock models such as autocorrelated see e g or random local clocks should ideally be included in our model comparison for example have shown that the distribution of estimated mitochondrial substitution_rates across species shows a very large variance with the rates spanning two orders_of the authors also show that the family taxonomic_level explains of this variance while the order taxonomic_level explains indicating that entire orders could all have for example low substitution_rates which may be appropriately modeled using autocorrelated relaxed_clock finally we have reported massive increases in computation speed using the beagle library for beast in combination with the latest graphics cards however the gtx we used here is essentially designed to offer tremendous single precision performance as required for visualization purposes in the gaming community hence its double precision performance is not keeping the same development pace which will also be the case for future cards from the same series double precision performance has recently increased with the advent of a new line of nvidia tesla k graphics cards designed for scientific computing further research will be needed to determine to what extent these new cards can improve_efficiency in the field of phylogenetics 
