gene_expression slim a sliding linear_model for estimating the proportion of true null_hypotheses in datasets with dependence structures motivation the pre estimate of the proportion of null_hypotheses plays_a in controlling false_discovery fdr in multiple_hypothesis however hidden complex dependence structures of many genomics datasets distort the distribution of p values rendering existing estimators less effective results from the basic non linear_model of the q value method we developed a simple linear algorithm to probe local dependence blocks we uncovered a non static relationship between tests p values and their corresponding q values that is influenced by data_structure and using an optimization framework these findings were exploited to devise a sliding linear_model slim to more reliably estimate under dependence when tested on a number of simulation datasets with varying data dependence structures and on microarray_data slim was found to be robust in estimating against dependence the accuracy of its estimation suggests that slim can be used as a stand alone tool for prediction of significant tests availability the r code of the proposed method is available atexperiments in the omics fields often involve hundreds to thousands of dependent_variables such as those encountered in genetic_linkage gene_expression and metabolic_profiling multiple_testing is necessary in order to identify statistically significant_variables for subsequent analyses without a flood of false_positives called by chance the false_discovery fdr approach and its many variants including the positive fdr pfdr based q value statistic have been widely used for false discovery control in multiple_hypothesis the q value represents the minimum pfdr that can occur for any possible greater than or equal to a p value point given a set of p values ranked in an increasing order p i i m m is the total number of tests to whom correspondence should be addressed the q value is calculated as this formula indicates that is the only unknown parameter to be pre estimated the accuracy of the estimate directly affects the q value calculation and the optimal_control of fdr a reliable estimate also provides a simple prediction of the number of genes that are differentially_expressed under the experimental_conditions in gene_expression analysis a widely used method is the estimator i e where is a pre chosen cutoff and p i is the number of p values greater than considering the non linear relationship between and we refer to this estimator as the non linear_model the underlying assumption is that the largest p values are most likely to come from a uniform_distribution of null features in the range in practice there is a bias versus variance tradeoff for choosing an optimal for the estimation of to balance this trade_off storey and tibshirani used a natural cubic spline smoothing css method to fit the nonlinear relationship across a range of as implemented in the qvalue software proposed an average estimate ae method which takes_advantage of multiple non linear estimators to reduce the estimation variance markitsis and lai recently_proposed a censored beta mixture_model cbmm based upon the beta uniform mixture bum method of pounds and to approximate the p value distribution both the original and the q value based fdr procedures were developed for independent test_statistics although these methods can be applied to weakly dependent data they are unreliable for datasets with inherent multiplicity and complex dependence the bum based_methods also cannot effectively handle irregular p value distribution to specifically handle multiple testing under dependence efron a developed an empirical bayesian_framework called locfdr to remedy the effects of data correlation however locfdr is only applicable to data with a large we have developed a linear estimator from the non linear method of storey to explore local properties of p value distributions as a means to better capture data dependence when applied to data with a uniform null p value distribution the slope and intercept of the linear_model reflect the proportions of nullan important issue in multiple_hypothesis is how to deal with the dependencies hidden among thousands of tests efron b has shown that correlation among variables considerably changes the theoretical null distribution_patterns we also observed that dependence structures lead to distorted distributions of null p values and this likely underlies the relatively large estimation errors by methods developed under the assumption of data independence the locfdr approach was specifically_designed to handle data containing dependence structures but it is only applicable when is large cbmm uses a censored beta uniform mixture_model to fit the distorted p value distribution alleviating to some degree the difficulty caused by dependence slim is based on a linear_model transformed from the non linear estimator the superior_performance of slim can be ascribed to its data partitioning and optimization schemes slim uses a sliding linear_model to partition data into local dependence blocks this reduces data complexity while enabling slim to utilize information from a broader range of p value distribution for estimation using simulated_data we uncovered a non static relationship between p values and q values of a given set of tests that is influenced by data_structure and scenarios slim employs an optimization scheme to explicitly exploit this relationship by minimizing the difference l between the fractions of tests called significant by the p value and q value methods the optimization scheme is particularly important to balance between positive_and errors thereby achieving fdr control thus slim effectively handles hidden dependence without the need to empirically adjust the null p value distributions the selection of a proper q value cutoff in multiple_hypothesis is not trivial especially given the dependence of the q value calculation on the estimation recalling that the number of significant tests in a given experiment is simply m we argue that an accurate estimation of can serve as an alternative to q value based significance testing using simulated_data slim was shown to outperform the other methods achieving the lowest fdr overall accompanied by the highest degree of accuracy in declaring significant tests this suggests that slim can be used as a stand alone tool in multiple testing for determination of significant tests in summary slim is a robust estimator especially suited for datasets with non uniform p value distribution_patterns due to data dependence slim is computationally_efficient and easy to implement it requires four user selected parameters n p max and b the latter two are proxies of the quantile parameter we recommend n b and p max as the default_settings users may wish to test a higher b to ensure sufficient granularity and a range of p max for optimal selection and estimate in addition to microarray_analysis slim has been applied to metabolite_profiling analysis in our laboratory and should be applicable to a wide_range of experiments 
