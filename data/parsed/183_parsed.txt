a high_throughput framework to detect synapses in electron microscopy_images motivation synaptic_connections underlie learning and memory in the brain and are dynamically formed and eliminated during development and in response to stimuli quantifying changes in overall density and strength of synapses is an important prerequisite for studying con nectivity and plasticity in these cases or in diseased conditions unfortunately most techniques to detect such changes are either low_throughput e g electrophysiology prone to error and difficult to automate e g standard electron_microscopy or too coarse e g magnetic_resonance to provide accurate and large_scale measurements results to facilitate high_throughput analyses we used a year_old experimental technique to selectively stain for synapses in electron microscopy_images and we developed a machine_learning framework to automatically detect synapses in these images to validate our method we experimentally imaged brain_tissue of the somatosensory_cortex in six mice we detected thousands of synapses in these images and demonstrate the accuracy of our approach using cross_validation with manually labeled data and by comparing against existing algorithms and against tools that process standard electron mi croscopy images we also used a semi_supervised algorithm that leverages unlabeled data to overcome sample_heterogeneity and improve performance our algorithms are highly_efficient and scalable and are freely_available for others to use the mammalian_brain can contain hundreds of millions of neurons each with thousands of specialized connections called synapses that enable indirect communication between cells estimates for the number of synapses in the mammalian_brain ranges into the trillions synapses are essential for the transfer of information across neuronal ensembles and individual synapses can be modulated by patterns of incoming neural_activity a phenomenon thought to underlie learning and memory changes in the relative_strength and number of synapses can be regulated by a myriad of factors including developmental age sensory experience drug_addiction estrus cycle and brain_pathology for example in a form of autism linked to mutation of the fragile x gene spine_density in the neocortex is elevated a feature that has also been observed in mice_carrying the same genetic_mutation rett_syndrome another neurodevelopmental_disorder is characterized by smaller brain_size caused by deficits in synaptogenesis that results in fewer spines similarly in alzheimers_disease and other dementias cognitive_deficits are associated with reduced synapse density in the hippocampus a brain structure critical for learning understanding how connectivity across neurons can change is thus an important question that drives contemporary neuroscience_research because synapse distribution is a useful and diagnostic_criterion to evaluate circuit_function in learning and disease there have been a variety of methods used to estimate synaptic_connectivity or overall synapse numbers electrophysiological_methods to estimate connectivity and the number of inputs per cell can be informative e g but these approaches are low_throughput and can typically only capture tens or hundreds of connections in reasonable amounts of time mri based techniques can be used to study network function at the level of brain_regions or voxels but they do not provide enough spatial_resolution to estimate neural connectivity anatomically synapse densities are measured via light_microscopy to identify specialized substructures called spines that stud the dendrites of neurons or using electron_microscopy em to identify ultrastructural_features that correspond to pre_and elements traditional approaches have used cumbersome manual detection to count synapses in these images e g and were thus constrained to small_scale measurements or required the use of specialized transgenic_animals limiting their usage for studying plasticity and development in wild_type since the early s bioimage_informatics has emerged as an important area in the analysis of biological images imaging datasets are usually much larger than other highthroughput biological_datasets e g confocal_microscopy data can range in the hundreds of gigabytes for a single imaging session accurately identifying elements of interest molecules cells synapses etc within these massive datasets requires the development of sophisticated and efficient computational_models this often involves a classification based strategy in which a small manually labeled training_set is used to learn a general model that can be used to analyze a larger collection of images automatically the key computational challenges involve the reliability and speed at which the analysis is done as well as dealing with the heterogeneity of biological structures and noise to whom correspondence should be addressed the author published_by this is an open_access the terms of the creative_commons http creativecommons org_licenses which permits non commercial re use distribution and reproduction in any medium provided the original_work for commercial re use please_contact permissions_oup com present in each image electron_microscopy data suffer particularly from these problems and often contain undesired variation in intensity and contrast within and across samples and preparations this presents a major computational challenge because model_parameters learned from one sample may not generalize to other samples although reconstruction and segmentation of conventional em images has helped answer important questions about brain structure and function these approaches have yet to reach the point of full automation which has limited their scale and accuracy 
