sequence_analysis pe assembler de_novo assembler using short paired_end motivation many de_novo genome_assemblers have been proposed recently the basis for most existing_methods relies on the de_bruijn a complex graph_structure that attempts to encompass the entire_genome such graphs can be prohibitively large may fail to capture subtle information and is difficult to be parallelized result we present a method that eschews the traditional graph based_approach in favor of a simple extension approach that has potential to be massively parallelized our results show that it is able to obtain assemblies that are more contiguous complete and less error_prone compared with existing_methods availability the software_package can be found atde novo genome_assembly has been a fundamental problem in bioinformatics since the advent of dna_sequencing the secondgeneration sequencing_technologies such as illumina solexa and abi solid have introduced a new sense of vigor to the field the short_length of the sequences coupled with high coverage and high_level of noise has transformed de_novo to a tractable yet challenging proposition the ease at which paired_end libraries can be generated on these platforms is an added advantage a number of works have been proposed to assemble short_reads the first few de_novo assemblers developed to handle highthroughput short_reads were based on base by base extension ssake vcake and sharcgs are examples using this principle to resolve ambiguities these methods adapted trivial heuristics such as selecting the base with maximum overlap or selecting the base with the highest consensus such arbitrary criteria results in substandard assemblies that were often a compromise between contiguity and error_rate furthermore the approaches were not scalable to handle medium or large_genomes therefore their use is restricted to assembling bac clones or small bacteria genomes they were also not designed to make use of paired_end thus greatly limiting their usefulness in assembling high_throughput data to whom correspondence should be addressed the more practical approaches for assembling high_throughput short_reads have spawned based on de_bruijn approach velvet is perhaps the most widely used method for de_novo today it is very fast in execution fairly memory_efficient and produces reasonably accurate assemblies similar to all other methods based on de_bruijn velvet requires the entire_genome to be stored in a graph_structure in the presence of noise the graph may be too large to be stored on system memory furthermore resulting assembly generated from velvet tends to contain many errors at small repeat regions another approach euler usr is very similar in concept to velvet but employs more sophisticated error_detection and correction steps however in practice we noted velvet produces more contiguous and complete assemblies in comparison with eulerusr both velvet and euler usr take full advantage of paired_end libraries one of the major shortcomings of de_bruijn approaches is the inability to parallelize the assembly process this is a critical requirement as many powerful computers utilize multiple processors where numerous threads can be run seamlessly in parallel introduction of abyss tackled this issue the core assembly algorithm of abyss is very similar to that of velvet but it allows de_bruijn to be distributed across multiple cores nodes and each core node can operate on the graph independently to a certain extent the assembly result of abyss is similar to that of velvet however we noticed that when executed in parallel in a multi core single computer abyss does not offer any advantage over velvet in term of execution time or memory_usage to utilize abyss efficiently it requires a multi node computing cluster that may seem a disadvantage in an era where computers are increasingly made faster by adding more cores within a single_cpu soapdenovo addressed many of these issues by introducing a de_bruijn graph_based method that can seamlessly takes_advantage of multi core systems allpaths allpaths appears to be the most accurate method at present it introduces an interesting hybrid_approach where the genome is still stored as a large graph however the graph is separated into different segments and assembly of these segments can be carried independently this makes it possible to run some stages of allpaths algorithm in parallel the high_accuracy of allpaths is contributed by the fact that it tries all possible ways to assemble every segments however this comes at a tremendous cost in terms of time and memory_usage and therefore it will not augment well for larger genomes we propose the method pe assembler that is capable of handling large_datasets and produces highly contiguous and accurate assemblies within reasonable time our approach is based on simple extension approach and does not involve representing the entire_genome in the form of a graph fundamentally it is similar to other extension approaches such as ssake vcake and sharcgs however it improves upon such early approaches in multiple_ways the extensive use of paired_end ensures that the dataset is localized within the region hence our method can be run in parallel to greatly speedup the execution while staying within reasonable system requirements ambiguities are resolved using a multiple path extension approach which takes_into sequence_coverage support from multiple paired libraries and more subtle information such as the span distribution of the paired_end pe assembler has demonstrated that it is possible to obtain complete and highly_accurate de_novo genome_assemblies using high_throughput sequencing_data within reasonable time and memory constraints the highlight of pe assembler is that it eschews the traditional graph based_approach in favor of a simple extension approach the advantages of this approach are numerous memory_requirements of graph based_approaches seem to increase exponentially as genome and data size increase this was highlighted by the inability of velvet and allpaths to cope with simulated hg chr dataset in contrast pe assembler produced a very usable assembly within a realistic memory limit our approach is fundamentally similar to other extension approaches such as ssake sharcgs and vcake but distinguishes itself due to its extensive use of paired_end not only does it make such approach scalable to larger genomes datasets by localizing data it also contributes to its high_accuracy as evident from both simulated and experimental_data results pe assembler is the least prone of all algorithms to misassemble different regions of the genome in a continuous segment perhaps the most important aspect of pe assembler is its ability to seamlessly parallelize the assembly process multiple threads can simultaneously assemble the genome at various positions across the genome while a simple detection mechanism will ensure that multiple assemblies of the same region are highly unlikely also noteworthy is that parallel assembly in pe assembler does not come at an extra cost in memory as in other methods such as allpaths or abyss being able to massively parallelize the assembly process at no extra overhead it will prove valuable in assembling mammalian_genomes as well as in larger metagenomics projects with minor modifications this approach can be extended to be run in a computer cluster across multiple nodes to further decrease the running time 
