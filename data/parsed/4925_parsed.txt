genome_analysis wiggletools parallel processing of large collections of genome_wide datasets for visualization and statistical_analysis motivation using high_throughput researchers are now generating hundreds of whole_genome assays to measure various features such as transcription_factor binding histone_marks dna_methylation or rna_transcription displaying so much data generally leads to a confusing accumulation of plots we describe here a multi_threaded library that computes statistics on large_numbers of datasets wiggle bigwig bed bigbed and bam generating statistical summaries within minutes with limited memory_requirements whether on the whole_genome or on selected regions with the advent_of research teams and consortia are generating large_numbers of datasets that are projected onto the same reference_genome in particular epigenomic assays quantify many continuous variables across the genome e g transcription_factor binding histone_marks dna_methylation chromatin_structure or rna_transcription although they differ in their protocols all the above assays include a sequencing step that generates a huge number of sequencing_reads these reads or tags are then aligned against the human_genome this placement information is normally stored in the bam file_format because the bam_files are generally large and information rich they are often summarized into bigwig files that describe a numerical variable such as read_depth across the genome these bam and bigwig files can then readily be displayed on most genome_browsers in the current context where researchers are testing many measurements across many samples displaying all these data creates confusing graphics either the plots are placed sideby side and an observer is forced to continually shift their attention from one plot to another or the plots are superimposed blurring the information content instead one could summarize all these datasets for each position in the genome similarly one could display the difference between case and control datasets fundamentally all of these datasets are simply vectors of numbers and statistics such as mean variance median etc can be generated from any such collection producing a meaningful summary of the data common statistical_tools such as r r core do not scale well to such large_datasets especially with respect to memory_requirements therefore we developed a tool that can perform rigorous statistical_tests across the whole_genome and detect regions of interest without practical memory constraints we drew inspiration from the popular bedtools package which computes overlaps and derived statistics between sets of regions converting numerical measurements into genomic_regions generally referred to as peak_calling or segmentation depending on the context is a convenient and common approach to handling genome_wide data however it does imply an inevitable loss of information as continuous variables are discretized and often binarized therefore we wanted a tool that natively reads the numerical_data contained in genomic files and computes statistics on it 
