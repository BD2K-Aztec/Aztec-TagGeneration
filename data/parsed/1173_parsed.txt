corpus domain effects on distributional semantic modeling of medical terms motivation automatically quantifying semantic_similarity and relatedness between clinical terms is an important aspect of text_mining from electronic_health which are increasingly_recognized as valuable sources of phenotypic_information for clinical genomics and bioinformatics_research a key obstacle to development of semantic relatedness measures is the limited availability of large_quantities of clinical text to researchers and developers outside of major medical_centers text from general english and biomedical_literature are freely_available however their validity as a substitute for clinical domain to represent semantics of clinical terms remains to be demonstrated results we constructed neural_network representations of clinical terms found in a publicly_available benchmark_dataset manually labeled for semantic_similarity and relatedness similarity and related ness measures computed from text corpora in three domains clinical_notes pubmed central articles and wikipedia were compared using the benchmark as reference we found that measures computed from full text of biomedical articles in pubmed central repository rho for similarity and for relatedness are on par with measures computed from clinical reports rho for similarity and for relatedness we also evaluated the use of neural_network based relatedness measures for query expansion in a clinical document retrieval task and a biomedical term word sense disambiguation task we found that with some limitations biomedical articles may be used in lieu of clinical reports to represent the semantics of clinical terms and that distributional semantic methods are useful for clinical and biomedical natural_language applications automated approaches for representing the semantic content of terms and similarity and relatedness between them have been widely used in a number of natural_language nlp applications in both general english knowledge_discovery among many other seefor a comprehensive_review in the general english domain distributional semantic approaches to measuring semantic_similarity and relatedness have been quite successful achieving correlations in the s and s with human judgments in the biomedical_domain the problem of representing lexical semantics of medical terms in such a way as to match human judgments of semantic_similarity and relatedness between them has proven to be more challenging state of the art approaches developed so_far for computing semantic_similarity and relatedness achieve only modest agreement with human judgments this applies to both distributional methods i e knowledge free and those based on relations in manually constructed ontologies such as wordnet and the unified_medical i e knowledge based for example garla and brant reported on a large systematic investigation of a wide_range of knowledge based and knowledge free approaches to computing semantic relatedness and similarity and found that knowledge based_approaches augmented with information content obtained from corpora of text e g outperformed distributional approaches based on first and second order semantic vectors e g lin and patwardhan and the authors used a number of publicly_available benchmarks including the university of minnesota semantic relatedness standard containing the largest number and diversity of medical term pairs to date in terms of agreement with human ratings and the umnsrs benchmark the best correlations range between and spearman rank correlation reported similar correlations between the output of the second order context vectors and a subset of the umnsrs benchmark another study bythat also included the umnsrs benchmark among others confirmed garla and brants findings demonstrating that their knowledge based method based on the wikipedia network hits sim achieved significantly_higher correlations with human ratings than distributional methods the spearman rank correlations for the umnsrs benchmark were and for semantic relatedness and semantic_similarity judgments respectively the distributional semantic methods evaluated inincluded word vec a neural_network based mechanism for semantic representation based on word embeddings that was originally proposed bytrained a skip gram vector representation of medical terms using word vec on the ohsumed corpus a collection of biomedical research_articles this approach achieved a correlation of on both relatedness and similarity human judgments with the umnsrs benchmark a similar study byalso examined word vec semantic representations derived from the pubmed central open_access pmc corpus similarly to the study bytested their approach on the umnsrs benchmark and reported spearman rank correlations of and for semantic relatedness and semantic_similarity judgments respectively these previous_studies targeted overall performance of distributional semantic and other approaches and did not examine the performance on subsets of the umnsrs dataset consisting of pairs of different semantic types we hypothesize that one of the reasons the umnsrs benchmark has been difficult to approximate with automated approaches particularly with knowledge free distributional approaches is because it consists of a large number of clinical concepts from a variety of semantic types disorders symptoms and drugs thus in order to model human judgments of semantic relatedness and similarity of this benchmark using distributional methods one may need to use very large_amounts of textual data from a corpus that closely matches the domain of clinical language represented by this benchmark previous_studies tended to rely on relatively small corpora that are from a closely_related but not perfectly_matched domains e g biomedical articles one exception to this was a study bythat used mayo_clinic clinical_notes as a source of training data however while that study had a good domain match the size of the corpus was relatively modest by current_standards m tokens the objective of the current study is to examine the effect of corpus size and sublanguage domain match on the agreement between relatedness and similarity_measures produced by the popular and highly_efficient distributional semantic method word vec and human judgments in the umnsrs benchmark_dataset in the current study we performed a large_scale evaluation of a popular neural_network learning approach word vec to representing the meaning of words in the medical domain and measuring the strength of association between them our results are overall slightly better than the best results reported so_far on the umnsrs benchmark bythat used a graph based_approach hits sim to represent word semantics that leveraged wikipedia as a network rho vs for relatedness and vs for similarity however these differences are probably not significant from a practical standpoint due to reasons having to do with inter_rater explained later in the discussion for comparison also used the word vec approach with skip gram representation of word contexts trained on ohsumed corpus of medline citations and the text of the unified_medical terms but obtained much lower correlations with human ratings rho the citations included in the ohsumed corpus are truncated at words therefore the total size of the ohsumed collection is at most m tokens the main differences of our studys use of word vec from the one reported byare that we used the bag of words representation of contexts instead of the skip gram representation and a much larger corpus of text over b tokens for training consisting of entire articles rather than just the medline citations containing only the abstract and title of the articles we believe that that the improved_performance of word vec approach observed in our study was mostly due to the latter two differences larger corpus and inclusion of entire articles however we did not directly test these assumptions in the current study at the outset of the current study we intuitively expected to find domain and corpus size effects we expected that since the umnsrs reference_standard was created based on ratings by clinicians semantic representations derived from the clinical domain would agree better with clinicians ratings than representations derived from pmc or wikipedia the findings of this negative and positive directions the detailed comparisons are shown in another interesting finding of the current study is that increasing the size of the corpus beyond a certain size does not seem to provide an additional advantage in the current study the performance on both similarity and relatedness benchmarks plateaued between m and m tokens which is consistent with the findings of another previous study byin which we found that the performance of another distributional semantics corpus based_approach to computing semantic relatedness plateaued after the training corpus reached clinical_notes m tokens these findings may be interpreted as providing_additional to show that the size of the corpus used for distributional semantic representations of medical terms does not matter beyond a certain point e g m tokens however an alternative explanation is that the corpus size does matter but the plateauing of the correlations with human ratings is a function of the test data rather than the training data and has more to do with the inter_rater one possible way to test this hypothesis in future work is to apply corpus based semantic relatedness measures in a secondary evaluation paradigm in which measures derived from corpora of different size would be used for another task such as word sense ambiguity resolution spelling correction or query expansion for information_retrieval in addition to cross domain comparisons of the word vec based semantic relatedness measures on the umnsrs reference_standard we also compared the use of semantically_related phrases derived from clinical and biomedical domains on two tasks directly relevant to medical nlp automatically derived phrases for expanding text queries to identify patients with heart_failure presented in the current study are consistent with the terms defined by experts in cardiovascular research that were used in prior studies_examining the utility of nlp for identification of patients with heart_failure from the unstructured text of ehr these manually defined terms consisted of cardiomyopathy heart_failure congestive_heart pulmonary_edema decompensated_heart volume fluid_overload the sets of top automatically derived terms related to heart_failure from both the clinical clincial all and biomedical pmc corpora in the current study contain all but one pulmonary_edema of these terms manually determined to be good search_terms for heart_failure cases our results also show that expanding text search queries with semantically_related terms based on word embeddings can significantly improve recall of the queries this is an important finding in the context of using nlp to identify potential candidates for clinical research studies such as clinical_trials and cohort_studies improved recall is particularly important for cohort_studies in which the completeness of ascertaining cases with a condition of interest is critical to minimizing potential_bias using structured billing codes a k a claims data for case_ascertainment has been shown to have limitations in terms of accuracy and completeness particularly in community_based samples furthermore a number of conditions e g symptoms and physical_examination may not have a diagnostic code entered as part of the medical_record using nlp to search unstructured text of ehr can complement the use of billing codes for improved recall of potential candidates for prospective and retrospective_studies improved recall is also relevant for clinical_trials from a more practical standpoint being able to identify any number of additional potential candidates for a clinical_trial can improve recruitment rates and consequently shorten the duration of the trial resulting in faster delivery of new therapeutic_interventions to the bedside in the current study we used the icd_code x to define the reference_standard with the understanding of the limitations inherent in this approach for comparing query expansions to each other and to the baseline query in order to minimize uncontrolled variability in these comparisons we did not use advanced nlp tools e g negation and family_history detection to make the text queries more precise therefore the precision of the queries reported in this study does not reflect the actual precision that can be expected from text queries enhanced with nlp in prior work we showed that using negation detection can improve precision of text queries to approximately and this number is likely to be higher for more sophisticated and customized nlp systems in the current study we focused on changes in precision between semantically expanded queries and the baseline our findings indicate that queries expanded with top semantically_related phrases from the clinical all and pmc corpora perform better than the baseline query and are comparable to each other the performance of these queries diverges on larger sets of expansions and with a large decrease in precision on queries derived from the pmc corpus this drop in precision can be attributed to the terms cardiac and hypertension that are relatively high on the list of pmc derived phrases queries with these two terms would capture the majority of patients with heart_disease with or without heart_failure and thus negatively_affect precision however despite the lower precision these results indicate that the public pmc corpus can potentially be used as an alternative source of semantic relatedness information for expanding queries when searching clinical texts this is particularly important in settings where the query expansion is not fully_automated but is curated by the end_user that can manually select more specific terms from the set of semantically_related suggestions while these findings are encouraging clearly further investigation is necessary to test their generalizability similarly to the evaluation on the clinical document retrieval task the evaluation of using word embeddings on a biomedical term wsd task showed that this type of representation improves wsd accuracy over basic co occurrence based_approaches the accuracy of that we achieved on this dataset with word embeddings derived from the pmc corpus is comparable to other previously_reported machine_readable dictionary based results on the same dataset b t the improvement in wsd accuracy with word embeddings is likely due to the fact that even extended sense definitions are bound to be sparse compared to occurrences in a text corpus of any reasonable size word embeddings may offer the advantage of smoothing over the gaps in representations based on sparse sense definitions the fact that better wsd accuracy is achieved with methods trained on pmc data is not surprising because the nlm wsd dataset is derived from the same domain what is interesting is that the accuracy achieved with word embeddings derived from the clinical domain is not that much lower than the accuracy achieved with in domain training data which constitutes additional evidence that the clinical and biomedical sources of text are similar with respect to semantic representations that can be derived from them 
