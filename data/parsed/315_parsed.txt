data_and an efficient method to estimate the optimum regularization_parameter in rlda motivation the biomarker_discovery process in high_throughput genomic_profiles has presented the statistical_learning community with a challenging_problem namely learning when the number of variables is comparable or exceeding the sample_size in these settings many classical techniques including linear_discriminant lda falter poor_performance of lda is attributed to the ill conditioned nature of sample covariance_matrix when the dimension and sample_size are comparable to alleviate this problem regularized lda rlda has been classically proposed in which the sample covariance_matrix is replaced by its ridge estimate however the performance of rlda depends heavily on the regularization_parameter used in the ridge estimate of sample covari ance matrix results we propose a range search technique for efficient estimation of the optimum regulariza tion parameter using an extensive set of simulations based on synthetic and gene_expression microarray_data we demonstrate the robustness of the proposed technique to gaussianity an assumption used in developing the core estimator we compare the performance of the technique in terms of accuracy and efficiency with classical techniques for estimating the regularization_parameter in terms of accuracy the results indicate that the proposed method vastly improves on similar techniques that use classical plug in estimator in that respect it is better or comparable to cross_validation based search_strategies while depending on the sample_size and dimensionality being tens to hundreds of times_faster to compute availability_and the source_code is available at https github com ridge estimation is a type of shrinkage and traces back to the pioneering work of b on estimating regression parameters they considered the standard linear_model y xb e where y is the n dimensional observation vector x is a known n p matrix b b b b p t is a p dimensional parameter vector to be estimated and e is the n dimensional error vector with mean and covariance_matrix r i p if we assume x is a full column rank matrix p n the ordinary least square solution to this familiar linear_model is given by b b x t x x t y however when p n the solution does not exist because x t x becomes degenerate even the solution obtained by generalized inverse form of matrix x t x is not working well b then formulated a problem in which the residual sum of squares is replaced by its penalized form given by l bjjy xbjj kjjbjj where k denotes a penalty factor_controlling the length of b minimizing l b results in the so_called ridge_regression given by b b x t x ki p x t y in this way the inverse of possibly ill conditioned x t x is stabilized by adding the scalar matrix ki p this idea was then used by di pillo to replace the estimate of the sample covariance_matrix used in linear_discriminant lda by its ridge estimate resulting in the so_called regularized lda rlda the goal is to improve the performance of lda in situations where dimensionality of observations p is larger or comparable to the number of measurements n di pillo attempts to determine the optimum value of the optimum regularization_parameter in rlda on this di pillos study peck and ness comment that he found the analytical solution to this problem intractable and so used a simulation_study to choose an optimum value for k the regularization_parameter he concluded that if an algorithm can be found which leads to a value of k near the optimum value then considerable improvement in the pcc probability of correct classification should occur suggested the use of cross_validation in finding the optimum value of regularization_parameter in this procedure cross_validation is used to estimate the true error of rlda for each value of the regularization_parameter selected from a pre specified set of size the estimate of the optimum regularization_parameter is then the one that results in the minimum cross_validation estimate of true error despite the computational_complexity of cross_validation in such a search algorithm e g see comments inand tasjudin and this approach has remained the most popular method in estimating the optimum value of regularization_parameter in rldafor instance seeand ye and xiong to cite just a few articles recently we constructed a generalized consistent estimator of true error of rlda in this regard we proposed an estimator that converges to true error in a double asymptotic sense in this setting the estimator converges to the actual parameter in an asymptotic scenario in which dimension and sample_size increase in a proportional manner n p and p n j in developing this estimator we assumed that the true distributions governing the data follow multivariate gaussian_model however the underlying mechanism to develop the estimator was based on double asymptotics and random_matrix both of which suggest applicability of the estimator in non gaussian settings as well see p xii in p in bai and silverstein and in this work we employ this estimator of true error in a one dimensional search to estimate the optimum regularization_parameter of rlda as such we employ data taken from seven gene_expression microarray_studies as well as synthetically generated gaussian and non gaussian data we compare the performance in terms of accuracy and efficiency of the search technique that uses this estimator with similar search schemes that use cross_validation or plug in estimators using an extensive set of simulations we observe that the proposed technique is an efficient method that can outperform cross_validation and plugin estimate based schemes in estimating the optimum regularization_parameter of rlda throughout this work we use boldface lower case letters to denote a column vector a boldface upper case letter denotes a matrix and tr is the trace operator the identity matrix of p dimension is denoted by i p 
