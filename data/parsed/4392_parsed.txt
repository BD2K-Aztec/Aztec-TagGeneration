genome_analysis srmapper a fast and sensitive genome hashing alignment_tool modern sequencing instruments have the capability to produce millions of short_reads every day the large number of reads produced in conjunction with variations between reads and reference genomic_sequences caused both by legitimate differences such as single_nucleotide and insertions_deletions indels and by sequencer errors make alignment a difficult and computationally_expensive task and many reads cannot be aligned here we introduce a new alignment_tool srmapper which in tests using real_data can align s of billions of base_pairs from short_reads to the human_genome per computer processor day srmapper tolerates a higher number of mismatches than current programs based on burrows_wheeler transform and finds about the same number of alignments in less time depending on read_length with higher performance_gain for longer read_length the current version of srmapper aligns both single and pair_end in base space fastq format and outputs alignments in sequence_alignment format srmapper uses a probabilistic_approach to set a default number of mismatches allowed and determines alignment_quality srmappers memory_footprint gb is small enough that it can be run on a computer with gb of random_access for a genome the size of a human finally srmapper is designed so that its function can be extended to finding small_indels as well as long deletions and chromosomal trans locations in future versions with the advent_of ngs instruments the amount of raw genetic sequence_information has exponentially increased during the past_few and it is expected to continue to grow at a high rate as sequencing cost continue to decrease instruments such as the hiseq illumina gs flx titanium roche and solid abi can generate billions of base_pairs gigabases or gb of data per day with increasingly high accuracies for illumina and for roche and abi respectively and with costs that have decreased to per human_genome newer instruments such as the ion proton life technologies can produce even higher amounts of data and are approaching the goal of a genome with the speed and cost factors making whole_genome practical researchers are sequencing and analysing large_numbers of genomes in hopes of finding genetic origins of many diseases such as cancers for example one recent study sequenced human_genomes searching for rare_mutations involved in ovarian_cancer with the dramatically_increased amount of raw_data analysis has become more challenging this is because of two factors the sheer amount of data gathered and the relatively short lengths of reads produced by current ngs instruments for example in the study aforementioned terabases of sequence_data would be gathered for a coverage of the genomes the short_length of the reads typically between and bp for illumina and abi and bp for roche complicates the building of a genome de_novo for example a genome may contain repetitive_regions it is difficult to reconstruct these regions and their flanking_sequences if the length of the reads is much shorter than that of the repeating_units ngs instruments can now perform pair end sequencing in which the sequence of the two ends of longer fragments are determined which has helped to resolve these problems nevertheless de_novo remains_challenging and is memory intensive and therefore difficult to perform on genomes larger than those of bacteria a popular alternative to de_novo is reference assembly in which reads are aligned against a pre_existing reference_genome there are three main classes of alignment tools read hashing tools reference hashing tools and burrows_wheeler transform bwt tools examples of reference hashing tools include bfast shrimp and wham tools that use the bwt include burrowswheeler aligner bwa bowtie and soap most genome hashing algorithms require large memory that they must be run on expensive large memory machines on the other hand many bwt methods carry a small memory_footprint that they can be run on computers with gb of random_access ram accessible by many users possessing only desktop machines among the genome hashing methods listed previously only bfast can be run on a computer with gb of ram for methods based on bwt both bwa and bowtie can be run on computers with gb of ram among these three bowtie is the most restrictive in terms of allowing variants between the reference and short_read allowing a maximum of three mismatches and no insertions_or to whom correspondence should be addressed indels making it a less attractive option for sequencing longer_reads which would be expected to have a higher number of mismatches and errors bwa and bfast both allow indels and mismatches among the three bowtie is slightly faster than bwa and both are significantly_faster than bfast however bfast has been shown to be more sensitive than the bwt based_methods for most datasets evaluated in here we introduce a new genome hashing alignment_tool srmapper the original design goal for srmapper was to build an alignment_tool that was not restrictively slow had as high_sensitivity as other widely used alignment tools and was capable of finding long deletions and other more complicated genomic alternations such as chromosomal_translocations from short_read the current version of srmapper already achieves the first two goals and uses a novel approach to determine the initial number of mismatches allowed and calculates alignment scores in evaluations on real_data srmapper is faster than bwa on datasets of length bp while aligning comparable number of reads as bwa for short_reads bp srmapper was faster than bwa but somewhat less sensitive this article explains the approach taken by srmapper compares its performance against the popular bwa package for multiple datasets of different read_length describes how we envision this first version of srmapper being used and discusses future extensions and improvements 
