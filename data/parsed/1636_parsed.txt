estimating the proportion of true null_hypotheses when the statistics are discrete motivation in high_dimensional testing problems p the proportion of null_hypotheses that are true is an important parameter for discrete test_statistics the p values come from a discrete distribution with finite support and the null_distribution may depend on an ancillary statistic such as a table margin that varies among the test_statistics methods for estimating p developed for continuous test_statistics which depend on a uniform or identical null_distribution of p values may not perform well when applied to discrete testing problems results this article introduces a number of p estimators the regression and t methods that perform well with discrete test_statistics and also assesses how well methods developed for or adapted from continuous tests perform with discrete tests we demonstrate the usefulness of these estimators in the analysis of high_throughput biological rna_seq and single_nucleotide data in multiple testing inferential problems we want to select which among a large number of hypotheses are true the proportion of truly null_hypotheses p plays_a in adjusting for multiple testing gives a benchmark for the number of statistically_significant tests which should be discovered and is an important measure of effect_size we assume that m null_hypotheses h h m are being tested corresponding to m parameters or features the ith hypothesis is associated with an observed test_statistic x i and observed p value p i a number of methods are available for estimating p when x i and hence p i are continuous these methods rely on modeling the mixture distribution arising from the mix of truly null and non null tests using various parametric and nonparametric approaches let n be the set fijh i is trueg in many cases with continuous response such as gene_expression microarray_studies and pixel wise intensity analysis of images it is reasonable to believe that the distribution of x i is the same for all i n for example we might perform a t test for each feature and expect the null_distribution to be students t alternatively we can use p i as the test_statistic in which case the null_distribution is uniform although it is not known which hypotheses are in n the empirical distribution of the test_statistics is a mixture of m observations from the null and m m m observations from an alternative distribution so that deviations of the empirical distribution from the known null can be used to estimate p for the discrete case the situation is more complicated each x i can take on only a finite number of values which often depend on an ancillary statistic which varies with i for example for fishers_exact and other tests of independence in two way tables the ancillary is a table margin as a result even for i n the distributionwith i and the empirical distribution of the observed statistics is a mixture for example displays p values from simulated gene_expression data with p for each feature gene there are two treatments and the null_hypothesis is no difference in mean expression_level the p values arising from the null_distribution are displayed in gray and the p values arising from the non null features are stacked in white the two histograms look very different in the p values come from two sample t tests where of the p values come from various non central t distributions note the relative uniformity of the null gray p values versus the non null white p values which are skewed toward small values in contrast displays p values coming from fishers exact tests where of the p values come from various non central hypergeometric distributions in this case both the null and non null p values are highly non uniform and there is a non zero probability of yielding a p value equal to under the null and alternative distributions the differences between continuous and discrete tests are apparent with real_data shows p values from real studies differential_expression of the primate liver study using rna_seq technology with three biological_replicates used in section andthe bovine iron single_nucleotide snp study used in section in this article we propose new methods for estimating p for discrete tests and compare them to some popular_methods developed for continuous data we also apply these methods to determine the proportion of differentially expressing genes in primate livers and for selecting snps associated with bovine muscular iron_levels there is no clear winner among the p estimators in part this is because discrete tests for count_data become closer to uniform as the size of the counts increase the regression_method requires computing f d the null_distribution conditional on the observed values of the ancillary statistic a d for each d these null distributions have more support points for larger values of a d adding to the computational_burden while providing fewer observations for computing jt as well it may be difficult to compute jt in studies in which the number of different values of the ancillary is large is also very computationally complex as the number of different null distributions increases thus it also is not an ideal candidate when there is a large number of unique ancillary statistics finally these methods do not extend readily to continuous tests in contrast the t methods naturally adapt to higher counts because the filtered features are those with small counts hence as frequencies increase and the p values are closer to continuous fewer features are filtered and the t method is closer to the continuous method on which it is based the t methods also adapt naturally to missing datafeatures with missing_data are naturally those with smaller counts as well in complex studies such as genome_wide with continuous response or rna_seq studies with biological replication it is still reasonable to assume that features with small counts will be associated with very low power tests hence the t methods are readily implemented for these reasons we recommend the use of t methods except in simple studies with multiple tests with the same ancillary statistic for which the regression_method may be better of the three t methods tested here storey t appears to be most resilient to different types of data ancillary distributions and sample_sizes it has become common practice in rna_seq studies to filter out genes with low total_counts generally the cut_off is selected arbitrarily for example the edger software recommends the total for genes retained should be about reads per million detected while the primate liver paper dropped genes with median count per sample of or less the t method suggests a more principled approachselect the largest p value for which the investigator would be willing to declare significance and filter out features for which the test_statistic has minimum achievable p value which is higher for tables with large margins in one direction such as those generated by rna_seq studies a simple rule of thumb is that the test has zero power at p when the minimum margin total reads for a gene is for tables with large margins in one direction such as those generated by genome_wide using snps a simple rule of thumb is that the test has zero power if the sum of the two smallest margins is in many scenarios tests are not independent for the rna_seq scenario the primary simulation_study was done using independently generated features we also simulated runs of rna_seq the th_percentile of p for different estimators for the snp simulations a p estimates for snp study with controls treated m b p estimates for snp study with controls treated m data with correlated test_statistics and m tests the algorithm used to simulate the data is provided in the supplementary_materials we found little difference in the performance of the p estimators between the independent_tests and the dependent tests the t methods still performed well overall for the snp scenario the snps were simulated in small correlated clusters in the primary study the results suggest that the p estimators do not require independence of the tests to work well 
