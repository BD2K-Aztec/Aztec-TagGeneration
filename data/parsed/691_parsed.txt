bayesian_variable for binary outcomes in high_dimensional genomic studies using non local priors motivation the advent of new genomic technologies has resulted in the production of massive data_sets analyses of these data require new statistical and computational_methods in this article we propose one such method that is useful in selecting explanatory_variables for prediction of a binary response although this problem has recently been addressed using penalized likelihood methods we adopt a bayesian_approach that utilizes a mixture of non local prior densities and point masses on the binary regression_coefficient vectors results the resulting method which we call imomlogit provides improved_performance in identifying true models and reducing estimation and prediction_error in a number of simulation_studies more importantly its application to several genomic_datasets produces predictions that have high_accuracy using far fewer explanatory_variables than competing_methods we also describe a novel approach for setting prior hyperparameters by examining the total_variation distance between the prior distributions on the regression parameters and the distribution of the maximum_likelihood es timator under the null_distribution finally we describe a computational algorithm that can be used to implement imomlogit in ultrahigh dimensional settings p n and provide diagnostics to assess the probability that this algorithm has identified the highest posterior_probability model availability_and software to implement this method can be downloaded at recent_developments in bioinformatics and cancer_genomics have made it possible to measure thousands of genomic variables that might be associated with the manifestation of cancer the availability of such data has resulted in a pressing need for the development of statistical_methods to use these data to identify variables that are associated with binary outcomes e g cancer or control survival or death the topic of this article is a statistical_model for identifying from a large number p of potential feature_vectors a sparse subset that are useful in predicting a binary outcome vector throughout this article we assume that the binary vector of interest is denoted by y and that the matrix of potential explanatory_variables is denoted by x letting x k denote the submatrix of x containing the true predictors we assume thatwhere f denotes a known binary link function assumed to be the logistic distribution in what follows and p is the n vector of success probabilities for y the regression_coefficient b k represents the nonzero regression effect for each column of x k in predicting p the primary statistical challenge addressed in this article is the selection of the submatrix x k to be used for the prediction of p a number of related methods have been proposed to address this problem these include the lasso which is a penalized likelihood method that maximizes a product of the binary likelihood_function implied by and a constraint on the sum of the absolute value of components of the regression_coefficient b k a closely_related method called smoothly clipped absolute deviation scad uses a non convex penalty function and has been demonstrated to have certain oracle properties in idealized asymptotic settings other penalized likelihood functions include the adaptive lasso and the dantzig selector these methods share asymptotic properties similar to scad in ultrahigh dimensions p n an effective computational technique for implementing the techniques described above is the iterative sure independence screening isis procedure which iteratively performs a correlation screening step to reduce the number of explanatory_variables so that penalized likelihood methods can be applied isis has been used in conjunction with several penalized likelihood methodsincluding adaptive lasso the dantzig selector and scad to perform model_selection a number of bayesian_methods have also been proposed for variable_selection notable among these are the approaches proposed by which used a mixture of normals approximation to spike and slab priors on the regression_coefficients proposed a hierarchical probit model along with mcmc based stochastic_search to perform gene_selection in high_dimensional settings using a latent response variable and gaussian priors on model coefficients provided a bayesian_approach to this problem employing singular value regression and classes of informative prior distributions to estimate coefficients in high_dimensional settings studied mixtures of g priors for bayesian_variable as an alternative to default g priors to overcome several consistency issues associated with the default g prior densities along more similar lines studied the utilization of non local priors in bayesian classifiers where they also address the problem of identifying variables with high predictive_power except for each of the bayesian_methods described above impose local prior densities on regression_coefficients in the true model that is the prior density on the regression_coefficients has a positive prior density_function at and in most cases has its mode at which from a bayesian perspective makes it more difficult to distinguish between models that include regression_coefficients that are close to and those that do not johnson and rossell proposed two new classes of non local prior densities to ameliorate this problem in the model_selection context non local prior densities are when a regression_coefficient in the model is this makes it easier to distinguish between coefficients that do not have an impact on the prediction of y from those that do used a markov_chain mcmc_algorithm to sample from the posterior_distribution on the model space the convergence properties of this algorithm were studied in johnson the primary goal of this article is to extend the methodology proposed infor application to binary outcomes and to compare the performance of this algorithm to leading penalized likelihood methods in addition we describe a default procedure for setting the hyperparameters i e tuning parameters in the non local priors and we examine a numerical strategy for identifying the highest posterior_probability model hppm to investigate the performance of the proposed model_selection procedure we applied our procedure to both simulated_data and real_data we compared the performance of our algorithm to isisscad in both real and simulated_data because isis scad has proven to be among the most successful model_selection procedures used in practice for the real_data we also compared our method to another bayesian procedure based on the product moment prior bayesian_variable for binary outcomesin this article we introduced a bayesian_method imomlogit for variable_selection in binary response regression problems in high and ultrahigh dimensional settings there are many applications associated with these type of data such data are of great interest to bioinformaticians and biologists who routinely collect gene_expression data to find prognostic_features to classify cancer_types for two real_datasets imomlogit identified sparse models with low prediction error_rates in both cases biological considerations suggest that the genes reported by imomlogit appear to be valid predictors of biological outcomes the primary disadvantage of the imomlogit procedure is that it is computationally much more intensive than isis scad and related penalized likelihood methods we are currently investigating methods for reducing the computational_burden of our algorithm by implementing various screening_procedures that are similar to those used in isis scad 
