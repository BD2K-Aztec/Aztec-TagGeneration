sequence_analysis adaptive reference free compression of sequence quality_scores motivation rapid technological_progress in dna_sequencing has stimulated interest in compressing the vast datasets that are now routinely produced relatively little attention has been paid to compressing the quality_scores that are assigned to each sequence even though these scores may be harder to compress than the sequences themselves by aggregating a set of reads into a compressed index we find that the majority of bases can be predicted from the sequence of bases that are adjacent to them and hence are likely to be less informative for variant_calling or other applications the quality_scores for such bases are aggressively compressed leaving a relatively small number at full resolution as our approach relies directly on redundancy present in the reads it does not need a reference_sequence and is therefore applicable to data from metagenomics and de_novo experiments as well as to re sequencing_data results we show that a conservative smoothing strategy affecting of the quality_scores above q leads to an overall quality_score compression of bit per value with a negligible effect on variant_calling a compression of bit per quality value is achieved using a more aggressive smoothing strategy again with a very small effect on variant_calling availability code to construct the bwt and lcp array on large_genomic data_sets is part of the beetl library available as a github repository atthe raw output of a dna_sequencer is converted by a program known as a base caller into nucleotide_bases each of which is typically assigned a quality_score that estimates the probability that the base has been sequenced correctly quality_scores have long been used to trim the low quality ends of reads and for accurate consensus sequence_determination more recently they have enabled more accurate alignments of the shorter sequences produced by next generation technologies by allowing the aligner to give lower weight to mismatches at less reliable base positions often quality_scores are expressed on an integer scale derived from the error probability p via the formula log p a scoring_scheme named after the phred base caller that first used it the widely used fastq format stores the sequence and metadata of a set of dna_reads as ascii text together with one character per score quality strings that encode the phred scores of their bases the different properties of these three data types have meant that many fastq compression_methods have treated them as three distinct data streams and applied separate compression strategies to each the metadata field tends to be formatted in ways that are specific to the technology that was used to generate the sequence and some fastq compressors have exploited such structure to improve compression however there is no global format specification for the metadata therefore any universally_applicable method for its compression must necessarily be a generic exercise in the compression of ascii text although standard text compressors such as gzip www gzip org jean loup gailly and mark adler do not significantly_outperform a navenave bits_per encoding on dna sequence_data applications such as re sequencing and de_novo typically rely on a fold or more oversampling of the underlying genome and this redundancy can be exploited to improve compression of the sequences themselves reference based compression tools such as cram encode reads in terms of differences between their sequences and the sites they align to on a reference_sequence sorting the reads by the coordinates of these alignments saves most of the overhead of storing their positions and is a convenient ordering for applications such as snp_calling and visualization despite these advantages reference based compression suffers when the reference is incomplete reads that do not align cannot be compressed subject to change or not present at all as in metagenomics motivating an interest in reference free compression_methods the tool quip creates an onthe fly de_novo to perform reference based compression against whereas scalce places similar reads near to each other in a sorted file facilitating good performance by standard_tools such as gzip that operate on a buffer of text at a time another widely used generic compression tool bzip www bzip org julian seward exemplifies burrowswheeler_transform bwt compression text is split into kb blocks and the bwt of each block is computed the bwt is a reversible permutation of the text that acts as a compression booster for the pipeline of standard compression steps that bzip subsequently applies in two of the present authors showed that although bzip performs comparably with gzip on dna_sequence the compression achieved by bwt based_methods improves by fold if the bwt of the entire read set is built as to whom correspondence should be addressed this captures redundancy between reads that were widely_spaced in the original file it was shown that compression can be further boosted by pre sorting the reads or applying an implicit sorting strategy while the bwt is being built enabling compression of better than bits_per to be achieved lossless approaches to the compression of quality_scores have exploited empirical relationships between the scores assigned to bases within a read for instance illumina quality_scores tend to be monotone decreasing along a read with a decrease in scores between adjacent bases that is usually small an overreliance on such observations potentially ties a compression scheme to a given sequencing_technology and makes it sensitive to changes in sequencing protocol moreover it is likely that illumina quality_scores at their full resolution contain a proportion of random noise that is impossible to compress striking evidence for this is given byin bonfield and mahoney which shows multiple entrants to the sequencesqueeze competition for fastq compression achieving similar lossless compressions of bits per score on a test_dataset but that no entrants were able to improve on this figure it is undesirable that when compressed the quality_scores should take up several times more space than the sequences themselves therefore we are led to consider compressing them in a lossy way found that a global reduction in the resolution of the scores from to values thus permitting each score to be stored in bits had no significant impact on the quality of variant_calls whereas strategies for global re quantization of quality_scores were studied in more detail by however treating all scores in the same way ignores the fact that most of them could likely be reduced in resolution or even discarded entirely with little impact on our ultimate_goal of ensuring that analyses performed with the reduced scores closely reflect the results obtained from the original data in a human re sequencing context for example if a large coverage of highquality bases unanimously supports a homozygous match to the reference_genome then a confident call can be made without the full resolution quality_scores of each individual base needing to be kept with this in mind cram allows an adaptive approach where only quality_scores that contribute to variant_calls that do not match the reference are kept this enables the vast_majority of scores to be omitted but it means compression cannot take place until analysis has been finalized this means any preanalysis transfer or storage of the data will not benefit from compression and is potentially problematic if the data subsequently need to be re analysed here we present an adaptive and reference free approach to lossy quality_score compression our central premise is that if a base in a read can with high_probability be predicted by the context of bases that are next to it then the base itself is imparting little additional information and its quality_score can be discarded or aggressively compressed at little detriment to downstream_analysis such predictions are made by considering all possible contexts present in the reads if every occurrence of some string q is followed by the same character p then the presence of a context q in a read can be said to predict that p will come next in the rest of this article we formalize this intuition and give algorithms that use the bwt of a set of reads to identify non essential quality_scores the bwt places all characters that precede a given context next to each other in a permuted string whereas another standard data_structure the longest common prefix array lcp then allows stretches of characters that precede contexts of a given length to be enumerated in a single_pass through the two data_structures this enables the majority of scores to be smoothed to an average value greatly improving compression we derive a formula to quantify the information lost during this smoothing process and justify our compression scheme empirically by showing that results using the compressed scores closely match the original data when our scheme is applied to whole_genome re sequencing_data we also show that we can use the bwt alone to compress quality_scores in a way that is almost as effective as bwt lcp compression thus avoiding the overhead of computing the lcp array moreover we demonstrate that our methods can be used in tandem with other approaches to boost the compression obtained this article aims to introduce the general idea of smoothing quality_scores based on the bwt and lcp of their associated reads section describes perhaps the simplest and most conservative_approach to this but the theoretical_framework presented in section allows comparison of future more sophisticated quality smoothing strategies note that the effect of smoothing or otherwise adjusting the quality_scores is to change the weightings of the different nucleotides in the distributions xw qv we can take a step further and consider adjusting low_probability bases in xw qv to zero our work can thus be extended to a new quality based view on de_novo error_correction that may provide an interesting alternative to existing_approaches many of which are based on the counting of k_mers as surveyed by such a strategy could be thought of as a quality aware extension of the hitec algorithm while enjoying the considerable space advantage of being based on a potentially compressed bwt instead of a suffix_array although it is an advantage that the relative entropy measures the information lost by quality smoothing in an application neutral way we also recognize that a single numerical quantity cannot fully model the effect of quality smoothing on the often complex multi_step analysis_pipelines that are applied to sequence_data we investigated this by measuring the effect of smoothed quality_scores on the results of widely_accepted tools for a well understood application transforming the smoothed scores back into their original reads added a significant boost to the compression already achieved by zip from exploiting similarities between the quality_scores of individual read it is equally simple to combine our approach with the application to the unsmoothed scores of one of the lossy re quantization schemes studied by however using our method in this way involves building the bwt and possibly lcp of a set of reads and then applying the inverse bwt permutation to the quality_scores to obtain a smoothed quality string for each read the overhead of these tasks may limit its practicality for downstream applications that operate on reads and does not use the potential of the bwt as well as allowing excellent lossless_compression storing sequences in bwt form also facilitates rapid analysis the sga and fermi assemblers both operate directly on bwt based compressed indexes of sets of reads and we ourselves have shown that similar data_structures can be the basis of both rna_seq and metagenomic analyses although the compression achieved on bwt space reads is less than in read space although the difference may be less clear_cut on a dataset where the q masking of read ends is less prevalent or has been switched off we therefore envisage that a key application of our work is to allow quality_scores to be used in a bwt space context while being stored in as compact a manner as the reads themselves conflict of interest l j and a j c are employees of illumina inc a public company that develops and markets systems for genetic_analysis they receive shares as part of their compensation 
