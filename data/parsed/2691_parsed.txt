genetics_and learning phenotype densities conditional on many interacting predictors motivation estimating a phenotype distribution conditional on a set of discrete valued predictors is a commonly_encountered task for example interest may be in how the density of a quantitative_trait varies with single_nucleotide and patient_characteristics the subset of important predictors is not usually known in advance this becomes more challenging with a high_dimensional predictor set when there is the possibility of interaction results we demonstrate a novel non parametric bayes method based on a tensor factorization of predictor dependent weights for gaussian kernels the method uses multistage predictor selection for dimension_reduction providing succinct models for the phenotype distribution the resulting conditional density morphs flexibly with the selected predictors in a simulation_study and an application to molecular_epidemiology data we demonstrate advantages over commonly used methods availability_and matlab_code available at https googledrive com host bw kifb k ioowq dfjtsvzxne ktdctf many areas of research are concerned with learning the distribution of a response conditional on numerous categorical discrete predictors the important predictors for characterization of this distribution are not usually known in advance and there may be hundreds_or of candidates methods that attempt to accommodate interactions among these predictors become mired in the enormous model space for example in a moderatedimensional case involving p categorical predictors each with d j possible realizations considering all possible levels of interaction leads to a space of possible models parallelization and technical tricks may work for smaller examples but data sparsity and the sheer volume of models force us to consider different approaches the conditional density may vary in more than just location illustrated this in an application to the conditional density of blood_glucose given insulin_sensitivity and age in the work that follows we present a novel non parametric bayes npb approach to learning conditional densities that makes use of a conditional tensor factorization to characterize the conditional distribution given the predictor set allowing for complex interactions between the predictors the particular form assumed for the conditional density gives rise to an attractive predictor selection procedure providing support for distinct predictor selection steps this addresses the challenges of high_dimensional and produces conditional density estimates that allow assessment of tail risks and other complex quantities we have presented a novel method for flexible conditional density regression in the common case of a continuous response and categorical predictors the simulation_study and real_data example suggest that this conditional tensor factorization method can have better performance than other modeling tools when there is substantial interaction between the predictors of interest the ctf does have a higher computational time requirement than the competitor methods but the improvement in prediction_accuracy and coverage still make the ctf an attractive method a particularly appealing aspect of the ctf is predictor selection which finds low_dimensional structure in the high_dimensional predictor set this reduction to more parsimonious models yields a succinct description of the ways in which the phenotype varies given exposure and snps finally a distinct advantage of the ctf is its ability to produce conditional density estimates this property of the ctf provides insight beyond a simple conditional expectation and makes it possible to answer more complex questions about the relationship between the response and the predictors 
