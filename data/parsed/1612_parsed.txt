ergc an efficient referential genome_compression algorithm motivation genome_sequencing has become faster and more affordable consequently the number of available complete genomic_sequences is increasing rapidly as a result the cost to store process analyze and transmit the data is becoming a bottleneck for research and future medical_applications so the need for devising efficient data compression and data reduction techniques for biological sequencing_data is growing by the day although there exists a number of standard data compression algorithms they are not efficient in compressing biological data these generic algorithms do not exploit some inherent_properties of the sequencing_data while compressing to exploit statistical and information_theoretic properties of genomic_sequences we need specialized compression algorithms five different next_generation data compression problems have been identified and studied in the literature we propose a novel algorithm for one of these problems known as reference based genome_compression results we have done extensive experiments using five real sequencing_datasets the results on real genomes show that our proposed algorithm is indeed competitive and performs better than the best known algorithms for this problem it achieves compression_ratios that are better than those of the currently best performing algorithms the time to compress and decompress the whole_genome is also very promising availability_and the implementations are freely_available purposes they can be downloaded fromnext generation sequencing ngs_technologies are producing millions to billions of short_reads from dna_molecules simultaneously in a single run within a very short time period leading to a sharp_decline in whole_genome sequencing_costs as a result we are observing an explosion of genomic_data from various species storing these data is an important task that the biologists have to perform on a daily basis to save space compression could play_an also when the size of the data transmitted through the internet increases the transmission cost and congestion in the network also increase proportionally here again compression could help although we can compress the sequencing_data through standard general_purpose algorithms these algorithms may not compress the biological_sequences effectively since they do not exploit inherent_properties of the biological data genomic_sequences often contain repetitive_elements e g microsatellite sequences the input_sequences might exhibit high levels of similarity an example will be multiple genome_sequences from the same species additionally the statistical and information_theoretic properties of genomic_sequences can potentially be exploited general_purpose algorithms do not exploit these properties in this article we offer a novel algorithm to compress genomic_sequences effectively and efficiently our algorithm achieves compression_ratios that are better than the currently best performing algorithms in this domain by compression_ratio we mean the ratio of the uncompressed data size to the compressed data size the following five versions of the compression problem have been identified in the literature i genome_compression with a reference here we are given many hopefully very similar genomic_sequences the goal is to compress all the sequences using one of them as the reference the idea is to utilize the fact that the sequences are very similar for every sequence other than the reference we only have to store the difference between the reference and the sequence itself ii reference free genome_compression this is the same as problem except that there is no reference_sequence each sequence has to be compressed independently iii reference free reads compression it deals with compressing biological reads where there is no clear choice for a reference iv reference based reads compression in this technique complete reads data need not be stored but only the variations with respect to a reference_genome are stored and v metadata and quality_scores compression in this problem we are required to compress quality sequences associated with the reads and metadata such as read name platform and project identifiers in this article we focus on problem we present an algorithm called ergc efficient referential genome compressor based on a reference_genome it employs a divide_and strategy at first it divides both the target and reference_sequences into some parts of equal size and finds one to one maps of similar regions from each part it then outputs identical maps along with dissimilar regions of the target_sequence the rest of this article is organized as follows section has a literature_survey section_describes the proposed algorithm and analyses its time complexity our experimental_platform is explained in section this section also contains the experimental_results and discussions section concludes the article next we present details on the performance evaluation of our proposed algorithm ergc with respect to both compression and running time we have compared ergc with two of the three best performing algorithms namely gdc and idocomp using several standard benchmark_datasets green is another state of the art algorithm existing in the literature but we could not compare it with our algorithm as the site containing the code was down at the time of experiments gdc green and idocomp are highly_specialized algorithms designed to compress genomic_sequences with the help of a reference_genome these are the best performing algorithms in this area as of now given a reference_sequence our algorithm compresses the target_sequence by exploiting the reference so it needs the reference_sequence at the time of decompression also we use the target and reference pairs of sequences illustrated insion ratios but it may not be possible to find variation files for every species and these algorithms will not work without variation files our algorithm does not employ variation files and so it can compress any genomic_sequence given a reference as a result we feel that algorithms that employ variation files form a separate class of algorithms and are not comparable to our algorithm again our proposed algorithm is devised in such a way that it is able to work with any alphabet used in the genomic_sequence every other algorithm works only with valid alphabets intended for genomic_sequence e g p fa a c c g g t t n ng the characters most commonly seen in sequences are in p but there are several other valid characters that are used in clones to indicate ambiguity about the identity of certain bases in the sequence it is not uncommon to see these wobble codes at polymorphic positions in dna_sequences it also differentiates between lower case and upper case letters gdc green and idocomp can differentiate between upper case and lower case letters specified in p but previous_algorithms like grs or rlz opt only work with a c g t and n in the alphabet idocomp replaces the character in the genomic_sequence that does not belong to p with n specifically ergc will compress the target_genome file regardless of the alphabets used and decompress the compressed file which is exactly identical to the target file this is the case for gdc and idocomp also but green does not include the metadata information and output the sequence as a single_line instead of multiple lines effectiveness of various algorithms including ergc is measured using several performance_metrics such as compression size compression time decompression time etc gain measures the percentage improvement over the compression achieved by ergc with respect to gdc and idocomp comparison results are shown in clearly our proposed algorithm is competitive and performs better than all the best known algorithms in tables and we show a comparison between compressed size from different algorithms and the actual size of individual chromosomes for some datasets memory_consumption is also very low in our algorithm as it only processes one and only one part from the target and reference_sequences at any time please note that we did not report the performance evaluation of gdc for every dataset as it ran at least h but did not able to compress a single chromosome for some datasets as stated above ergc differentiates upper case and lowercase characters it compresses target file containing the genomic_sequence to be compressed and metadata if any with the help of a reference the decompression procedure produces exactly the same file as the input it does not depend on the alphabets and is universal in this sense consider dataset d where the target and reference_sequences chromosomes are from yh and hg respectively in this setting gdc runs indefinitely idocompfor details now consider dataset d where the target and reference_sequences are from yh and ko respectively the compressions achieved by gdc and idocomp are roughly_equal whereas ergc is about better than them gdcs compression time is longer than both of idocomp and ergc but it decompresses the sequences very quickly ergcs compression is approximately and faster than idocomp and gdc respectively next consider d gdc runs indefinitely for this dataset the percentage improvement ergc achieves with respect to idocomp is specifically ergc takes fewer disk_space compared to idocomp for this particular dataset ergc is also faster than idocomp in terms of both compression and decompression times shows a comparative_study of different algorithms including ergc with respect to compression_ratio compression and decompression time in brief the minimum and maximum improvements observed from datasets d d were and with respect to idocomp respectively the minimum and maximum improvements over gdc observed were and respectively ergc compresses at least and at most faster than idocomp although it is better than idocomp and gdc in compression time for every dataset it is slower than gdc with respect to decompression for datasets d d 
