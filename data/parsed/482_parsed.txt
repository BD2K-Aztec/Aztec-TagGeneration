a robust clustering_algorithm for identifying problematic samples in genome_wide high_throughput genotyping_arrays provide an efficient way to survey single_nucleotide snps across the genome in large_numbers of individuals downstream_analysis of the data for example in genome_wide gwas often involves statistical_models of genotype_frequencies across individuals the complexities of the sample_collection process and the potential for errors in the experimental assay can lead to biases and artefacts in an individuals inferred genotypes rather than attempting to model these complications it has become a standard practice to remove individuals whose genome_wide data differ from the sample at large here we describe a simple but robust statistical algorithm to identify samples with atypical summaries of genome_wide variation its use as a semi_automated quality_control tool is demonstrated using several summary_statistics selected to identify different potential problems and it is applied to two different genotyping_platforms and sample collections availability the algorithm is written in r and is freely_available at www well the advent of new technologies which can simultaneously genotype hundreds of thousands of single_nucleotide snps across the genome has permitted large_scale studies of human genetic_variation a major application of these technologies is to undertake genome_wide gwas to identify snps that correlate with phenotypes such as disease an important step in providing convincing_evidence of association is to argue that the observed correlation is not an artefact of either the sampling strategy for example hidden population_structure or systematic_biases in inferring genotypes for example differences in call rates in doing so it has become standard practice to calculate summaries of genome_wide variation that are not expected to vary to whom correspondence should be addressed a list of participants and affiliations appear in supplementary_material systematically between study individuals and then to identify and remove outlying individuals under the correct statistical_model losing data that is collected at some expense nearly always results in reduced statistical_power real effects however when the model fails to capture the data generating process inclusion of outlying individuals often leads to an increase in false_positives exclusion of individuals prior to analysis is a trade_off between loss of power due to reduced sample_size and the benefit of controlling the number_of the typical approach to identify potentially problematic samples is to calculate summary_statistics of genome_wide data and then by visualizing their distributions across individuals to manually choose a threshold based on their values for the majority of the data to automate this process_requires an algorithm to infer the distribution of normal study individuals therefore allowing inference of outliers for the approach to be applicable in many settings different summary_statistics genotyping_platforms and sample collections requires a robust model for the outlying individuals results are shown_inand supplementary figures s s for c and ukbs samples genotyped on affymetrix and illumina platforms respectively as well as being statistically principled in practice it is helpful that once the prior distributions have been specified identification of outliers is automatic empirically it appears to make sensible inference for a range of normal and outlier distributions suggesting it is useful for quality_control in gwas 
