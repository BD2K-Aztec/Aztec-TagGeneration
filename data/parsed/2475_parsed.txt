information_theoretic evaluation of predicted ontological annotations motivation the development of effective methods for the prediction of ontological annotations is an important goal in computational_biology with protein_function and disease_gene gaining wide recognition although various algorithms have been proposed for these tasks evaluating their performance is difficult owing to problems caused both by the structure of biomedical_ontologies and biased or incomplete experimental annotations of genes and gene_products results we propose an information_theoretic framework to evaluate the performance of computational protein_function we use a bayesian_network structured according to the underlying ontology to model the prior probability of a proteins function we then define two concepts misinformation and remaining uncertainty that can be seen as information_theoretic analogs of precision and recall finally we propose a single statistic referred to as semantic distance that can be used to rank classification_models we evaluate our approach by analyzing the performance of three protein_function predictors of gene_ontology and provide evidence that it addresses several weaknesses of currently used metrics we believe this framework provides useful insights into the performance of protein_function prediction_tools ontological representations have been widely used in biomedical_sciences to standardize knowledge representation and exchange modern ontologies are typically viewed as graphs in which vertices represent terms or concepts in the domain of interest and edges represent relational ties between terms e g is a part of although in theory there are no restrictions on the types of graphs used to implement ontologies hierarchical organizations such as trees or directed_acyclic have been frequently used in the systematization of biological_experiments organismal phenotypes or structural and functional descriptions of biological_macromolecules in molecular_biology one of the most frequently used ontologies is the gene_ontology go which standardizes the functional_annotation of genes and gene_products the development of go was based on the premise that the genomes of all living_organisms are composed of genes_whose perform functions derived from a finite molecular repertoire in addition to knowledge representation go has also facilitated large_scale analyses and automated annotation of gene_product function as the rate of accumulation of uncharacterized sequences far outpaces the rate at which biological_experiments can be carried_out to characterize those sequences computational function prediction has become increasingly useful for the global characterization of genomes and proteomes as well as for guiding biological_experiments via prioritization the growing importance of tools for the prediction of go annotations especially for proteins presents the problem of how to accurately_evaluate such tools first because terms can automatically be associated with their ancestors in the go graph the task of an evaluation procedure is to compare the predicted graph with the true experimental annotation furthermore the structure of the ontology introduces dependence between terms which must be appropriately considered when comparing two graphs second go as most current ontologies is generally unfinished and contains a range of specificities of functional descriptions at the same depth of the ontology third protein_function is complex and context dependent thus a single biological experiment rarely results in complete characterization of a proteins function this is particularly evident in cases when only high_throughput experiments are used for functional characterization leading to shallow annotation graphs this poses a problem in evaluation as the ground_truth is incomplete and noisy finally different computational_models produce different outputs that must be accounted for for example some models simply predict an annotation graph possibly associating it with a numerical score whereas others assign a score to potentially each node in the ontology with an expectation that a good decision threshold would be applied to provide useful annotations there are two important factors related to the development of evaluation metrics first because both the experimental and predicted annotation of genes can be represented as subgraphs of the generally much larger go graph it is unlikely that a given computational_method will provide an exact prediction of the experimental annotation thus it is necessary to develop metrics that facilitate calculating degrees of similarity between pairs of graphs and appropriately address dependency between nodes ideally such a measure of similarity would be able to characterize not only the level of correct prediction of the true albeit incomplete annotation but also the level of misannotation the second important factor related to the evaluation metric is its interpretability this is because characterizing the predictors performance should be meaningful to a downstream user ideally an evaluation metric would have a simple probabilistic interpretation in this article we develop an information_theoretic framework for evaluating the prediction_accuracy of computer generated ontological annotations we first use the structure of the to whom correspondence should be addressed the author published_by this is an open_access the terms of the creative_commons http creativecommons org_licenses which permits non commercial re use distribution and reproduction in any medium provided the original_work for commercial re use please_contact permissions_oup com ontology to probabilistically model via a bayesian_network the prior_distribution of protein experimental annotation we then apply our metric to three protein_function prediction_algorithms selected to highlight the limitations of typically_considered evaluation metrics we show that our metrics provide added value to the current analyses of the strengths_and of computational_tools finally we argue that our framework is probabilistically well founded and show that it can also be used to augment already existing evaluation metrics in this work we propose an information_theoretic framework for evaluating the performance of computational protein_function we frame protein_function as a structured output learning_problem in which the output space is represented by consistent subgraphs of the go graph we argue that our approach directly addresses evaluation in cases where there are multiple true and predicted leaf terms associated with a protein by taking the structure of the ontology and the dependencies between terms induced by a hierarchical ontology into account our method also facilitates accounting for the high_level of biased and incomplete experimental annotations of proteins by allowing for the weighting of proteins based on the information content of their annotations because we maintain an information_theoretic foundation our approach is relatively immune to the potential dissociation between the depth of a term and its information content a weakness of often used topological metrics in this domain such as precision_recall or roc based evaluation at the same time because we take a holistic_approach to considering a proteins potentially large set of true or predicted functional_associations we resolve many of the problems introduced by the practice of aggregating multiple pairwise similarity comparisons common to existing semantic_similarity although there is a long history and a significant body of work in the literature regarding the use of semantic_similarity to the best of our knowledge all such metrics are based on singlestatistics and are unable to provide insight into the levels of remaining uncertainty and misinformation that every predictor is expected to balance therefore the methods proposed in this work extend modify and formalize several useful informationtheoretic metrics introduced during the past_decades in addition both remaining uncertainty and misinformation have natural information_theoretic interpretations and can provide meaningful_information to the users of computational_tools at the same time the semantic distance based on these concepts facilitates not only the use of a single performance measure to evaluate and rank predictors but can also be exploited as a loss function during training one limitation of the proposed approach is grounded in the assumption that a bayesian_network structured according to the underlying ontology will perfectly model the prior probability_distribution of a target variable an interesting anomaly with this approach is that the marginal probability and subsequently the information content of a single term i e consistent graph with a single leaf term calculated from a bayesian_network does not necessarily match the relative term frequency in the database instead the conditional_probability tables are estimated as relative_frequencies ad_hoc solutions that maintain the term information content are possible but would result in sacrificed interpretability of the metric itself one such solution can be obtained via a recursive definition iav iv p u pv iau and iaroot where i v is estimated directly from the database finally rationalizing between evaluation metrics is a difficult_task the literature presents several strategies where protein_sequence proteinprotein_interactions or other data are used to assess whether a performance metric behaves according to expectations in this work we took a somewhat different approach and showed that the demonstrably biased protein_function data can be shown to provide surprising results with well understood prediction_algorithms and conventional evaluation metrics thus we believe that our experiments_provide of the usefulness of the new evaluation metric 
