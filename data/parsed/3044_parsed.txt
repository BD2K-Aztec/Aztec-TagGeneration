genome_analysis modeling genome_coverage in single_cell motivation single_cell dna_sequencing is necessary for examining genetic_variation at the cellular_level which remains hidden in bulk sequencing_experiments but because they begin with such small amounts of starting_material the amount of information that is obtained from single_cell sequencing_experiment is highly_sensitive to the choice of protocol employed and variability in library_preparation in particular the fraction of the genome represented in single_cell libraries exhibits extreme variability due to quantitative biases in amplification and loss of genetic_material results we propose a method to predict the genome_coverage of a deep sequencing_experiment using information from an initial shallow sequencing_experiment mapped to a reference_genome the observed coverage statistics are used in a non parametric empirical_bayes poisson_model to estimate the gain in coverage from deeper sequencing this approach allows researchers to know statistical_features of deep sequencing_experiments without actually sequencing deeply providing a basis for optimizing and comparing single_cell protocols or screening libraries availability_and the method is available as part of the preseq software_package source_code is available at http smith labresearch org preseq the capability to sequence the dna of a single_cell is essential to analyzing biological_diversity in heterogeneous populations of cells single_cell dna_sequencing is also necessary in applications like preimplantation_genetic based on the genotype of an individual cell biopsied from a blastocyst recent efforts have used single_cell to examine genotypic heterogeneity in tumors rates of somatic_mutations recombination_rates in the germ_line and probing of the genetic_diversity in unculturable bacterial_populations such as those naturally_occurring in the ocean or the human_gut the challenges associated with single_cell genome_sequencing are all due to the fact that the relevant dna only exists in a single_copy for example the nuclear_dna of a human cell weighs picograms while most standard library preparations specify a minimum input in the nanogram range special protocols are needed to prepare dna_sequencing libraries in single_cell applications whole_genome amplification wga is conducted prior to pcr with the goal of producing more copies of the genome in the form of long amplicons that uniformly cover the original genome biases in wga can dramatically alter the representations of different parts of the genome in the sequencing library methods have been developed to minimize wga amplification_bias by reducing the limiting volume for multiple_displacement mda to avoid exponential preferential amplification or looping of the amplicons to induce quasi linear amplification despite these advances whole_genome amplification remains far from uniform a major_problem in single_cell and low input sequencing is the loss of loci in the process of sequencing there are multiple opportunities for portions of the genome to disappear in the library_preparation making them unavailable for sequencing and subsequent observation this situation is known as locus dropout and creates significant problems for downstream_analysis for diploid cells locus dropout presents the additional difficulty that the dropout of one allele is easily mistaken for homozygosity new single_molecule sequencing_technologies still require some form of whole_genome amplification prior to pcr_amplification suggesting these problems will persist it is our goal here to investigate for a single_cell dna_sequencing library the genome_coverage from deep_sequencing which we define as the expected number of bases in the reference_genome covered by sequencing using high_throughput short_read technology the traditional mathematical_model of sequencing assumes that all parts of the genome are represented in the sequencing library in uniform abundance resulting in a simple poisson_distribution for the number of reads covering each base the possibility of unknown dropout implies this model is inadequate for single_cell additionally the uniformity assumption is lost due to a myriad of biases inherent to high_throughput these problems still exist for single_cell sequencing_experiments but are exacerbated by the low starting_material in addition to biases specific to wga one example is the observation that priming efficiency and extension rate of the dna_polymerase used in mda is dependent on nucleotide content leading to uneven amplification the highly non uniform molecular abundances and unknown dropout in the sequencing library are not the only problems in specifying a model for the genome_coverage in single_cell sequencing_experiments the coverage of local bases will be highly_correlated there is the natural correlation caused by nearby bases being covered by the same read additionally we expect broad correlations due correlated molecular abundances of nearby regions one example is local correlations related to nucleotide content due to the uneven amplification of mda these all create problems in mathematically modeling the sequencing_process as misspecification can create significantly biased_estimates our aim in this paper is to present a method for estimating the genome_coverage of a reference_genome in a deep sequencing_experiment based only on information from a shallow initial sequencing run one key to our method is treating sequenced nucleotides as independent observations despite the fact that the true unit of sampling is the sequenced read we show that the loss of information caused by this assumption is acceptable we adapt the non parametric empirical_bayes approach we developed previously for estimating library complexity which abstracts the sequencing_process as a capturerecapture experiment by applying our method to publicly_available human single_cell sequencing_data from a variety of sources and technologies we demonstrate the method to be accurate and widely_applicable we then investigate practical_considerations in applying the method including methods to reduce the running time and ways to reduce the cost of initial experiments finally we apply our method to a broad swath of recent shallow single_cell sequencing_experiments to show the variability of genome_coverage for differing protocols we described a method for predicting the genome_coverage gained from deeper sequencing of a single_cell genome_sequencing library based on a compound poisson_model of sequencing by ignoring local dependence we can approximate the number of bases covered by additional sequencing with a non parametric empirical_bayes estimator this estimator is extremely accurate for predicting additional coverage from relatively small amounts of additional sequencing but suffers from large instabilities for large_amounts of additional sequencing applying rational function approximations removes the instability and allows us to make accurate long_range predictions the running time of the algorithm may be unreasonably long for single_base estimates to facilitate researchers in obtaining quick and accurate estimates we introduced a strategy to reduce the running time of the algorithm significantly with a small cost in accuracy by randomly binning reads by choosing the bin size the researcher has the option to control how quickly estimates can be obtained keeping in mind the trade_off of accuracy and variance there is appreciable variability in genome_coverage both for the deeply sequenced libraries and for the extrapolated lowcoverage libraries this variability exists even for libraries originating in the same lab using the same protocol in such cases our method can help for selection of the best libraries to sequence deep this can help researchers in knowing the trade_off between sequencing_depth and observed loci prior to committing resources for deep_sequencing this is particularly important for studies involving single nucleotide variation that require deep_sequencing rather than broader variation such as copy_number one field where this is becoming increasingly important is full genome pre_implantation which until recently was considered impossible due to technological constraints finally a major barrier to the development of new technologies or optimization of current protocols is the resources_required to compare genome_coverage across libraries naive use of shallow test sequencing runs to compare libraries is often misleading as samples that initially appear to be high_complexity may suffer from large locus dropout supplementary the method we have presented can provide the information required for deep evaluation of libraries without deep_sequencing though we presented our analysis to the problem of sequencing single human cells the method is equally applicable to sequencing_projects with a reference_genome from low or highly degraded pr ufer et_al input bacterial samples with a reference_genome or when mapping to a reference_genome of a closely_related with unknown overlap 
