deep_learning of the tissue regulated splicing code motivation alternative_splicing as is a regulated process that directs the generation of different transcripts from single genes a computational_model that can accurately_predict splicing patterns based on genomic_features and cellular_context is highly_desirable both in understanding this widespread phenomenon and in exploring the effects of genetic_variations on as methods using a deep neural_network we developed a model inferred from mouse rna_seq that can predict splicing patterns in individual tissues and differences in splicing patterns across tissues our architecture uses hidden_variables that jointly represent features in genomic_sequences and tissue_types when making predictions a graphics_processing was used to greatly reduce the training time of our models with millions of parameters results we show that the deep architecture surpasses the performance of the previous bayesian_method for predicting as patterns with the proper optimization procedure and selection of hyperparameters we demonstrate that deep architectures can be beneficial even with a moderately sparse dataset an analysis of what the model has learned in terms of the genomic_features is presented alternative_splicing as is a process whereby the exons of a primary transcript may be connected in different ways during pre_mrna this enables the same gene to give rise to splicing isoforms containing different combinations of exons and as a result different protein products contributing to the cellular diversity of an organism furthermore as is regulated during development and is often tissue dependent so a single gene can have multiple tissue_specific functions the importance of as lies in the evidence that at least of human multi exon genes are alternatively_spliced and that the frequency of as increases with species complexity one mechanism of splicing regulation occurs at the level of the sequences of the transcript the presence_or of certain regulatory_elements can influence which exons are kept while others are removed before a primary transcript is translated into proteins computational_models that take into account the combinatorial_effects of these regulatory_elements have been successful in predicting the outcome of splicing previously a splicing code that uses a bayesian neural_network bnn was developed to infer a model that can predict the outcome of as from sequence_information in different cellular contexts one advantage of bayesian_methods is that they protect against overfitting by integrating over models when the training data are sparse as is the case for many datasets in the life_sciences the bayesian_approach can be beneficial it was shown that the bnn outperforms several common machine_learning such as multinomial_logistic mlr and support_vector for as prediction in mouse trained using microarray_data there are several practical_considerations when using bnns they often rely on methods like markov_chain mcmc to sample models from a posterior_distribution which can be difficult to speed up and scale up to a large number of hidden_variables and a large_volume of training data furthermore computation wise it is relatively expensive to get predictions from a bnn which requires computing the average predictions of many models recently deep_learning methods have surpassed the state_ofthe performance for many tasks deep_learning generally refers to methods that map data through multiple levels of abstraction where higher levels represent more abstract entities the goal is for an algorithm to automatically learn complex functions that map inputs to outputs without using hand crafted features or rules one implementation of deep_learning comes in the form of feedforward neural_networks where levels of abstraction are modeled by multiple non linear hidden layers with the increasingly rapid_growth in the volume of omic_data e g genomics transcriptomics proteomics deep_learning has the potential to produce meaningful and hierarchical representations that can efficiently be used to describe complex biological_phenomena for example deep networks may be useful for modeling multiple_stages of a regulatory_network at the sequence level and at higher levels of abstraction ensemble methods are a class of algorithms that are popular owing to their generally good performance and are often used in the life_sciences the strength of ensemble methods comes from combining the predictions of many models random_forests is an example as is the bayesian_model averaging method previously used to model the regulation of splicing recently neural_network learning has been improved using a technique called dropout which makes neural_networks behave like an ensemble_method dropout works by randomly removing hidden neurons during the presentation of each training example the outcome is that instead of training a single model with n hidden_variables it approximates to whom correspondence should be addressed the author published_by this is an open_access the terms of the creative_commons http creativecommons org_licenses which permits non commercial re use distribution and reproduction in any medium provided the original_work for commercial re use please_contact permissions_oup com the training of n different networks each on a different subset of the training data it is described as an extreme form of bagging and is a computationally_efficient way of doing model averaging with large_datasets learning with mcmc methods can be slow and can be outperformed by stochastic optimization_methods in practice these algorithms process small_subsets minibatches of data at each iteration and update model_parameters by taking small steps in the direction of the gradient to optimize the cost_function it is common to use stochastic gradient_descent to train feedforward neural_networks the learning algorithm backpropagation is also conceptually_simple involving for the most part matrix multiplications which makes them suitable for speedup using graphics_processing gpu here we show that the use of large many hidden_variables and deep multiple hidden layers neural_networks can improve the predictive performances of the splicing code compared with previous work we also provide an evaluation method for researchers to improve and extend computational_models for predicting as another goal is to describe the procedure for training and optimizing a deep neural_network dnn on a sparse and unbalanced biological dataset furthermore we show how such a dnn can be analyzed in terms of its inputs to date aside from a small number of works deep_learning methods have not been applied in the life_sciences even though they show tremendous promise we show results supporting that dnn with dropout can be a competitive algorithm for doing learning and prediction on biological_datasets with the advantage that they can be trained quickly have enough capacity to model complex relationships and scale well with the number of hidden_variables and volume of data making them potentially highly suitable for omic datasets different from the previous bnn which used hidden units our architecture has thousands of hidden units with multiple non linear layers and millions of model_parameters supplementary we also explored a different connection architecture compared with previous work before each tissue type was considered as a different output of the neural_network here tissues are treated as an input requiring that the complexity of the splicing machinery in response to the cellular_environment be represented by a set of hidden_variables that jointly represent both the genomic_features and tissue context besides a different model architecture we also extended the codes prediction capability in previous work the splicing code infers the direction of change of the percentage of transcripts with an exon spliced in psi relative to all other tissues here we perform absolute psi prediction for each tissue individually without the need for a baseline averaged across tissues we also predict the difference in psi psi between pairs of tissues to evaluate the models tissue_specificity we show how these two prediction tasks can be trained simultaneously where the learned hidden_variables are useful for both tasks we compare the splicing codes performance trained with the dnn with the previous bnn and additionally optimized a mlr classifier on the same task for a baseline comparison a gpu was used to accelerate training of the dnn which made it feasible to perform hyperparameter search to optimize prediction_performance with cross_validation we present three sets of results that compare the test performance of the bnn dnn and mlr for splicing pattern prediction the first is the psi prediction from the lmh code tested on all exons the second is the psi prediction evaluated only on targets where there are large variations across tissues for a given exon these are events where psi ae for at least one pair of tissues to evaluate the tissue_specificity of the model the third result shows how well the code can classify psi between the five tissue_types hyperparameter tuning was used in all methods the averaged predictions from all partitions and folds are used to evaluate the models performance on their corresponding test_dataset similar to training we tested on exons and tissues that have at least junction reads for the lmh code as the same prediction target can be generated by different input configurations and there are two lmh outputs we compute the predictions for all input combinations containing the particular tissue and average them into a single prediction for testing to assess the stability of the lmh predictions we calculated the percentage of instances in which there is a prediction from one tissue input configuration that does not agree with another tissue input configuration in terms of class_membership for all exons and tissues of all predictions agreed with each other have predictions that are in adjacent classes i e low and medium or medium and high and otherwise of those predictions that agreed with each other correspond to the correct class label on test data for the predictions with adjacent classes and for the remaining predictions this information can be used to assess the confidence of the predicted class_labels note that predictions spanning adjacent classes may be indicative that the psi value is somewhere between the two classes and the above analysis using hard class_labels can underestimate the confidence of the model 
