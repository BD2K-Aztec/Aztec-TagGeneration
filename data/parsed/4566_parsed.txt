databases_and the amordad database engine for metagenomics motivation several technical_challenges in metagenomic_data analysis including assembling metagenomic sequence_data or identifying operational_taxonomic are both significant and well known these forms of analysis are increasingly cited as conceptually flawed given the extreme variation within traditionally defined species and rampant horizontal_gene furthermore computational requirements of such analysis have hindered content based organization of metagenomic_data at large_scale results in this article we introduce the amordad database engine for alignment_free content based indexing of metagenomic_datasets amordad places the metagenome comparison problem in a geometric context and uses an indexing strategy that combines random hashing with a regular nearest_neighbor graph this framework allows refinement of the database over time by continual application of random hash functions with the effect of each hash function encoded in the nearest_neighbor graph this eliminates the need to explicitly maintain the hash functions in order for query efficiency to benefit from the accumulated randomness results on real and simulated_data show that amordad can support logarithmic query time for identifying similar metagenomes even as the database size reaches into the millions availability_and source_code licensed under the gnu_general version is freely_available for download from http smithlabresearch org amordadmetagenomicshasrevolutionizedourknowledgeofmicrobialcommunities such studies have uncovered intercommunity rules governingthebehaviorofthemicrobialnetworksandvariousaspectsof their symbiotic or parasitic lives within their host ecosystems with technological_advancements on the horizon whole metagenome shotgun wms sequencing has increasingly become popular the results of many wms projects ranging from direct study of simple biofilms in specific niches to complicated microbial consortia like human_gut are now publicly_accessible the first phase of the human_microbiome provided an unprecedented opportunity to explore our other genome however making sense of these data has proven tremendously challenging these challenges stem from both the extreme size and number of possible metagenomes which may be regarded as continuous mixtures of species whose boundaries can be poorly_defined popular existing frameworks such as mg rast and megan rely heavily on alignment based_algorithms to analyze metagenomic_data assembly of raw_reads into longer contigs binning of sequencing_reads and homology searching to characterize the predicted operational_taxonomic otus are among tasks that require sequence_alignment in such frameworks the computational requirements of these tasks however have reduced their usefulness in large_scale projects alignment_free approaches have been successfully_applied to overcome the scalability problems associated with alignment and assembly a large class of alignment_free algorithms rely on the the frequency of oligonucleotides of fixed_length k called k_mers to represent biological_sequences in metagenomics several k merbased algorithms have been proposed mostly targeting the phylogenetic characterization of the metagenomes the idea is to estimate the abundance level of different bacterial families in a metagenome by assigning accurate phylogenetic labels to its reads or contigs we introduce amordad a content based database engine for metagenomics designed to support rapid indexing and retrieval even as data_volumes reach massive scales in the most basic form the user makes a query by providing a metagenomic sample sequence and asks for the most similar metagenomes in the database similarity between two metagenomes is computed and query responses are ranked according to similarity with the query we describe a procedure for assigning an empirical significance score to query responses in this context which provides additional insight about retrieved metagenomes proximity scores also allow for a given query efficient identification of the full set of metagenomes sharing some threshold similarity with the query amordad does not attempt any sequence_alignment or assembly and is agnostic of otus focusing on the raw metagenome itself as the fundamental data element indexing is based on feature_vectors specifically k_mer sequences in the next section we describe the combination of random hashing and regular nearest_neighbor graph strategies which enable low complexity queries both in terms of time and memory_usage together these dual index structures allow the database to reorganize itself continually as new data is added an essential property that is difficult to achieve in highdimensional geometric databases we demonstrate the efficiency of amordad in handling up to millions of metagenomes to whom correspondence should be addressedwe have two objectives in studying the performance of amordad i investigating its potential for identifyingwe presented amordad a database engine for whole metagenome sequencing_data amordad uses alignment_free principles representing metagenomes as points in a high_dimensional geometric space lsh is used to efficiently index the database points to improve accuracy and efficiency beyond what is practical from lsh alone amordad augments this indexing with a regular nearest_neighbor graph the randomness in amordad is continually refreshed by resampling new random hash functions although only a fixed number of hash functions is alive within amordad at any given time those that are extinct have contributed to optimizing connectivity in the graph and thus continue to assist queries even if they are no longer used for hashing results from a series of experiments have demonstrated this approach to have a significant effect on query efficiency and accuracy the ability to cope with the rapid accumulation of data was a central design goal lsh is a well known approach to index high_dimensional but it is also a randomized method a major_drawback is the number of hash tables one must maintain to provide guarantees on the accuracy of queries imposing time and space constraints this is a well known problem and solutions have been proposed to overcome this pitfall these solutions mostly rely on the fact that each hash function i e each bit in our scheme provides a ranking based on the proximity of points to a certain query therefore even if the closest neighbor is not hashed to the same bucket as the query it is highly probable that buckets close to the query bucket contain the nearest_neighbor consequently one may avoid generating many hash tables at the cost of checking more buckets from each table this clever strategy has a direct effect on the indexing memory_consumption however as the distance between the query and its nearest_neighbors increases there is a rapid_growth in the number of buckets that must be checked this presents a challenge in our applications because of the inherent diversity of microbial_communities in many circumstances in fact if we view a set of biologically related metagenomes as a cluster in high dimensional_space the diameter of some complex clusters like human_gut might be large enough to make the process of searching close buckets to the query bucket time consuming our approach augmenting the lsh indexing with a graph_structure avoids having to explicity search additional buckets and depends on transitivity of relationships encoded in the graph intuitively if the underlying distance measure between metagenomes satisfies the triangle inequality the graph should guide the search in the most appropriate directions additionally the lsh provides a means of streamlining the database maintenance process hash tables are maintained in a fixed size queue that is continually updated and each new hash_table contributes to refining edges in the graph as a consequence we are able to keep the graph in a near ideal state without requiring explicit operations to maintain the graph after metagenomes are added to or removed from the database this design decision acknowledges that in the near future large and distributed metagenome databases will be updated frequently and efficiency of queries will be more important to users of amordad it is clear that many applications of amordad will involve maintaining large metagenomic_data clusters so that new metagenomes can be understood through their neighborhoods in the graph from this perspective the graph becomes the primary structure and the lsh serves simply to find the right neighborhood without exhaustive_search finally we address some limitations of this work we represent each metagenome by a feature_vector that included the frequencies of all k_mers for a fixed k we did not use feature_selection and were bound to relatively small values of k many of the k_mers were almost certainly irrelevant in our experiments on real_data more flexibility could be obtained if a metagenome is represented by a bag of features composed of a variety of words i e subsets of k_mers for varying k from statistical point_of this is equivalent to shifting the paradigm from a full markov_chain of order k toward a variable_length markov_chain b uhlmann and wyner the main_challenge is to find a parsimonious algorithm for feature_extraction at large_scale more important than irrelevant features however are features representing technical_artifacts for example of the sequencing_experiments 
