sequence_analysis blue correcting sequencing_errors using consensus and context motivation bioinformatics_tools such as assemblers and aligners are expected to produce more accurate results when given better quality sequence_data as their starting_point this expectation has led to the development of stand alone tools whose sole purpose is to detect and remove sequencing_errors a good error_correcting tool would be a transparent component in a bioinformatics pipeline simply taking sequence_data in any of the standard formats and producing a higher quality version of the same data containing far fewer errors it should not only be able to correct all of the types of errors found in real sequence_data substitutions insertions deletions and uncalled bases but it has to be both fast enough and scalable enough to be usable on the large_datasets being produced by current_sequencing and work on data derived from both haploid and diploid organisms results this article_presents blue an error_correction algorithm based on k_mer consensus and context blue can correct substitution deletion and insertion errors as well as uncalled bases it accepts both fastq and fasta formats and corrects quality_scores for corrected bases blue also maintains the pairing of reads both within a file and between pairs of files making it compatible with downstream tools that depend on read pairing blue is memory_efficient scalable and faster than other published tools and usable on large sequencing_datasets on the tests undertaken blue also proved to be generally more accurate than other published algorithms resulting in more accurately aligned_reads and the assembly of longer contigs containing fewer errors one significant feature of blue is that its k_mer consensus table does not have to be derived from the set of reads being corrected this decoupling makes it possible to correct one dataset such as small set of mate_pair reads with the consensus derived from another dataset such as illumina_reads derived from the same dna_sample such cross correction can greatly improve the quality of small and expensive sets of long_reads leading to even better assemblies and higher quality finished genomes availability_and the code for blue and its related tools are available from http www bioinformatics csiro au blue these programs are written in c and run natively under windows and under mono on linux the introduction of the first life_sciences sequencer in marked the beginning of a revolution in biological_research sequencing_technology has continued to advance rapidly producing ever more data at a lower cost but the quality of these data have improved at a much slower rate a single run on an illumina_hiseq system can now produce up to billion paired_end but these will still have an overall error_rate of the nature of these errors depends on the sequencing_technology being used and its underlying biochemistry the single base at a time sequencing by synthesis technique used by illumina results mostly in substitution_errors technologies based on different chemistries such as those used by and ion_torrent systems are prone to misreport the length of strings of the same base homopolymers resulting in insertion and deletion errors the tools used to analyze sequence_data are all error tolerant to some extent aligners will tolerate some number of mismatches when they are mapping reads to a reference some of which will prove to be errors and other genuine differences between the organism being sequenced and the reference similarly assemblers can be built to tolerate errors to some degree and their success at doing this is a significant factor in their overall effectiveness and accuracy an alternative way of addressing the problem of sequencing_errors is to use a stand alone error_correction tool whose sole purpose is to take a set of reads and improve their quality by finding and fixing errors such tools are founded on the high levels of redundancy present in typical sequencing_datasets with each location in the sequenced_genome being covered by many reads most of which will agree about which base is actually present recently surveyed a number of the published error_correction tools and categorized them into three classes of algorithms k spectrum based suffix_tree array based and multiple_sequence based the three classes of algorithm differ both in how they detect errors and how these errors are corrected we refer the reader tofor a full discussion of these three classes of algorithms and their history blue is a k spectrum algorithm that uses read context to choose between alternative replacement k_mers with the overall goal of minimizing the number of changes needed to correct an entire read all k spectrum based_algorithms first tile their input reads to produce a set of distinct overlapping subsequences of length kto whom correspondence should be addressed k_mers together with their repetition counts such a set can then be used to distinguish k_mers that come from the organism being sequenced and so recur many times from those that are derived from reads containing sequencing_errors typically only appearing once or a few times shows a k_mer repetition histogram for a set of illumina_reads derived from a typical bacterium clostridium sporogenes pa those k_mers from the error_free parts of reads will have repetition counts that lie somewhere on the right hand side rhs peak in this histogram shows a comparable histogram for a strongly heterozygous diploid organism helicoverpa_armigera with two overlapping peaks one corresponding to the k_mers found on both alleles and the other to those k_mers found on only one finally shows the histogram derived from tiling four lanes of homo_sapiens data illumina_hiseq from sra err to err given such a consensus table of k_mers and counts a repetition depth threshold can then be used to identify good k_mers as shown in a simple datasetwide threshold is unlikely to be usable though as uneven coverage along a genome and the presence of repetitive_regions is likely to result in the rejection of correct k_mers in poorly covered areas and the acceptance of sequencing_errors in highcoverage areas blue uses a partitioned hash_table to hold the k_mers corresponding to the rhs peaks the consensus about what k_mers are really present in the genome being sequenced the data loaded into this table is generated by the associated tiling tool tessel which simply takes a set of reads tiles it into overlapping k_mers and writes out a file of distinct canonical k_mers and their repetition counts for each strand decoupling the building of the consensus from the correction algorithm in this way makes it possible to use blue to cross correct read datasets such as using a large and inexpensive illumina dataset to correct a smaller more expensive but longer set of reads this style of crosscorrection results in a dataset that conforms to the consensus found in the illumina data effectively generating long illumina_reads that can be used to great effect in assemblies and when finishing genomes repetitive_regions in genomes including ribosomes transposons and shared regulatory_sequences are challenging for all error_correction algorithms reads that cross the boundaries of these repeated regions may be erroneously corrected as the change in depth of coverage at their edges may look very much like an error the choice of which possible fix is correct including doing nothing really depends on context and cannot simply be decided purely by considering a single k_mer or similar short sequence in isolation blue addresses this problem by evaluating alternative fixes in the context of the read being corrected the metrics computed for every alternative reflect the impact that each one would have on the rest of the readwill this fix get us to the end of the read with no or few additional fixes or will we have to effectively rewrite much of the rest of the read it does this by recursively exploring the tree of potential corrected reads the next section discusses the approach we took to testing blues performance and effectiveness and comparing it with other published tools section_describes the blue error_correction algorithm and section discusses the results of the performance and effectiveness tests section discusses future work and possible improvements the primary goal in the development of blue was to create a practical tool that would help biologists get more accurate results from their sequencing_datasets blue had to be sufficiently fast and memory_efficient to allow it to correct todays large_datasets using reasonable resources and effectively transparent so it could be used within existing analytical workflow tools such as galaxy just taking in a sequencing dataset and writing it out again after removing as many errors as possible while maintaining file_formats quality_scores and read pairings our tests have shown that blue meets these goals it is faster than the other algorithms tested and its low memory_requirements make it practical to use with current large sequencing_datasets blue has been shown to be more accurate than any of the other algorithms we tested on both illumina and data the assembly tests showed that blue corrected reads consistently produced the longest and most error_free contigs of all the tools tested blues ability to correct insertion and deletion errors allows it to be used with great effect on datasets generated on the and ion_torrent platforms decoupling the reads being corrected from the set of reads used to generate the k_mer consensus table allows for cross correcting long homopolymer prone reads with short but cheaper illumina_reads resulting in even better correction of these datasets blue has already been used to improve the assemblies for published microbial_genomes derived from pure_cultures it has also been used on metagenomic_datasets to improve draft genome_assemblies of the dominant organisms in these communities correcting metagenomic sequence_datasets works only when the dominant organisms are taxonomically distant and so share few k_mers in this case correcting the reads has the useful side effect of removing rare_variants of the dominant organisms giving both better assemblies and improving the performance of the assemblers themselves blue has also been successfully used on diploid data both human and insect blue is currently being used on a major insect genome_project and its ability to cross correct long mate_pair reads with illumina data have proven to be useful to this team blue will continue to be tested and refined on new types of sequencing_data as these emerge with an immediate focus on pacbio another area of anticipated work is improving the correction of diploid data at those places where differences between the two alleles cause difficulties for assemblers 
