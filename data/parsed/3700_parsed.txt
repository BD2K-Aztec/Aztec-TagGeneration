prioritizing hypothesis_tests for high_throughput data motivation the advent_of data has led to a massive increase in the number of hypothesis_tests conducted in many types of biological studies and a concomitant increase in stringency of significance thresholds filtering_methods which use independent information to eliminate less promising tests and thus reduce multiple testing have been widely and successfully_applied however key_questions remain about how to best apply them when is filtering beneficial and when is it detrimental how good does the independent information need to be in order for filtering to be effective how should one choose the filter cutoff that separates tests that pass the filter from those that dont result we quantify the effect of the quality of the filter information the filter cutoff and other factors on the effectiveness of the filter and show a number of results if the filter has a high_probability e g of ranking true_positive features highly e g top then filtering can lead to dramatic_increase e g fold in discovery probability when there is high redundancy in information between hypothesis_tests filtering is less effective when there is low redundancy between hypothesis_tests and its benefit decreases rapidly as the quality of the filter information decreases furthermore the outcome is highly dependent on the choice of filter cutoff choosing the cutoff without reference to the data will often lead to a large loss in discovery probability however navenave optimization of the cutoff using the data will lead to inflated_type we introduce a data based method for choosing the cutoff that maintains control of the family_wise via a correction_factor to the significance threshold application of this approach offers as much as a several fold advantage in discovery probability relative to no filtering while maintaining type_i control we also introduce a closely_related method of p value weighting that further improves_performance availability_and r code for calculating the correction_factor is available at http www stat uga edu people faculty paul schliekelman a dominant trend in biology in recent_years has been the development of high_throughput techniques and the dramatic_increase in the resolution of available data however most of the information gained is not relevant for any particular question at hand and comes at the cost of more hypothesis_tests and thus more stringent statistical thresholds there is often high redundancy between tests and the gain in information may be slower than the increase in resolution thus higher_resolution will not always lead to higher probability of discovery given the realities of multiple testing it is unlikely that a mere increase in throughput and resolution will greatly_increase discoveries rather it will be necessary to combine high_throughput data with other sources of information in order to better target investigations the problems of multiple testing are well understood and many methods have been proposed for using external informationto filter for the most promising features of the data such methods typically have two stages first some filtering criterion is used to select the most promising features then only those features are tested for the effect of interest these include methods for microarrays rnaseq genome_wide and epistasis despite the popularity of filtering_methods key_questions remain about their general statistical_properties discussed the conditions sufficient for maintaining type_i control and showed that the key requirement is that the null_hypothesis distribution of the test_statistics after filtering should be the same as the null_hypothesis distribution before filtering they showed that some filtering techniques in use can violate this requirement their focus was on the conditions for filtering to be valid little is known about the conditions required for filtering to be successful in significantly increasing discovery probabilities and our focus is on this question in this paper we address two major_issues first we evaluate the usefulness of filtering and determine major factors_affecting its behavior we quantify the effect of the filtering statistic in terms of its probability of ranking true effects highly we show that a strong filter that has a high_probability of ranking true_positives in e g the top can greatly_increase discovery probability as much as fold when there is high redundancy between features and when a good cutoff is known in advance even a random filter can increase power when the cutoff_point is well chosen on the other hand filtering is less effective when there is low redundancy between features and if the filter statistic is not able to reliably rank true_positive features highly second most applications of filtering_methods have used ad_hoc approaches to choosing the filter cutoff_point the filter cutoff_point refers to the value of the filtering criterion which separates features that will be included in the second stage and those which will not we show that the filter cutoff has a large effect on the performance of filtering_methods a good choice can make a several fold_difference in discovery probability relative to a poor choice furthermore inappropriate ad_hoc methods can greatly inflate false_positive we introduce a general and rigorous method for choosing the filter cutoff and show that this approach can increase discovery probabilities by several fold relative to no filtering we also introduce a simple and intuitive method for weighting p values that is closely_related to our filtering method and improves the performance further 
