genetics_and a new way to protect privacy in large_scale genome_wide increased availability of various genotyping techniques has initiated a race for finding genetic_markers that can be used in diagnostics and personalized_medicine although many genetic risk_factors are known key causes of common diseases with complex heritage patterns are still unknown identification of such complex_traits requires a targeted study over a large collection of data ideally such studies bring together data from many biobanks however data aggregation on such a large_scale raises many privacy issues results we show how to conduct such studies without violating privacy of individual donors and without leaking the data to third parties the presented solution has provable security guarantees genome_wide gwas are one of the driving reasons behind the formation of nationwide and privately funded gene banks many chronic_diseases and various cancer_types are known to have genetic disposition factors although many underlying_genetic signatures have been successfully identified for mendelian_disorders not many genetic risk_factors for complex_diseases have been discovered and confirmed gwas have identified some risk_factors for type_ii and for a few other common diseases gwas have been modestly successful in pharmacogenetics and cancer research the size and structure of a study cohort are the main limiting_factors in such studies as the individual impact of genomic differences is usually small larger_sample increase the sensitivity of statistical_tests and make it possible to apply a wide_range of data_mining methods ideally studies should use nationwide and continent wide patient_cohorts formation of such cohorts is becoming feasible as genotyping costs are rapidly decreasing in addition to nationwide biobanks e g the uk biobank several personal_genomics companies such as andme and navigenics already possess large and diverse patient_cohorts biobanks are also forming large collaboration networks such as p g and hugenet to combine their patient_cohorts and improve study quality privacy of individual gene donors is one of the biggest concerns in such projects in many countries genotype data are classified as sensitive data that can be handled by complying with specific restrictions e g hipaa in the usa and the data protection directive in the european_union these restrictions are justified as a leak of genetic_information can cause genome based discrimination when more health related patterns have been discovered standard anonymization methods are not applicable to genotype data as the data themselves are an ultimate identity code only out of million single_nucleotide snps are needed to uniquely identify a person moreover the size of online genotype databases for genealogy studies such as sgmf and yhrd has made re identification of anonymized genotype data a real threat re identification attacks based on combining inferred phenotypes with public data become practical as the list of known associations between genotype and phenotypic_traits evolves finally showed that even aggregated pools of genomic_data can leak private information although follow_up studies softened initial claims the threat remains these findings created a debate whether one can promise privacy of genotype data in consent_forms at all p g in the following we show how to set up an infrastructure where the genotype data can be stored and processed so that none of the peers involved in the process can reconstruct the data and thus the risk of accidental leaks and malicious data abuse is greatly_reduced the data analysis algorithms are executed in an oblivious manner so that only the desired outcome is revealed to the user and nothing else differently from well known data perturbation and masking techniques security guarantees are cryptographic these guarantees depend on the computational_complexity of well established mathematical problems and not on the background_knowledge of potential attackers as such the presented methodology is applicable to protecting biobanks and other medical_databases to whom correspondence should be addressed the author published_by all_rights this is an open_access the terms of the creative commons attribution license http creativecommons org licenses by which_permits distribution and reproduction in any medium provided the original_work to demonstrate the feasibility of our approach we used the sharemind multi party computation platform to implement core algorithms for gwas our choice was mainly motivated by the efficiency and ease of use of the sharemind platform alternative platforms and fairplaymp should give similar results we used genotypes from the hapmap project measured with the affymetrix mapping k array as the main data source in each experiment we divided the data randomly into case and control_groups and performed genome_wide search for highly_differentiated snps for that we used cryptographically secure counterparts of standard statistical_tests used in gwas two tests for independence cochran_armitage test for trend and tdt as our algorithms return exactly the same outputs as original algorithms we report only performance results for various sub tasks to show the variability of running_times we report the mean and standard_deviation of four independent runs each of the donors has measured snps first we ran the algorithm on the data of donors and then we went on to test the data of and donors we performed the experiments on three servers running sharemind each server was an off_the server grade machine with gb ram of which less was used twelve ghz intel_xeon westmere cores of which two were used and a gb s local_area network lan connection at the moment the network connection is the bottleneck in terms of algorithm running time however sharemind has been successfully used in real applications the time spent on data_acquisition and secure storage does not depend on the statistical_test used later on it depends only on the number of snps and the number of gene donors the average time it takes to encode and share the snps for the described case can be seen in note that secret sharing and uploading data are done only once for each dataset hence this is a singletime cost the time needed to form casecontrol groups depends on the application scenario when the analyst has direct access to phenotype data and can form case and control_groups by herself himself then there is no computational_overhead in more involved cases the case and control_groups must be constructed based on secret shared phenotype attributes in this case the overhead depends on the complexity of inclusion_criteria for case and control_groups filtering results presented inshow that formation of such groups can be done in seconds for typical inclusion_criteria that consist of simple comparison operations mixed with logical conjunctives the time needed to perform the statistical_test depends on the test but in all cases it can be broken down into counting allele_frequencies and evaluating test_statistic as tables and clearly show the main performance bottleneck is frequency counting which scales linearly w r t the total number of snp measurements as our encoding is optimized for test a better encoding will enhance the performance of the cochranarmitage tests but not beyond results the total duration of the analysis is the sum of the frequency analysis and evaluation as the other parts have a negligible duration the presented results clearly show that cryptographically secure evaluation of statistical_tests on genome_wide scale is practically feasible the expected running time is hours instead of a few minutes when computed non securely however the latter is not a significant slowdown compared with the time needed to acquire the data if the secure analysis method is not used although the results prove the practical feasibility of cryptographically secure gwas the solution is also notably more resource demanding than the alternatives we have to analyze further whether potential benefits outweigh costs we consider three potential application scenarios and contrast our approach with the alternatives 
