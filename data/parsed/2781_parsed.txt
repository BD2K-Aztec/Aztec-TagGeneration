sequence_analysis making automated multiple_alignments of very large_numbers of protein_sequences motivation recent_developments in sequence_alignment software have made possible multiple_sequence msas of sequences in reasonable times at present there are no systematic_analyses concerning the scalability of the alignment_quality as the number of aligned sequences is increased results we benchmarked a wide_range of widely used msa packages using a selection of protein_families with some known structures and found that the accuracy of such alignments decreases markedly as the number of sequences grows this is more or less true of all packages and protein_families the phenomenon is mostly due to the accumulation of alignment errors rather than problems in guide_tree construction this is partly alleviated by using iterative refinement or selectively adding sequences the average_accuracy of progressive methods by comparison with structure based benchmarks can be improved by incorporating information derived from high_quality structural_alignments of sequences with solved structures this suggests that the availability of high_quality curated alignments will have to complement algorithmic and or software developments in the long_term availability_and benchmark data used in this study are available at http www clustal org omega homfam tar gz and http www clustal org omega bali fam tar gz multiple_sequence msas of many thousands of protein_sequences are becoming commonplace the biggest families in pfam have sequences or domains and will expand greatly as new genome_sequences are being sequenced msas are an integral part of how pfam and other domain databases are used and maintained metagenomics research routinely involves the use of alignments of tens_of of sequences and almost all phylogenetic_analysis involves generating an msa as a starting_point however the generation of the msa can be computationally too intensive for very large_scale phylogenetic_analysis recent work on predicting protein_structure from sequence_alignments is based on having very high_quality alignments of many thousands of sequences e g alignments of thousands of sequences have also been used in the area of virus classification and epistasis only a few of the standard msa packages are capable of aligning tens_of of sequences in a recent article we described a new package called clustal omega which makes it practical to align protein_sequences on a desktop computer and is as accurate as some of the most computationally_demanding methods that can only align a few hundred sequences the parttree program of the mafft package and kalign can also make alignments of this size although with a lower accuracy some studies suggest that the quality of an alignment may increase as more sequences are added this is only true if the newly added sequences are few and carefully_chosen as of yet no systematic_analyses have been conducted for large_numbers of homologous_sequences in this article we look at some of the issues that occur when making alignments of sequences using standard automatic msa packages for small alignments we confirm a limited increase in accuracy however only if the added sequences are carefully_chosen we find a universal trend towards marked decrease in alignment_accuracy as large_numbers of sequences are added indiscriminately we explore strategies that attenuate this deterioration but they are useful only in certain cases the strategy that best preserves alignment_accuracy with very large_datasets is to use a very high_quality alignment of a small subset of the sequences to help guide the alignment this suggests that very large_alignments of high_quality may be possible but only if very high_quality alignments such as those from structure superpositions or expert curated alignments are available all of the standard automatic msa packages behave very similarly when the number of sequences to be aligned is increased into the thousands although few families exhibit a marked improvement in accuracy the average accuracyas measured on structure based benchmarksdecreases steadily this raises two obvious questions what is the reason for the fall off and how can it be fixed the simplest explanation for the fall off in accuracy is attrition owing to the accumulation of noise and or alignment errors as sequences are added all of the widely used algorithms are based directly_or on progressive alignment which aligns the sequences according to the branching order in a guide_tree this requires a series of alignment steps at any of which alignment errors can be made these errors cannot be reversed except by iteration of the alignment process such alignment errors occur less frequently with programs such as t coffee that use consistency but such programs cannot easily cope with sequences the presence of fragments change in homfam tc score for different algorithms and different sampling schemes a clustal omega b kalign c mafft lins i d probalign random_sampling with bullets and thicker lines sampling of sequences of high similarity with circles of low similarity with crosses and of in between similarity with diamonds and boxes frameshifts swapped domains and very large insertions_or will aggravate this situation with small numbers of sequences the algorithms have proved to be very robust for general use with very large_numbers of sequences however the number of opportunities for irreversible alignment errors increases steadily even if during progressive alignment only one sequence_alignment in a thousand introduces a serious error in a dataset of this will occur times in large_datasets the scope for errors is simply very great by fixing the guide tree_topology we were able to separate out the effects of possible errors in guide_tree construction from alignment errors guide_tree construction certainly has an effect on alignment_accuracy but it is not the main source of error here iteration does help to delay the fall off in accuracy to an extent we tested various combinations of iteration of guide_tree construction and alignment for small_to datasets the effects are noticeable but the fall off in accuracy inevitably follows either the iteration strategy needs to be changed or it needs to be done more intensively this would have the effect of greatly_increasing alignment times carefully choosing the sequences to be aligned certainly has a beneficial_effect again for modest increases in dataset size we observe the best results when sequences of intermediate similarity are added clearly demonstrates that sequences that are very similar did not improve accuracy perhaps if huge alignments are desired new sequences to be added to the dataset must be selected carefully using progressive alignment packages is not the only way to make very large_alignments however in the pfam_database hmmer is used in a simple process to add sequences one at a time to a smaller seed alignment the accuracy of such alignment schemes has not been tested much and it has probably been assumed that the accuracy is low the full pfam alignments are not intended as high_accuracy alignments in clustal omega there is a facility to use a pre_existing hmm to help the alignment of a new set of sequences in a process called epa in the long_term the most obvious solution to the issue of how to make very large_alignments may be to use smaller high_quality alignments as seeds or external profiles and algorithms for extending alignments such as pagan papara or as explained in progressive alignment alone can make the alignments but the accuracy will be a serious issue without new algorithms or strategies being developed 
