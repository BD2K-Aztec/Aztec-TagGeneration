flexible data_integration and curation using a graph_based approach_motivation the increasing diversity of data available to the biomedical scientist holds_promise for better understanding of diseases and discovery of new treatments for patients in order to provide a complete picture of a biomedical question data from many different origins needs to be combined into a unified representation during this data integration_process inevitable errors and ambiguities present in the initial sources compromise the quality of the resulting data_warehouse and greatly diminish the scientific value of the content expensive_and manual_curation is then required to improve the quality of the information however it becomes increasingly difficult to dedicate and optimize the resources for data_integration projects as available repositories are growing both in size and in number everyday results we present a new generic methodology to identify problematic records causing what we describe as data hairball structures the approach is graph_based and relies on two metrics traditionally used in social_sciences the graph density and the betweenness_centrality we evaluate and discuss these measures and show their relevance for flexible optimized and automated data curation and linkage the methodology focuses on information coherence and correctness to improve the scientific meaningfulness of data_integration endeavors such as knowledge bases and large data warehouses with the current quantity and diversity of data available in the biomedical_domain it becomes increasingly necessary to combine the information coming from multiple sources in a variety of formats into a unified representation this practice goes by the name of data_integration or record_linkage different reasons motivate the exercise it can be for instance the appealing possibility to query and analyze information about complementary thematics e g genedisease relationship consolidation of some knowledge existing about one topic or the absorption of a source into another for maintenance needs e g dataset coming from company acquisitions recently data_integration efforts have been particularly active in the drug_discovery domain with platforms such as transmart or open phacts focused on building a pharmacological space from public_repositories other examples_include chemspider a resource providing a central hub related to chemical names and structures from various sources or identifiers org a cross reference platform for biomedical identifiers and data connections fundamentally data_integration can be seen as a problem of creating the correct links between equivalent yet disconnected database records this process is sometimes called stitching or reconciliation once records are rightfully associated it becomes possible to query across or merge them if necessary as links are created between entries it is v c the author published_by all_rights for permissions please_e journals permissions_oup com intuitive to rely on an abstract graph_structure to represent the problem faced vertices or nodes are records or entities of interest edges represent the associations between them an illustration of the graph abstraction is the semantic_web this series of standards relies on the resource_description rdf and a graph_structure to facilitate the interoperability and integration of independent pieces of information on the world_wide the semantic_web also provides means to establish equivalence between segregated records sameas or exactmatch relations with the use of rules or reasoners for instance moreover the biomedical_domain is rich in unique identifiers e g chemical_structure identifiers like inchikeys and cross_references which can be used to automatically assess equivalence between database entries if one record references another it might be possible to deduce that the two entities are the same for instance all these strategies are automatable and reliable in theory however complications can arise quickly if cross_references are absent errors present in the original sources or if the data is fuzzy and ambiguous by nature such as with drug names and chemical_structures for instance the effect of erroneous information is dramatic for data_integration records can get incorrectly associated with other entries themselves recursively linked to other records this cascade of events leads to the creation of unwanted hairballs which we define as groups of records not equivalents and not supposed to be linked but yet connected because of the deficient state of the data in such a scenario manual intervention from expert curators is required in order to improve the quality and scientific validity of the dataset unfortunately with the increasing size of biomedical databases and repositories it becomes more likely that such errors will arise by chance and more expensive_and to correct them by hand in this document we use an in house data_integration project related to the creation of a drug_product terminology in order to illustrate a generic and flexible_approach to identify and handle problematic and erroneous records during the integration_process the drug terminology is built from millions of database entries present in eight heterogeneous sources including four from third_party vendors integrity cortellis pharmaproject and adisinsight one developed internally and three public drug databases drugbank part of chembl and chebi our motivation is to build and maintain an integrated database also called data_warehouse from these various sources each of which contains a part of the desired information see the graph_based method presented allows researchers to isolate and prioritize problematic entries and flexibly adjust the curation work required over an automatic integration to maximize the quality and scientific usefulness of the data the goal of the methodology is to integrate large_amounts of heterogeneous data coming from different sources while detecting and handling errors to optimize the required curation equivalent records from different databases can indeed be linked based on the common labels they share e g a brand name as presented in the methodology section link step the approach is very straightforward and assumes that entities with the same name are identical however this strategy fails when a label is misassigned in an original record or when a name is ambiguously used to describe different entities as a result links between records are incorrectly created and propagated which creates hairballs or large sets of records erroneously grouped together for such cases the source entries containing the misleading information at the origin of the hairball must be excluded we present in this section the results and evaluation of the methodology and characterize with examples the hairballs and their resolution towards better quality scientific content we started with eight different sources describing drug records and containing a total of unique label entries after the link step graphs were created the final database of integrated records contains entries i e the drug_products and labels in the process problematic records were identified and excluded from the build of total starting number of records we present in this manuscript a generic methodology to integrate and merge data coming from different repositories into a central warehouse containing the consolidated information as opposite to previous_approaches see section the method emphasizes error_detection and is qualified as flexible for multiple reasons first theof the records with high bc after the first cleaning step the lipitor related entries are still mixed with other products blue graph low_density some graphs are now dense and do not need further cleaning for instance b refers to gemfibrozil and b to adenosine c the hairball is subject to a second iteration step where more records are excluded arrows on the graph d the entries related to the atorvastatin are eventually well isolated blue graph and ready to be merged as a high_quality consolidated record the other drug_product d is fenofibrate see_supplementary for bigger version of this figure assertion of equivalence between records can be implemented as a series of steps including for instance data_cleaning and transformation in order to best match the desired outcome e g chemical_structures cross_references free_text label secondly the approach can handle n numbers of data_sources and is not limited to handle only pairs of records thirdly the density measure helps to extract numbers regarding how confident one can be in regards to the quality of the integrated information and to determine how much curation is required on the top of the data finally the betweenness_centrality identifies the erroneous records and can be used to resolve issues either automatically or manually thresholds for graph measures can be flexibly adjusted too in order to optimise the work based on the time and resources available the abstraction of the data_integration problem as a graph_representation has been already introduced by previous work yet the use of graph measures on the top of the data to assess the quality of the integration and to detect anomalies is novel as far as our knowledge goes from a usability perspective we believe that the quality of the integrated data is more important than the technology or standards used in this regard the methodology can be implemented in a variety of frameworks from rdf graphs to traditional relational databases as needed by the user other graph indicators could be used in the future for similar tasks and to quantify different problem types coming from data_integration as an example we decided not to assign weights to the edges of the graphs this choice was motivated by the sparsity of the data we estimated that records sharing one label were just as likely to be equivalent to records sharing multiple labels considering weights could result in the computation of different graph metrics useful to characterize a particular type of records assuming a rationale could be defined for the alternative indicators the same way it was done here with the density and bc a particularly challenging_task resulting from data_integration initiatives is the maintenance of the created resource the update of records and the incorporation of new information this can be implemented alongside the methodology proposed in different ways first it is possible to create incremental versions of the warehouse with this approach the new data_sources are downloaded following a certain time period and the integrated dataset is rebuilt from scratch each time following the same graph based_methodology new and corrected records are tested again and excluded if necessary the main_drawback of this option comes from the difficulty of maintaining unique identifiers as equivalences can vary from one release to the other the second possibility for data update considers the current data_warehouse just as any standard starting source and tries to match and disambiguate new records following the same methodology as presented in this article identifiers are easier to maintain as the new version derives from the old one but it might become challenging to keep track of the origin of the data removed information from starting sources such as synonyms can also be trickier to detect and correct in conclusion the goal of data_integration is to provide a complete picture of a scientific problem we introduced here a methodology emphasizing data coherence and correctness to fulfill this greater task where ambiguities redundancies and errors are identified and removed in summary the methodology_presented addresses practical concerns faced by large_scale data_integration exercises prevalent nowadays in the drug_discovery domain in an ideal world database entries and cross_references would be perfectly maintained and errors non existent which would enable the straightforward creation of a semantic network between entities in practice a costly and tedious curation step is often necessary in order to control the quality of the resource and the scientific value of its content the presented methodology focuses on this aspect in order to best prepare the integrated data for the derivation of scientific_knowledge 
