genecodeq quality_score compression and improved genotyping using a bayesian_framework motivation the exponential reduction in cost of genome_sequencing has resulted in a rapid_growth of genomic_data most of the entropy of short_read lies not in the sequence of read bases themselves but in their quality scoresthe confidence measurement that each base has been sequenced correctly lossless compression_methods are now close to their theoretical limits and hence there is a need for lossy methods that further reduce the complexity of these data without impacting downstream_analyses results we here propose genecodeq a bayesian_method inspired by coding theory for adjusting quality_scores to improve the compressibility of quality_scores without adversely impacting geno typing accuracy our model leverages a corpus of k_mers to reduce the entropy of the quality_scores and thereby the compressibility of these data in fastq or sam bam cram files resulting in compression_ratios that significantly exceeds those of other methods our approach can also be combined with existing lossy compression_schemes to further reduce entropy and allows the user to specify a reference_panel of expected sequence_variations to improve the model accuracy in addition to extensive empirical evaluation we also derive novel theoretical insights that explain the empirical performance and pitfalls of corpus based quality_score compression_schemes in general finally we show that as a positive side effect of compression the model can lead to improved genotyping accuracy availability_and genecodeq is available at over the past_decade unprecedented advances in next_generation ngs_technologies have led to a dramatic reduction in sequencing cost and much faster data production_rates these technological_advances are fostering increasingly wide_ranging applications in biotechnology public healthcare and personalized_medicine furthermore as genomic sequencing_data has grown exponentially they have outpaced advances in some information_technologies that they indirectly rely on computing_power and storage in particular genome_sequencing results in a large storage footprint for each genome thus storing and transferring raw sequencing information is becoming prohibitively_expensive and effective compression_schemes of raw_sequencing are indispensable the majority of ngs_data output consists of read sequence_information whereby each nucleotide base is associated with a sequence confidence_level also referred to as quality_score produced by the base caller the phred scale q log p encodes with integer quality_score q an estimate of the probability p that the base has been called incorrectly this information is used both for the quality_control of raw read data and for downstream processing including genome_assembly read_mapping and genotyping in compressed genomic_datasets quality_score values take up the dominating share for example when compared to the read sequence_data quality_scores of illumina_reads take at least more storage although this can be an even higher ratio when using more aggressive sequence compression quality_scores are more difficult to compress due to a larger alphabet in original_form and intrinsically have a higher entropy with lossless_compression algorithms and entropy encoders reaching their theoretical limits and delivering only moderate compression_ratios there is a growing interest to develop lossy compression_schemes to improve compressibility further quantizing quality_scores i e reducing the alphabet size is the most basic approach to improve compressibility in a lossy manner one such approach of reducing all quality values to eight levels bins has become a widely used standard for the illumina_platform and is enabled by default on the most recent machines another approach_called p involves local quantization so that a representative quality_score replaces a contiguous set of quality_scores that are within a fixed distance of the representative score similarly the r scheme replaces contiguous quality_scores that are within a fixed relative_distance of a representative score other lossy approaches improve compressibility and preserve higher fidelity by minimizing a distortion metric such as meansquared error or l based errors qualcomp and qvz however adoption of lossy compression_schemes for quality_scores has been slow due to concerns about adverse_effects on downstream_analyses in particular genotyping accuracy however there are also reports that compression_schemes such as p block r block qvz and qualcomp can under some circumstances lead to a slight improvement in genotyping accuracy a number of more recent_approaches utilize the sequence_data itself to guide the quality_score compression quartz achieves this using a reference corpus built from frequent mers across reads from individuals sequenced in the genomes_project read base_pairs that match any one mer in the corpus up to one allowed mismatch per mer have their quality_score set to a fixed high value this sparsification of quality_scores reduces entropy thus improving quality_score compressibility a different approach leon utilizes the dataset itself for building its set of k_mers and to generate a reference probabilistic de brujin graph in this case bases in a read that have enough highly frequent k_mers covering it within the dataset are set to a fixed high value quality_score both methods were reported to improve genotyping accuracy in this article we present genecodeq a lossy_compression scheme that is inspired by coding theory and bayesian_inference uniquely genecodeq uses a statistical_approach to objectively reason about the compressibility of quality_scores briefly our model estimates the posterior_probability of a sequencing_error given the evidence of the full read including quality_scores together with information from a reference corpus as a result the posterior estimates of most quality_scores are boosted above a saturation point of the phred scheme corresponding to very high_confidence this approach results both in a significant reduction in entropy and better genotyping accuracy when compared to existing_methods weve used sequences from na for evaluation purposes due to availability of high_quality trio validated snp calls to validate genotyping accuracy unmapped raw_reads were obtained for two datasets bullet srr a genomes_project genomes dataset with gigabases at coverage bullet na j a public dataset from the garvan institute with gigabases at coverage see supplementary_materials for details resulting variant_calls from genotyping were compared to the illumina platinum set http www illumina com platinumgenomes which served as a gold_standard in the supplementary_material we also provide results for additional datasets the raw fastq_files were processed with genecodeq quartz r p block c qvz qualcomp leon and illumina bin quantization we used the gatk best practices workflow and the samtools recommended workflow with and without basequality score recalibration bqsr in addition to running genecodeq in its default mode we also explored its behaviour with a reference only corpus see section for more details as well as in combination with other approachesillumina bin p block and r block see section for more details for comparison quartz was also run with a reference only corpus and in combination with illumina bin quantization we found that the bqsr stage produces a greater variable change to genotyping accuracy than most of the lossy_compression algorithms do alone pre bqsr this makes_it to separate and properly assess the impact of lossy_compression alone on postbqsr results on the lossless dataset applying bqsr resulted in a significant drop in genotyping accuracy combinations of bqsr with lossy_compression also resulted in lower genotyping accuracy versus the original across almost all methods however since bqsr had a large variable impact sometimes the lossy compressed postbqsr results appeared better than the lossless post bqsr results skewing relative comparisons there are strong indications that this is due to flaws with bqsr itself rather than due to the lossy_compression a detailed discussion included additional results can be found in the supplementary_material the improvements with genecodeq are seen across each combination of workflows for brevity the main results shown here are with the gatk best practices workflow excluding bqsr similar results are seen with the samtools recommended workflow which can be found in the supplementary_material along with bqsr results variant_calls that resulted from the datasets and produced using these workflows were ranked by their confidence_levels quality of the variant call to generate receiver_operating roc_curves this approach avoids choosing a specific quality threshold and hence is well suited to compare alternative_methods see alsowhere a similar approach is used however if two roc_curves have different domains i e a different set of variants for the x axis then they are not directly comparable to one another for the results here and in the supplementary_material the roc_curves and area under the curve auc were calculated with a common domain as additional quality_metrics we also report precision recall and f score metrics see_supplementary for more details on the evaluation approach for all these metrics to estimate the impact in compressibility both the raw and modified quality_scores were compressed with bzip http www bzip org unless a custom entropy coder was already integrated into the lossy_compression i e qvz qualcomp and leon we chose bzip because under its default options it consistently yielded superior compression numbers compared to zip http www zip org and gzip http www gzip org we note however that zip in its ppmd mode can perform even better and these results are included in the supplementary_material this includes significantly better zip re compressed results for leon which ordinarily uses the poorer performing zlib for its internal compression reports quality_score compression_rates and genotyping accuracy metrics for these approaches shows a scatter_plot summarizing the best results in depicting the genotyping accuracy and compressibility 
