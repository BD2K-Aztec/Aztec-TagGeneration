genome_analysis assessing the validity and reproducibility of genome_scale predictions motivation validation and reproducibility of results is a central and pressing issue in genomics several recent embarrassing incidents involving the irreproducibility of high profile studies have illustrated the importance of this issue and the need for rigorous methods for the assessment of reproducibility results here we describe an existing statistical_model that is very well suited to this problem we explain its utility for assessing the reproducibility of validation experiments and apply it to a genome_scale study of adenosine_deaminase acting on rna adar mediated rna_editing in drosophila we also introduce a statistical_method for planning validation experiments that will obtain the tightest reproduci bility confidence_limits which for a fixed total number of experiments returns the optimal number of replicates for the study availability downloadable software and a web_service for both the analysis of data from a reproducibility study and for the optimal_design of these studies is provided atthe issue of validation and reproducibility of scientific results has recently been the subject of intense discussion in the scientific_community several eye_opening reports have either claimed insufficient validation of bold research findings or shown an inability to replicate such results in genomics genetics oncology neuroscience pharmacology proteomics and psychology problems with reproducibility have been demonstrated with widely used technologies such as microarrays sirna based screens and mass_spectrometry this has led to appeals for increased statistical rigor platforms for the publication of neutral studies and attempted replicates whether successful or not and a system wide committed effort toward generating work that is reproducible placing at least as much emphasis on reproducibility as is currently placed on novelty recent attempts at addressing the issue of reproducibility include the reproducibility initiative which for a fee will carry out independent_validation of research findings and issue a certificate of reproducibility for those studies that validate https www scienceexchange com reproducibility and sciencecheck which provides a platform for researchers to report on the reproducibility and utility of the literature method s that they have worked with http www sciencecheck org although these are extremely important_contributions neither organization provides a quantitative measure of reproducibility in light of this there is an urgent need for statistical_tools for quantitatively evaluating reproducibility to help address this need we introduce the application of a well suited bayesian_hierarchical for assessing the reproducibility of validation experiments in the context of evaluating top tier predictions of high_throughput genomic studies we focus on studies in which a large number of predictions are made concerning a biological_phenomenon of interest there are many studies of this type in the recent_literature in drosophila alone predict new gene_promoters identify thousands of targets of six transcription_factors involved in regulation of the anteriorposterior axis in the embryo n gre et_al find binding_sites of six proteins associated with insulators dna_sequences that block the spread of regions of modified chromatin and interaction between other regulatory_elements andfind evidence for genes whose transcription_start are sites of polymerase ii stalling because validation of all predictions is typically infeasible often a few compelling and biologically_interesting cases are selected for further study leaving a long list of unvalidated predictions the reader is left unsure about both the fraction of the list that is valid and the effect of biological and sample_preparation variation our model takes_advantage of multiple biological and technical_replicates in each of which validation of a random_sample to whom correspondence should be addressed of the top tier list is carried_out from these data we can assess the reproducibility of the validation_studies and predict what another investigator could reasonably expect to see in a followup study the use of replicates whether technical biological or simulated has been shown to be useful in many contexts and kerr and churchill simulate microarray replicates to determine the stability of clusters of genes that exhibit similar expression_patterns in the search for differentially_expressed technical_replicates provide additional power for microarrays and biological_replicates reduce false_positives in conclusions drawn from serial_analysis data and improve accuracy in calls made from rnaseq_data use replicate time series datasets to capture time delayed associations between microbes in a larger scale take on the replicates meta_analyses of genomewide_association like those described in zeggini and ioannidis combine datasets from multiple laboratories to gain enough power to detect associations between particular genes and diseases such as type_ii and crohns_disease in some cases replicates have been incorporated into optimal study_design many articles have been written on the number of replicates required to detect a certain fold_change in gene_expression via microarray_studies for genome_wide find the required number of samples to replicate an association across studies with a certain level of between study heterogeneity andpropose multistage designs which for a given budget maximize the power to find associations auer and doerge advocate careful design of rna_seq experiments including sampling randomization replication and blocking to our knowledge no one in the biology community has used biological or technical_replicates to assess the reproducibility of validation_studies like those discussed here or has proposed a method for optimal_design of such experiments with respect to reproducibility there has been considerable work on assessing the reproducibility of high_throughput experiments especially in the context of ranked lists of putative sites in this context reproducibility is most closely_related to precision or stability in that the relevant issue is the similarity of two ranked lists generated from biological_replicates or different high_throughput platforms different ranking algorithms etc there are many different measures used to assess the similarity of two or more ranked lists from spearmans rank correlation to overlap counts for the top k sites to weighted overlap counts that emphasize correlation between high_ranking sites over that of low ranking sites improve on these measures with a mixture_model consisting of reproducible and irreproducible sites which assigns each signal a reproducibility index based on its consistency across replicates which approximates its probability of being reproducible they define the irreproducible discovery rate idr an analog of the false_discovery for multiple_hypothesis which determines the expected rate of irreproducible discoveries for sites whose probability of being irreproducible is below some threshold their methods provide a principled method for selecting sites for further study and for evaluating ranking algorithms although here we also address the issue of reproducibility our focus is different we are not concerned with the precision of high_throughput or ranking algorithms but rather with the reproducibility of independent_validation experiments that seek to verify findings of such high_throughput experiments the validation experiments taken individually give us information about the accuracy of the findings whereas our model of biological_replicates assesses the reproducibility of the given validation scheme in the face of biological and sample_preparation variation because the model we describe depends on validation of random_samples here we first review how a single simple random_sample drawn from the top tier list can be used to estimate the valid fraction of top tier predictions because this method does not account for biological and sample_preparation variability it is not sufficient to assess reproducibility as factors as seemingly benign as laboratory conditions reagent lots cell generations and individual experimenter techniques have been shown to affect results of biological_experiments so motivated we describe how our hierarchical_model uses data from multiple replicates to compute a probability_distribution of validation results for an as yet unseen replicate hierarchical_models described in many statistical textbooks including have many uses in computational genomics and are well suited to the task of assessing reproducibility as they provide a way to simultaneously model similarities and differences between groups the need for reproducibility in scientific_research has always been central but has only recently become a major focus of the greater scientific_community here we present a procedure that addresses these issues in the context of high_throughput studies like that described in our companion article where thousands of predictions are made and only a relatively small fraction can be validated we studied closely the example of adar editing sites but the method is generalizable and could just as easily apply to any of the high_throughput site prediction experiments described earlier studies of this kind have become more frequent with the advent_of technologies like illumina and large collaborations like encode modencode and the genomes_project most studies already have their own schemes for validation which they carry out with varying levels of statistical rigor the authors and any investigators hoping to carry out follow_up studies would all benefit from a carefully_designed validation study using statistically random_samples in biological_replicates to assess reproducibility as the accuracy of technology inevitably grows it may be tempting for investigators to assume a single replicate is sufficient to address reproducibility implicitly assuming that technical variation is the only variation that matters however even with perfect technology multiple biological_replicates are still necessary to assess the reproducibility of a set of results it is worth noting that each of our validation replicates was performed in a pool of flies each we expect that biological_replicates will be even more crucial in studies where replicates consist of individual model_organisms such as mice or rats in our analysis of adar mediated editing data we found that the confidence_intervals for individual replicates showed substantial variation this was not unexpected given the documented variation in adar activity in individual flies and observations on the effects of experimental_conditions described earlier and it underscored the need for statistical_tools that can address the effect of such variation on the reproducibility of results our software predicted that a new experiment using the same protocol under the same experimental_conditions would validate on average of sites and that of the time the percentage validated would be at least we emphasize the lower_bound percentile e g the th_percentile because it represents a worst_case for a future experiment of course because these results are affected by our choice of the diffuse prior on and the intervals we generate may be too conservative in the case where more is known about these parameters a priori there is a technical limitation to our model that arises from the statistical formulation the predictive distribution we describe is well defined everywhere except in the precise circumstance in which every replicate pool has a validation rate of either or we consider such extremes to be very unlikely in most validation_studies however this limitation occasionally comes to bear in experimental_design where we simulate thousands of distributions this affects almost all simulations with means and most simulations with means and with wide variances otherwise the effect is negligible our software will not return results in the few nonnegligibly affected cases our model makes two major assumptions that the validation experiments in each replicate follow a binomial_distribution and that the proportions valid in each replicate follow a beta_distribution because we require that each replicate test a random_sample drawn from the whole population of predictions the results of each replicate follow a hypergeometric_distribution which is very well approximated by a binomial_distribution as long as the number of predictions n tested in a single replicate is influence of parameters on optimal predictive distribution here we see the effect of varying the four input_parameters expected mean expected standard_deviation expected fraction of successful experiments and total number of experiments for each curve we held three parameters fixed and varied the parameter of interest for each tuple we found the maximum th_percentile over all predictive distributions for all assignments of experiments to replicates from the plot we see that the desirable higher th percentiles result from data with high mean low standard_deviation large_numbers of experiments and high fraction of successful experiments much smaller than the total number of predictions n a typical rule of thumb is n n the beta_distribution was chosen for the model primarily because as a conjugate prior to the binomial_distribution it makes computation feasible however another real advantage is that the beta_distribution is extremely flexible in that it can approximate most smooth unimodal distributions we illustrate this flexibility in the context of our model in supplementary together this suggests that the assumptions of our model will be appropriate for most applications there are three places in our model where we rely on numerical approximations during predictive inference we compute the posterior_distribution of the hyperparameters over a grid as there is no closed form expression for computing it directly then we sample from the grid to approximate the predictive distribution and in our optimal study_design we rely on a sampling approximation to find the average mean standard_deviation and th_percentile of the population of predictive distributions resulting from particular input_parameters for predictive inference we follow the procedure outlined in gelman et_al computing over a dense enough grid that we believe captures the important features of the posterior_distribution and sampling a large number of points to approximate the predictive distribution we found that neither varying the grid density nor increasing the number of samples noticeably changed the results data not shown for study_design we sample a large number of distributions is the default and if the user downloads our software he or she can increase the number of samples if desired so as to obtain narrower error_bars finally it should be noted that the results of any reproducibility analysis can only be generalized to the population to which the replicates belong just as the results of any study should only be generalized to the population from which the data are drawn we assume that follow_up validation_studies follow the original protocol under the exact experimental_conditions as the original experiments to the extent that this is not possible the credibility intervals that we report may be too narrow to accurately_reflect the population of follow_up validation_studies performed by other investigators in other laboratories therefore care must be exercised in how claims of reproducibility are made and authors should be sure to specify the population to which their results generalize in some cases large collaborations between laboratories such as those associated with modencode will be able to carry out replicates that represent a larger portion of the possible variability and will be able to make even stronger claims about the reproducibility of their findings validation and reproducibility are bedrock principles throughout science that have until recently received_limited we present this work as an aid in advancing these crucial principles in the field of genomics 
