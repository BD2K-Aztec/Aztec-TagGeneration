genome_analysis using genome query_language to uncover genetic_variation motivation with high_throughput dna sequencing_costs dropping for human_genomes data storage retrieval and analysis are the major bottlenecks in biological studies to address the large data challenges we advocate a clean separation between the evidence collection and the inference in variant_calling we define and implement a genome query_language gql that allows for the rapid collection of evidence needed for calling variants results we provide a number of cases to showcase the use of gql for complex evidence collection such as the evidence for large structural_variations specifically typical gql queries can be written in lines of high_level code and search large_datasets gb in minutes we also demonstrate its complementarity with other variant_calling tools popular variant_calling tools can achieve one order of magnitude speed up by using gql to retrieve evidence finally we show how gql can be used to query and compare multiple datasets by separating the evidence and inference for variant_calling it frees all variant detection_tools from the data intensive evidence collection and focuses on statistical_inference availability gql can be downloaded fromas sequencing_costs drop we envision a scenario where every individual is sequenced perhaps multiple times in their lifetime there is already a vast_array of genomic_information across various large_scale sequencing_projects including the genome_project genomes and the cancer_genome tcga in many of these projects a re sequencing strategy is applied in which whole genomes are sequenced redundantly with coverage between and the clone inserts bp and sequenced_reads bp are typically short and are not de_novo assembled instead they are mapped back to a standard reference to decipher the genomic_variation in the individual relative to the reference even with advances in single_molecule sequencing and genomic assembly we are many years away from having finished and error_free assembled sequences from human_donors at least in the near to mid_term we expect that the bulk of sequencing will follow the resequencing mapping variant_calling approach e g mapped_reads represented by bam_files from a single individual sequenced with coverage are relatively inexpensive to generate but they are storage intensive gb as sequencing becomes more accessible and larger numbers of individuals are sequenced the amount of information will increase rapidly this will pose a serious challenge to available community resources although raw archiving of large_datasets is possible the analysis of this huge amount of data remains a challenge to facilitate access some of the large_datasets have been moved to commercially_available cloud platforms for example the genome data are available on amazons ec genomescloud the genomes on amazon can be analyzed remotely using appropriate software frameworks like galaxy that allow for the pipelining integration of multiple analysis tools as well as tools like genome_analysis gatk and samtools the promise of this approach is that much of the analysis can be done remotely without the need for extensive infrastructure on the users part even with these developments a significant challenge_remains each individual genome is unique and the inference of variation relative to a standard reference remains_challenging in addition to small_indels and substitutions the so_called single_nucleotide or snvs an individual might have large structural changes including but not limited to insertions deletions inversions translocations of large segments bp in size incorporation of novel viral and microbial elements and recombination mediated rearrangements further many of these rearrangements may overlap leading to more complex structural_variations the detection of these variations remains_challenging even for the simplest snvs and there is little consensus on the best practices for the discovery of more complex rearrangements for large remotely located datasets it is often difficult to create a fully customized analysis it is often desirable to download the evidence reads required for the detection of variation to a local machine and experiment with a collection of analysis tools for the actual inference in that case we are back again to the problem of building a large local infrastructure including clusters and large disks at each analysis site in addition to the resources in the cloud as an example we consider the use of paired_end and mapping pem for identifying structural_variation in to whom correspondence should be addressed pem fixed_length inserts are selected for sequencing at both ends and the sequenced sub reads are mapped to a reference_genome without variation the distance and orientation of the mapped_reads match the a priori expectation however if a region is deleted in the donor relative to the reference ends of the insert spanning the deleted_region will map much further apart than expected similarly the expected orientation of the read_alignments for illumina_sequencing is a orientation is suggestive of an inversion event using pem evidence different callers still use different inference mechanisms gasv arranges overlapping discordantly mapping pair_end on the cartesian plane and draws the grid of possible break_point locations under the assumption that the discordancy is a result of a single sv breakdancer finds all areas that contain at least two discordant pair_end and it uses a poisson_model to evaluate the probability that those areas contain a sv as a function of the total number of discordant reads of each of those areas variationhunter reports that regions of sv are the ones that minimize the total number of clusters that the pair ends can form given the complexity of the data and the different inference methodologies all of these methods have significant type false_positive and type falsenegative errors further as the authors of variationhunter point out there are a number of confounding_factors for discovery for example repetitive_regions sequencing_errors could all lead to incorrect mappings at the same time incorrect calls cannot be easily detected because tools need to be modified to re examine the source data in addition the run time of the entire pipeline of those tools is not negligible given that they have to parse the raw_data a starting_point of our work is the observation all tools follow a two step procedure implicitly or explicitly for discovery of variation the first stepthe evidence stepinvolves the processing of raw_data to fetch say the discordant pair_end the second stepthe inference stepinvolves statistical_inference on the evidence to make a variant call moreover the evidence gathering step is similar and is typically the data intensive part of the procedure for example in snv discovery the evidence step is the alignment pile up of nucleotides to a specific location whereas the inference step involves snv estimation based on alignment_quality and other factors by contrast for svs such as large_deletions the evidence might be in the form of a lengthdiscordant reads and b concordant reads mapping to a region the inference might involve an analysis of the clustering of the length discordant reads and looking for copy_number decline and loss_of in concordant reads in this article we propose a genome query_language gql that allows for the efficient querying of genomic fragment data to uncover evidence for variation in the sampled genomes note that our tool does not replace other variant_calling tools but it is complementary to existing efforts it focuses on the collection of evidence that all inference tools can use to make custom inference first by providing a simple interface to extract the required evidence from the raw_data stored in the cloud gql can free callers from the need to handle large data efficiently second we show how existing_tools can be sped up and simplified using gql with larger speed ups possible through a cloud_based parallel gql implementation third analysts can examine and visualize the evidence for each variant independent of the tool used to identify the variant software layers and interfaces for genomics we also place gql in the context of other genomics software it is helpful to think of a layered hourglass model for genomic processing at the bottom is the wide instrument layer customized for each instrument for calling reads this is followed by mapping compression layers the narrow waist of the hourglass and subsequently multiple application layers some of these layers have been standardized many instruments now produce sequence_data as fastq format which in turn is mapped against a reference_genome using alignment tools such as bwa and maq further aligned_reads are often represented in the compressed bam format that also allows random_access recently more compressed alignment formats have come into vogue including slimgene cram and others as well as compression tools for unmapped reads at the highest level standards such as vcf vcf describe variants the output of the inference stage of in this context we propose additional layering specifically we advocate the splitting of the processing below the application layer to support a query into an evidence layer deterministic large data movement standardized and an inference layer probabilistic comparatively smaller data movement little agreement on techniques for evidence gathering the closest tools are samtools bamtools biohdf and gatk samtools consists of a toolkit and an api for handling mapped_reads together they comprise the first attempt to hide the implications of raw data_handling by treating datasets uniformly regardless of the instrument source samtools also provide quick random_access to large files and provide a clean api to programmatically handle alignments the tool combines index sorted bam_files with a lightweight and extremely efficient binning that clusters reads that map in neighboring locations thus samtools can quickly return a set of reads that overlap with a particular location or create a pileup i e all bases seen in reads that map to any reference locus bamtools is a c api built to support queries in a json format bedtools closely aligned with samtools allows intervalrelated queries through a clean unix and a python interface although powerful these tools still require programmer level expertise to open binary files assign buffers read_alignments and manipulate various fields the gatk is built on top of samtools and reduces the programming complexity of data_collection gatks api provides two main iterator categories to an application programmer the first iterator traverses individual_reads the second iterator walks through all genome loci either individually or in adjacent groups the toolkit which is written based on the map reduce framework and thus easily parallelizable is an excellent resource for developers of applications that need to determine local realignment quality_score re calibration and genotyping the support of many of these tools for looking at paired ends and consequently for structural_variation is limited depending in gatks case on the existence of the optional fields rnext and pnext of the sam bam alignments gatk pairend the single biggest difference between our proposed tool gql and others is that gql has a sql like declarative syntax in its own language as opposed to a procedural syntax designed to help programmers rather than the end_user declarative languages such as gql and sql not only raise the level of abstraction of data access but also allow automatic data optimization without programmer intervention by asking users to specify what data they want as opposed to how they want to retrieve it we will show that gql can facilitate automatic optimizations such as the use of indices and caching these seem harder to support in other tools without explicit programmer directives further it is feasible to compile gql queries to a distributed cloud_based back end finally gql queries allow genomes to be browsed for variations of interest allowing an interactive_exploration of the data as we show in our results although the ucsc browser also allows genome browsing it does only by position or string which we refer to as syntactic genome browsing by contrast gql allows browsing for all regions containing reads that satisfy a specified property e g discrepant reads and view the results on the ucsc browser we refer to this as semantic genome browsing and give many examples in section our performance results indicate that such browsing can be done interactively in minutes using a single cheap cpu 
