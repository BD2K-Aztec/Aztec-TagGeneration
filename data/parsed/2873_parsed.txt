merging of multi string bwts with applications motivation the throughput of genomic_sequencing has increased to the point that is overrunning the rate of downstream_analysis this along with the desire to revisit old data has led to a situation where large_quantities of raw and nearly impenetrable sequence_data are rapidly filling the hard drives of modern biology labs these datasets can be compressed via a multi string variant of the burrowswheeler_transform bwt which provides the side benefit of searches for arbitrary k_mers within the raw_data as well as the ability to reconstitute arbitrary reads as needed we propose a method for merging such datasets for both increased compression and downstream_analysis results we present a novel algorithm that merges multi string bwts in olcs n time where lcs is the length of their longest common substring between any of the inputs and n is the total length of all inputs combined number of symbols using on log f bits where f is the number of multi string bwts merged this merged multi string bwt is also shown to have a higher compressibility compared with the input multi string bwts separately additionally we explore some uses of a merged multi string bwt for bioinformatics applications availability_and the msbwt package is available through pypi with source_code located at https code google comthe throughput of next_generation ngs_technologies has increased at such a rate that it is now on the cusp of outpacing downstream computational and analysis_pipelines the result is a bottleneck where huge datasets are held on secondary storage disk while awaiting processing raw sequence e g fastq_files composed of sequence and quality strings are the most common intermediate piling up at this bottleneck moreover the rapid development of new analysis tools has led to a culture of archiving raw sequence files for reanalysis in the future this hoarding tradition reflects an entrenched notion that the costs of data generation far exceed the costs of analysis the storage overhead of this bottleneck can be somewhat alleviated through the use of compression however decompression generally requires_additional computational stages to decompress datasets before their use which further impacts the throughput of subsequent analyses this in turn has led to the need for algorithms that can operate directly on compressed data others have previously_proposed representing raw_sequencing in a form that is more compressible and indexed in a way that is suitable for direct queries by downstream tools our primary contribution is a method for merging these indexable representations of ngs raw_sequence to increase compressibility and search through all merged datasets with one query the entire collection of reads can be efficiently searched for specific k_mers and the associated reads recovered we leverage a burrowswheeler_transform bwt variant that has been adapted to string collections byfor representing raw_sequence originally the bwt was introduced as an algorithm for permuting a string to improve its compressibility the bwt of a string is closely_related to a suffix_array for the same string in fact it is merely the concatenation of the symbols preceding each suffix after those suffixes have been sorted a special end of string symbol commonly is used as the predecessor of the strings first symbol the bwt increases string compressibility because it tends to group similar substrings together which creates long runs of identical predecessor symbols the bwt was exploited bywho proposed an fm_index data_structure that allows for searches of the bwts implicit suffix_array to be performed additionally these searches were shown to run in o k time where k is the query length meaning that the bwts length does not affect the query time moreover they showed that the fm_index can be constructed on the fly in a single_pass over a strings bwt the combination of the bwt and the fm_index allows large strings to be compressed into a smaller searchable form a basic example of the bwt and the associated fm_index is shown in in bioinformatics the bwt has proven to be a useful tool for aligning short_reads the fundamental problem of short_read is to take small strings and place them along a larger string such that the edit_distance between corresponding letters is minimized the bwt is most often used to represent a reference_genome so that it can be searched for smaller substrings two prominent aligners bwa and bowtie take advantage of the bwt for alignment as sequencing and alignment rises in prominence storing billions of reads on disk has become a common problem recently several researchers have worked to apply the compression of the bwt to these large short_read sets the bwt can be trivially constructed with multiple strings by simply concatenating them with a distinguishing breaking symbol as was done by multi string bwts constructed this way generate suffixes that combine adjacent strings to whom correspondence should be addressed proposed a different multi string bwt structure where component strings were both lexicographically ordered and would cycle on themselves rather than transition to an adjacent string when repeated fm_index searches were applied both versions allow reads to be compressed and indexed to perform searches theversion was modified further byto increase the compression by modifying the order of the component strings both multi string bwt construction methods require a preprocessing of the entire uncompressed string collection before assembling the bwt in the string dataset must first be sorted on large_datasets this might require an out of core or external sorting algorithm theapproach uses heuristics to choose a string ordering that maximizes the compression benefits of the bwt this also requires an examination of the entire corpus in this article we address the problem of merging two or more multi string bwts such that the result is a multi string bwt containing the combined strings from each constituent multistring bwt additionally we require the strings in the resulting bwt to be annotated such that the origin of each string in terms of which input it came from can be identified later the reasons for merging include adding new information to an existing dataset more data from a sequencer combining different datasets for comparative_analysis and improving the compression others have addressed problems related to merging bwts and three of these are of particular interest the first is a bwt construction algorithm which incrementally constructs a bwt in blocks and then merges those blocks together the algorithm creates partial bwts in memory these partial bwts are not true independent bwts because they reference suffixes that are not included in the partial bwt they either were processed previously or will be processed later the partial bwts are then merged into a final bwt on disk by comparing the suffixes either implicitly or explicitly depending on the location of the suffixes their algorithm is primarily applicable to constructing a bwt of long strings however it could be adapted by inserting one string at a time almost as if the string set were one long string the memory overhead of the modified algorithm would require reconstituting the string collections for all but one of the inputs the one used as the starting_point and then iteratively going through each string one at a time until the merged result was constructed the second algorithm proposed by is also a bwt construct algorithm that creates a multi string bwt by incrementally inserting a symbol from each string columnwise until all symbols are added to the multi string bwt given a finished bwt they also describe how to add new strings to the bwt using this algorithm this algorithm could be adapted to solve the proposed bwt merging problem by keeping one input in the bwt format and decoding all of the other inputs into their original string collections then their construction algorithm would merge each collection into the bwt as with the first algorithm the main issue with this approach is storage overhead of decoding each bwt into its original string collection the third algorithm is a suffix_array merge algorithm proposed by sir en which computes the combined suffix_array for two inputs these suffix_arrays are actually represented as two multi string bwts the algorithm searches for the strings of one collection in the other bwt to determine a proper interleaving of the first suffix_array into the second once the interleaving is calculated the merged bwt is trivially assembled the algorithm requires an additional auxiliary index such as the fm_index to support searching the memory overhead of an unsampled fm_index is o n where n is length of the bwt sampling of the fm_index impacts search performance moreover this algorithm is ill suited to multiple datasets more than two to merge in this case the algorithm performs multiple merges until only one dataset remains our algorithm merges two or more multi string bwts directly without any search index or the need to reconstitute any string or suffix of the input bwts the only auxiliary data_structures required are two interleave arrays which identify the input source of each symbol in the final_result so the only auxiliary data_structures used by the algorithm are stored as part of the result the merging is accomplished by permuting the interleaves of the input bwts which we prove is equivalent to a radix sort over the suffixes of the string collections one motivation for merging bwts is to improve on the compression achieved by separate bwts depending on the types of data being merged the merged bwt and its associated interleave are also useful for asking certain biological_questions the most basic benefit is performing a single query in place of multiple queries to separate datasets for example the comparison algorithm proposed by performs two queries to separate bwts to find splice_junctions in their method one dataset contained dna and the other contained rna for the same sample as the sequences in each dataset are naturally similar the combined version should compress well furthermore as separate files the algorithm needs of k time to search f bwts for a given k_mer which is reduced to o k when a merged bwt is used instead in this regard the merging provides a speedup in downstream_analyses in addition to the compression bwts in general can also be applied to de_novo sequence_assembly in fact some existing assemblers use the bwt as the underlying data_structure several de_novo techniques currently use the de_bruijn as the underlying data_structure bwts can be used as efficient and compact de_bruijn representations with enhanced functionality the presence count and sample origin of individual k_mers are determined using the bwts fm_index the k_mer size can be varied without any modifications to the bwt and the surrounding context i e the containing read fragment of each k_mer is accessible a de_bruijn constructed from a merged bwt for a species would include separate paths for haplotypes thus representing a pan_genome of the merged population merged multi string datasets constructed from biological_replicates can be used to increase_statistical in de_novo and other analyses as well such datasets can also be used to examine the consistency between replicates as well as the variants between diverse samples without the overhead of aligning discuss the advantages of using replicates to help reduce errors and biases in experiments with the eightway merge of biological_replicates from the merged bwt and the corresponding interleave can be used to calculate the abundance and variance of a given k_mer for all replicates simultaneously also mention how using replicates from different platforms can be useful to reduce bias in addition to this benefit we think that combining different datasets in de_novo is useful for extending contigs for example a bwt consisting of short_reads such as illumina could be merged with long_reads such as pacbio to produce a merged bwt with the ability to query both datasets alignment is another common use for reads given a reference_genome a bwt can be used to search for evidence of the genome in the reads in this situation the counts from the query would be similar to pileup heights from an alignment regions with lower than expected counts can be reexamined by selecting reads from nearby regions and generating a consensus and thereby detecting variants including snps and indels as if we were aligning the genome to the reads instead of the reads to the genome additionally there is potential for algorithms that merge bwts from raw sequencing files with a bwt of the reference_genome ideally this would lead to a merged bwt where strings from the genome are located near similarnotes experiments are grouped into blocks each experiment compares the merged results in bold to the totals for separate files note that in all experiments there is a decrease in the number of rl entries and increase in average rl when moving from individual files to a single merged file indicating that the merged version is more compressible than separate files plot showing the distribution of rls for eight separate ff sample files higher first then lower and a merged file containing all eight samples lower first then higher note that for the merged file there are more runs of longer length and fewer runs of shorter length this is because the merged bwt has brought the similar components of each bwt together leading to longer runs strings from the sequenced read fragments making interleave the basis for alignment in some situations researchers are only interested in a specific local effect instead of global analysis a classic example is designing primers for targeted_sequencing both blast and blat search for k_mers within a database of strings allowing for small errors similar algorithms could be executed using the merged multi dataset bwt as the database of strings this would allow for queries for k_mer evidence among all of the datasets in a merged bwt simultaneously thus replacing raw sequencing files i e fasta with bwts has several advantages beyond improving compression the indexing capabilities of the bwt increase the inherent utility of the data by allowing it to be searched and quantified furthermore the interleave vector generated by merging bwts enables finding both sequence_similarities and differences between datasets without needing to align finally as the bwts are purely data_driven they are unaffected by new genome builds 
