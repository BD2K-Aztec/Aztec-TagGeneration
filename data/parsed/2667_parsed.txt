genome_analysis large_scale compression of genomic sequence_databases with the_burrows motivation the burrowswheeler_transform bwt is the foundation of many algorithms for compression and indexing of text data but the cost of computing the bwt of very large string collections has prevented these techniques from being widely_applied to the large sets of sequences often encountered as the outcome of dna sequencing_experiments in previous work we presented a novel algorithm that allows the bwt of human genome_scale to be computed on very moderate hardware thus enabling us to investigate the bwt as a tool for the compression of such datasets results we first used simulated reads to explore the relationship between the level of compression and the error_rate the length of the reads and the level of sampling of the underlying genome and compare choices of second stage compression algorithm we demonstrate that compression may be greatly_improved by a particular reordering of the sequences in the collection and give a novel implicit sorting strategy that enables these benefits to be realized without the overhead of sorting the reads with these techniques a coverage of real human_genome sequence_data compresses losslessly to under bits_per allowing the gb of sequence to fit into only gb of space trimming a small proportion of low quality bases from the reads improves the compression still further this is times smaller than the size achieved by a standard bwt based compressor bzip on the untrimmed reads but an important further advantage of our approach is that it facilitates the building of compressed full text indexes such as the fm_index on large_scale dna_sequence collections availability code to construct the bwt and sap array on large genomic_datasets is part of the beetl library available as a github repository at https github com beetl beetl in this article we present strategies for the lossless_compression of the large number of short dna_sequences that comprise the raw_data of a typical sequencing_experiment much of the early work on the compression of dna_sequences was motivated by the notion that the compressibility of a dna_sequence could serve as a measure of its information content and to whom correspondence should be addressed hence as a tool for sequence_analysis this concept was applied to topics such as feature_detection in genomes and alignment_free of sequence comparison a comprehensive_review of the field up to is given by however grumbach and tahi in have been echoed by many subsequent authors in citing the exponential_growth in the size of nucleotide sequence_databases as a reason to be interested in compression for its own sake the recent and rapid evolution of dna_sequencing has given the topic more practical relevance than ever the outcome of a sequencing_experiment typically comprises a large number of short sequencesoften called readsplus metadata associated with each read and a quality_score that estimates the confidence of each base and deorowicz and grabowski both describe methods for compressing the fastq file_format in which such data are often stored the metadata is usually highly redundant whereas the quality_scores can be hard to compress and these two factors combine to make it hard to estimate the degree of compression achieved for the sequences themselves however both schemes employ judicious combinations of standard text compression_methods such as huffman and lempel ziv with which it is hard to improve substantially upon the naive method of using a different bit code for each of the four nucleotide_bases for example gencompress obtains bits_per henceforth bpb compression on the escherichia coli_genome an experimenter wishing to sequence a diploid_genome such as a human might aim for fold average coverage or more with the intention of ensuring a high_probability of capturing both alleles of any heterozygous variation this oversampling creates an opportunity for compression that is additional to any redundancy inherent in the sample being sequenced however in a wholegenome shotgun experiment the multiple copies of each locus are randomly dispersed among the many millions of reads in the dataset making this redundancy inaccessible to any compression method that relies on comparison with a small buffer of recently seen data this can be addressed by reference based compression which saves space by sorting aligned_reads by the position they align to on a reference_sequence and expressing their sequences as compact encodings of the differences between the reads and the reference however this is fundamentally a lossy strategy that achieves best compression by retaining only reads that closely match the reference limiting the scope for future reanalyses such as realignment to a refined referencereads simulated from the e coli_genome k strain allowed us to assess separately the effects of coverage read_length and sequencingwe compared different compression_schemes both on the raw input_sequences and the bwt rlo denotes a reverse lexicographical ordering of the reads sap is the dataset where all the reads are ordered according to the same asprevious array the x axis gives the coverage level whereas the y axis shows the number of bits used per input symbol gzip bzip ppmd default and ppmd large show compression achieved on the raw_sequence bwt bwt sap and bwt rlo give compression results on the bwt using ppmd default as second stage compressor error on the level of compression achieved first a coverage of error_free base reads was subsampled into datasets as small as shows a summary plot of the compression_ratios at various coverage levels for compression both on the original reads and the bwt transform we found the ppmd mode m ppmd of zip to be a good choice of second stage compressor for the bwt strings referred to asin the following rlo sorting the datasets led to a bwt that was slightly more compressible than the sap permuted bwt but the difference was small bpb versus bpb at both over less than the bpb taken up by the compressed bwt of the unsorted reads in contrast when gzip http www gzip org jean loup gailly and mark adler bzip and default ppmd were applied to the original reads each gave a compression level that was consistent across all levels of coverage and none was able to compress bits_per however a sweep of the ppmd parameter_space yielded a combination mo mmem m that attained bpb on the dataset in the following we will refer to this parameter setting as this is because the e coli_genome is small enough to permit several fold redundancy of the genome to be captured in the gb of working space that this combination specifies for a much larger genome such as human this advantage disappears of the reads less well than ppmd default as well as being several times slower we also investigated the effects of sequencing_errors on the compression_ratios by simulating datasets of bp reads with different rates of uniformly_distributed substitution error finding that an error_rate of approximately doubled the size of the compressed bwt bpb compared with bpb for error_free data at the same coverage we were interested in the behaviour of bwt based compression techniques as a function of the read_length to this end we fixed a coverage of and simulated error_free e coli reads of varying lengths as the read_length increased from bp to bp the size of the compressed bwts shrank from bpb to bpb this is not surprising since longer_reads allow repetitive_sequences to be grouped together which could otherwise potentially be disrupted by suffixes of homologous_sequences finally we assessed the performance of the compressors on a typical human_genome resequencing experiment available at http www ebi ac uk ena data view era containing gb of base reads or coverage of the human_genome in addition to this set of reads we created a second dataset by trimming the reads based on their associated quality_scores according to the scheme described in bwa setting a quality_score of as the threshold removes of the input bases we again constructed the corresponding datasets in rlo and sap order shows the improvement in terms of compression after the trimming eliminating of the bases improves the compression_ratio by or compressing the entire gb down to gb analyzing the srx dataset allowed us to compare our results with recoil but the reads are noisier and shorter than more recent datasets and at under the oversampling of the genome is too 
