transformations for the compression of fastq quality_scores of next_generation the growth of next_generation means that more effective and efficient archiving methods are needed to store the generated data for public dissemination and in anticipation of more mature analytical_methods later this article examines methods for compressing the quality_score component of the data to partly address this problem results we compare several compression policies for quality_scores in terms of both compression effectiveness and overall efficiency the policies employ lossy and lossless transformations with one of several coding schemes experiments show that both lossy and lossless transformations are useful and that simple coding methods which consume less computing_resources are highly_competitive especially when random_access to reads is needed next_generation ngs offers new directions in genome science by allowing entire genomes to be sequenced at lower_costs given its rapid_growth the problem of economically storing and quickly restoring sequencing_data is becoming a concern for both researchers and operators of data centers the most notable examples of data_repositories is the sequence_read archives sra by the international nucleotide_sequence database collaboration insdc at ncbi ebi and ddbj which helps in disseminating publicly_funded data while the storing of raw_data is infeasible their hope is to be able to store at least the bases and their corresponding quality_scores on the other hand as the amount of data continues to rise it is conceivable that the burden of storing such data will gradually shift to research_laboratories hospitals or even individuals effective means of storing sequencing_data will be needed in these cases as well even though the resources available will differ greatly to whom correspondence should be addressed the above trends suggest that economical representation of sequencing_data is important in response compression of dna_sequences has been an active research_topic for many years in contrast relatively little attention has been devoted to quality_scores the set of quality_scores is far larger than the four dna nucleotides this has the potential to make the problem more difficult than sequence compression whenever the economical representation of quality_scores was taken into account the scores were considered together with all other components hence it is not clear how well the quality_scores component can be compressed note that the little attention given to quality_scores does not mean that they are useless in fact they are slowly becoming a necessary part of many data analyses they can be used to trim reads at either end see the galaxy tool for an example or be used for read_mapping as demonstrated by the maq software other future applications may also be possible in this article we address the issue of economical representation of quality_scores as a stand alone component this separation allows us to focus on the topic at hand and does not limit us from combining our results with other works that are devoted to compressing dna_sequences alone to have a clear understanding and to supply different levels of economy trade_off we break the process of economical representation into three independent and optional components lossy transformation lossless transformation and coding or compression rather than advocating a single method for each component we will explore various options this section reports the main experiment results for the dataset srr additional results including those for the dataset srr are provided in the supplementary_material all experiments were conducted on a set of ghz core intel_xeon e with gb of ram and hyper threading demonstrates the compression effectiveness as compression_ratios achieved by the described lossless transformations with various block sizes the most noticeable feature in the figure is the near immobility of the by symbol curves and the variability of the by value curves across the four graphs as anticipated in general the lossless transformations improve compression_ratio for the by value methods and have almost no effects on the by symbol compression_schemes moreover all the by symbol compression_schemes are not effective when the block size k is small the effect of lossless transformation on the by value compression_schemes is tremendous in fact these compression_schemes are unsuitable for the original q scores as shown in it is understandable as the minimal q scores of is already a large value for these methods which reserve short code words to small values while we can take off from all q scores before encoding to improve compression effectiveness that simple operation is not considered here because it cannot do better than the minshifting transformation and c clearly show the difference in performance between minshifting and freqordering the former has a very low_cost in terms of block headers and performs well when k but gets worse when k grows since the impact of the minimal value lessens on the contrary while freqordering spends more for the block headers the absolute cost is stabilized when k is large enough making freqordering the clear winner it should page be noted however that the combination k minshifting binary is the simplest fromand c but achieves a very impressive compression_ratio this combination can be a good choice when random_access is a need finally shows that gaptranslating is the best overall lossless transformation for the by value coding schemes with the only exception of binary it is interesting to see that the compression_ratio is almost stable across different k moreover the compression_ratio attained by interp is very close to that achieved by the by symbol compression_schemes with large k the solid black lines indicate the zero order self information for each transformed dataseta limit that can be improved upon through block creation or more complex models based on we will further consider four representative coding schemes gamma interp huffman and libbzip effects of lossy transformations compression_ratio achieved with different levels of granularity of the lossy transformation logbinning for the dataset srr is shown in as one would expect the more stringent the lossy transformation the better the compression_ratio with k the combination of gaptranslating interp is the clear winner with k or although libbzip with no lossless transformation is the best interp and gamma with gaptranslating can achieve fairly close compression_ratios in general when a relative mapping accuracy of is acceptable the lossy transformation logbinning with can be applied at this setting various combinations of lossless transformations and coding methods can achieve compression times averaged over three trials for srr lossless transformation gaptranslating and no lossy transformation for a k and b k and with no lossless transformation and lossy transformation logbinning at for c k and d k ratios of around bit per quality scorea dramatic progress in comparison to the level of bits per quality_score achieved in the absence of any lossy transformation in this work we have considered how to economically represent quality_scores in ngs_data as a combination of three main components lossy transformation lossless transformation and coding of them the first two are optional but they can considerably affect the whole process by separating these components we were able to see the effects each of them can bring and also to identify the best settings in order to achieve good system performance in our study we proposed three lossy transformations introduced or made use of three lossless transformations investigated several by value coding schemes and considered more conventional bysymbol methods moreover we demonstrated how data blocking can affect both compression_ratios and running_times finally we also proposed a method to assess the usability or worthiness of any lossy transformation by employing the read_mapping tool maq we found that while full fledged compression systems such as libbzip are widely_accepted for economical representation of ngs they are not the best choice for quality_scores here simple codes such as the static code gamma and parameterized codes interp and golomb when accompanied by the lossless transformation gaptranslating are highly_competitive they can achieve_similar levels of compression while using less time in particular unlike their counterparts that are effective only with large block sizes the simple codes have the distinguished feature of being unaffected by this factor this is an important point because it essentially removes the block size parameter from the process and allows simple codes to greatly outperform their counterparts when small block sizes are in use note that small block sizes offer a number of advantages which we have not explored first peak memory_usage is reduced second since blocks are coded independently errors in the compressed data stream can be isolated easily third random_access in compressed data can be supported at the additional low_cost of an index before each block finally independent blocks mean that our findings would apply to higher_coverage datasets as expected for the lossy transformations compression_ratios positively correlate with the number of distinct quality_scores of the three proposed transformations logbinning is the most effectiveit achieves excellent relative mapping performance even when employing only a few distinct quality_scores with this choice compression_ratio is improved by around six times while processing time when coupled with by symbol coding schemes is reduced significantlyall suggesting that lossy transformations are useful our results for srr are supported by those of srr which appear in the supplementary_material even though the reads are longer the relative performance of the transformation and compression_methods remain the same our view is that as ngs_data continues to grow lossy and lossless transformations can work in tandem for example ngs_data can be compressed losslessly and kept in off line storage such as tape backup while lossy versions of the data can be shared between users and research_laboratories for daily use employing lossy transformations requires consideration since such changes are more easily noticeable and difficult to assess compared with images and video_data our evaluation with maq is meant to address_this in the future we plan to reorder the reads as was done byfor the sequence bases so that each block possesses reads that have similar quality_score patterns with respect to running time we intend to parallelize some of our methods across blocks furthermore the effect of lossy transformations on other applications of ngs_data such as rna_seq and snp_calling also needs to be evaluated our implementation dubbed qscores archiver is available from http www cb k u tokyo ac jp asailab members rwan under the lesser general_public version or later to allow users to combine it with their fastq compression systems 
