data_and bayesian rule learning for biomedical data_mining motivation disease_state prediction from biomarker profiling studies is an important problem because more accurate classification_models will potentially lead to the discovery of better more discriminative markers data_mining methods are routinely applied to such analyses of biomedical datasets generated from high_throughput omic technologies applied to clinical samples from tissues or bodily fluids past work has demonstrated that rule models can be successfully_applied to this problem since they can produce understandable models that facilitate review of discriminative biomarkers by biomedical_scientists while many rule based_methods produce rules that make predictions under uncertainty they typically do not quantify the uncertainty in the validity of the rule itself this article_describes an approach that uses a bayesian score to evaluate rule models results we have combined the expressiveness of rules with the mathematical rigor of bayesian_networks bns to develop and evaluate a bayesian rule learning brl system this system utilizes a novel variant of the k algorithm for building bns from the training data to provide probabilistic scores for if antecedent then consequent rules using heuristic best first search we then apply rule based_inference to evaluate the learned models during fold_cross performed two times the brl system is evaluated on published omic datasets and on average it performs on par or better than other readily available rule learning methods moreover brl produces models that contain on average fewer variables which means that the biomarker panels for disease prediction contain fewer markers for further verification and validation by bench scientists high_throughput omic_data that measure biomarkers in bodily fluids or tissues are accumulating at a rapid pace and such data have the potential for the discovery of biomarkers for early_diagnosis monitoring and treatment of diseases such as cancer data_mining methods that learn models from high_dimensional are being increasingly used for the multivariate_analyses of such biomedical datasets together with statistical univariate_analyses some insights into predictive_biomarkers of disease_states can be gleaned though to whom correspondence should be addressed the results may not generalize due to the small sizes of available training data typically less than samples due to the large imbalance between variable dimensionality several thousand and the sample_size a few hundred there is a need for data_mining methods that can discover significant and robust biomarkers from high_dimensional rule learning is a useful data_mining technique for the discovery of biomarkers from highdimensional biomedical data we have previously_developed and applied rule learning methods to analyze omic_data successfully rules have several advantages including that they are easy for humans to interpret represent knowledge modularly and can be applied using tractable inference procedures in this article we develop and evaluate a novel probabilistic_method for learning rules called the bayesian rule learning brl algorithm this algorithm learns a particular form of a bayesian_network bn from data that optimizes a bayesian score and then translates the bn into a set of probabilistic rules the use of the bayesian_approach allows prior_knowledge as probabilities to be incorporated into the learning_process in a mathematically coherent fashion the possibility of over fitting is attenuated by the incorporation of prior probabilities into the rule discovery process brl outputs the predictive rule model with the best bayesian score which represents the probability that the model is valid given the data the remainder of the article is organized as follows section_presents the brl algorithm and briefly_reviews other popular rule learning methods section_describes the datasets and the experimental_setup to evaluate brl section_presents the results of applying brl to published omic datasets and compares its performance with multiple rule learning algorithms section_presents our conclusions the average baccs obtained from fold_cross performed two times for each of the datasets are shown inaverages over the genomic_datasets ga and their sds as well as averages over the proteomic_datasets pa and their sds bold numbers indicate highest performance on a dataset as can be seen from the average baccs for the datasets both brl and brl clearly perform better than the other rule learning methods this holds for both the genomic_datasets and the proteomic_datasets we see that brl has the highest bacc on datasets while brl has the highest bacc on datasets on the remaining five datasets c has the highest bacc on three ties with brl on one and conjunctive rule learner has the highest on one only the first dataset is very easy to classify by all rule learners as seen in the performance of both brl and brl are statistically_significantly better than c its nearest competitor in terms of bacc when compared with each other brl outperforms brl the average rcis obtained by the various rule learning methods are shown in brl has the highest rci on datasets whereas brl has the highest rci on datasets there was one tie among the two brl methods and c in addition c has the highest rci on one dataset in we compare the difference in performance using the rci measure between c with brl and brl and both brl methods are statistically_significantly better than c the difference in performance using the rci measuredepicts a comparison of the average number of variables markers appearing in the rule models for c brl and brl when run with default parameter_settings the average was calculated over the models generated from folds obtained from stratified fold_cross repeated two times on the datasets as shown the brl models have less variables on average in their models than c if each predictor variable has only two discretized ranges of values then brl with default_parameters would generate between and rules on average however discretization could yield a larger number of value ranges for a variable thereby increasing the number of rules generated by brl to reduce the number of rules we can prune rules with zero coverage that is those rules whose left_hand side does not match any of the samples in the training data we notice that pruning does not harm brls performance however rule pruning could cause problems during testing since rules that do not match training data could still match test data we include an example of pruned rules and also c rules in the supplementary_material the variables chosen in brls predictive_models are often different from those chosen by c there are several advantages that accrue from brl that are not available in current rule learning algorithms brl allows for the evaluation of the entire rule set using a bayesian score using such a score results in a whole model evaluation instead of a per rule or local evaluation which often occurs with ripper and c the bayesian score allows us to capture the uncertainty about the validity of a rule set brl currently uses this score only for model_selection however the score could be utilized in extensions to brl for performing inference when rule sets can be weighted by this score which would be a form of bayesian_model averaging a bayesian_approach allows incorporation of both structure and parameter priors when training data are scarce such as in omic_data analysis it is useful to incorporate_prior to improve the accuracy of learned models for example a scientist could define all of the variable relationships using either a knowledge_base or restrict the possible variables with which to build the model in a bayesian_approach a scientist might provide prior_knowledge specifying conditional independencies among variables constraining or even fully specifying the network_structure of the bn in addition to providing such structure priors the scientist might also specify knowledge in the form of prior distributions over the parameter_values of the model structure priors are arguably the most useful however because in our experience scientists are often more confident about structural_relationships than about parameter_values we have not explored informative priors in this article we used uniform parameter and uniform structure priors exploring informative structure priors in this domain is a direction for future_research there are different ways of representing non informativeness of parameters using the dirichlet priors we have explored one approach it would be useful to explore other approaches as well an interesting open_problem is to investigate methods for brl rule ordering and pruning within a set of rules for example pruning a set of brl rules based on using local structure and scores would be worth investigating a major advantage of brl is that it can find models with fewer variables markers that have equivalent or greater classification_performance than those obtained from several other rule learning methods fewer variables mean fewer markers for biological verification and subsequent validation this is important in biomarker_discovery and validation_studies that have to be designed carefully and under tight resource constraints 
