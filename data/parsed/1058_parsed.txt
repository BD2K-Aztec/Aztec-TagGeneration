compacting de_bruijn from sequencing_data quickly and in low memory motivation as the quantity of data per sequencing_experiment increases the challenges of fragment assembly are becoming increasingly computational the de_bruijn is a widely used data_structure in fragment assembly algorithms used to represent the information from a set of reads compaction is an important data reduction step in most de_bruijn based_algorithms where long simple paths are compacted into single vertices compaction has recently become the bottleneck in assembly pipelines and improving its running time and memory_usage is an important problem results we present an algorithm and a tool bcalm for the compaction of de_bruijn bcalm is a parallel algorithm that distributes the input based on a minimizer hashing technique allowing for good balance of memory_usage throughout its execution for human sequencing_data bcalm reduces the computational_burden of compacting the de_bruijn to roughly an hour and gb of memory we also applied bcalm to the gbp loblolly pine and gbp white_spruce sequenc ing datasets compacted graphs were constructed from raw_reads in less than days and gb of memory on a single machine hence bcalm is at least an order of magnitude more efficient than other available methods availability_and source_code of bcalm is freely_available at https github com gatb bcalm contact rayan chikhi univ lille frmodern sequencing_technology can generate billions of reads from a sample whether it is rna genomic_dna or a metagenome in some applications a reference_genome can allow for the mapping of these reads however in many others the goal is to reconstruct long contigs this problem is known as fragment assembly and continues to be one of the most important challenges in bioinformatics fragment assembly is the central algorithmic component behind the assembly of novel genomes detection of gene_transcripts rna_seq species discovery from metagenomes structural variant_calling continued improvement to sequencing_technologies and increases to the quantity of data produced per experiment present a serious challenge to fragment assembly algorithms for instance while there exist many genome_assemblers that can assemble bacterial sized genomes the number of assemblers that can assemble a high_quality mammalian_genome is limited with most of them developed by large teams and requiring extensive resources for even larger genomes such as the gbp picea glauca white_spruce graph construction and compaction took tb of memory h and cpu cores in another instance the whole genome_assembly of gbp pinus taeda loblolly pine required gb of memory and three months of running time on a single machine most short_read fragment assembly algorithms use the de_bruijn to represent the information from a set of reads given a set of reads r every distinct k_mer in r forms a vertex of the graph while an edge connects two k_mers if they overlap by k characters the use of the de_bruijn in fragment assembly consists of a multi_step pipeline however the most data intensive steps are usually the first three nodes enumeration compaction and graph cleaning in the first step sometimes called k_mer the set of distinct k_mers is extracted from the reads in the second step all unitigs paths with all but the first vertex having in degree and all but the last vertex having out degree are compacted into a single vertex in the third step artifacts due to sequencing_errors and polymorphism are removed from the graph the second and third step are sometimes alternated to further compact the graph after these initial steps the size of the data is reduced gradually e g for a human dataset with coverage to overcome the scalability challenges of fragment assembly of large sequencing_datasets there has been a focus on improving the resource_utilization of de_bruijn construction in particular k_mer has seen orders_of improvements in memory_usage and speed as a result graph compaction is becoming the new bottleneck but it has received_little recently we developed a compaction tool that uses low memory but without an improvement in time other parallel approaches for compaction have been proposed as part of genome_assemblers however most are only implemented within the context of a specific assembler and cannot be used as modules for the construction of other fragment assemblers or for other applications of de_bruijn e g metagenomics in this paper we present a fast and low memory algorithm for graph compaction our algorithm consists of three stages careful distribution of input k_mers into buckets parallel compaction of the buckets and a parallel reunification step to glue together the compacted strings into unitigs the algorithm builds upon the use of minimizers to partition the graph however the partitioning strategy is completely novel since the strategy ofdoes not lend itself to parallelization due to the algorithms complexity we formally prove its correctness we then evaluate it on whole_genome human pine and spruce sequencing_data the de_bruijn for a whole human_genome dataset is compacted in roughly an hour and gb of memory using cores for the gbp pine and spruce genomes k_mer and graph compaction take only days and gb of memory improving on previously_published by at least an order of magnitude in this paper we present bcalm an open_source parallel and lowmemory tool for the compaction of de_bruijn bcalm constructed the compacted de_bruijn of a human_genome dataset in mins and gb of memory furthermore k_mer and graph compaction using bcalm of the gbp white_spruce and the gbp loblolly pine sequencing_datasets required only days and gb of memory each bcalm is different from previous_approaches in several regards first it is a separate module for compaction with the goal that it can be used as part of any other tools that build the de_bruijn while parallel genome_assemblers offer impressive performance there are many situations where differences in data require the development of a new assembler and hence it is desirable to buildfor bcalm and bcalm we used k and and respectively abundance cutoffs were set to for chr and for whole human we used cores for the parallel algorithms abyss meraculous and bcalm meraculous aborted with a validation failure due to insufficient peak k_mer depth when we ran it with abundance cutoffs of we were able to execute it on chromosome with a cutoff of but not for the whole_genome for the whole_genome we show the running_times given in the exact memory_usage was unreported there but is less than tb meraculous was executed with prefix blocks the k_mer size was and the abundance cutoff for k_mer was 
