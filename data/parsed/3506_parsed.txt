data_and permutation importance a corrected feature importance measure motivation in life_sciences interpretability of machine_learning is as important as their prediction_accuracy linear_models are probably the most frequently used methods for assessing feature relevance despite their relative inflexibility however in the past_years effective estimators of feature relevance have been derived for highly complex or non parametric_models such as support_vector and randomforest rf models recently it has been observed that rf models are biased in such a way that categorical_variables with a large number of categories are preferred results in this work we introduce a heuristic for normalizing feature importance measures that can correct the feature importance bias the method is based on repeated permutations of the outcome vector for estimating the distribution of measured importance for each variable in a non informative setting the p value of the observed importance provides a corrected measure of feature importance we apply our method to simulated_data and demonstrate that i non informative predictors do not receive significant p values ii informative variables can successfully be recovered among non informative variables and iii p values computed with permutation importance pimp are very helpful for deciding the significance of variables and therefore improve model interpretability furthermore pimp was used to correct rf based importance measures for two real_world case studies we propose an improved rf model that uses the significant_variables with respect to the pimp measure and show that its prediction_accuracy is superior to that of other existing_models availability r code for the method presented in this article is available atin recent_years statistical_learning has gained_increased in a large number of research_fields there exist two main goals for the application of statistical_learning either the generation of a possibly black_box model that predicts a variable of interest given a number of putatively predictive features or the generation of insight into how the predictive features impact on the variable of interest given that the prediction model performs reasonably well this latter task of feature discovery or feature ranking is the essence of biomarker_discovery in bioinformatics and life_sciences for instance unfortunately not all statistical_learning methods can be used for identifying interesting_features because their underlying methods are too complex to analyze contributions of single covariates to the overall results this problem applies for instance to artificial_neural and support_vector svms with nontrivial kernels however in the case of svms recently approaches to interpreting models that apply sequence kernels were presented in life_sciences the most frequently_applied methods for quantifying feature importance are linear_models and decision_trees linear svm and linear logistic_regression are well studied theoretical_models that can provide interpretable classification rules via model_parameters moreover in difficult situations when the number of predictors exceeds greatly the number of available samples regularizers such as the lasso penalty can be used for obtaining sparse models however linear classifiers fail to discover complex dependencies in the training data this is clearly a drawback when biological data are analyzed since biological_processes usually involve intricate interactions decision_trees are suitable for finding non linear prediction rules that are also interpretable although their instability and lack of smoothness have been a cause of concern the randomforest rf classifier was designed to overcome these problems and recently became very popular because it combines the interpretability of decision_trees with the performance of modern learning algorithms such as artificial_neural and svms the author of rf proposes two measures for feature ranking the variable_importance vi and gini importance gi a recent study showed that if predictors are categorical both measures are biased in favor of variables taking more categories the authors of the article ascribe the bias to the use of bootstrap sampling and gini split criterion for training classification and regression trees cart in the literature the bias induced by the gini_coefficient has been reported for years and it affects not only categorical_variables but also grouped variables i e values of the variable cluster into well separated groupse g multimodal gaussian_distributions in general in biology predictors often have categorical or grouped values e g microarrays and sequence mutations propose a new algorithm cforest for building rf modelspage in this work we proposed an algorithm for correcting for two biased measures of feature importance the method permutes the response vector for estimating the random importance of a feature under the assumption that the random importance of a feature follows some distribution gaussian lognormal or gamma the likelihood of the measured importance on the unpermuted outcome vector can be assessed the resulting p value can serve as a corrected measure of variable relevance we showed how this method can successfully adjust the feature importance computed with the classical rf algorithm or with the mi measure we also introduced an improved rf model that is computed based on the most significant features determined with the pimp algorithm simulation a demonstrated that the gi of the rf and mi favor features with large number of categories and showed how our algorithm alleviates the bias simulation b demonstrated the usefulness of the algorithm for generating a correct feature ranking for all methods the feature ranking based on the unprocessed importance measures could be improved when feature importances of rf are distributed among correlated features our method assigns significant scores to all the covariates in the correlated group even for very large group size this improves model interpretability in applications such as microarray_data classification where groups of functionally_related are highly_correlated pimp was used to correct for rf based gi measures for two realworld datasets both case studies use features based on nucleotide or amino_acid as already discussed bycategorical features e g nucleotide_sequences are often used together with derived continuous features e g free fold energy for improving the prediction model in this case it may happen that the continuous variables are preferred by tree based_classifiers as they provide more meaningful cut_points for decisions pimp on the c to u dataset demonstrated successful post_processing page 
