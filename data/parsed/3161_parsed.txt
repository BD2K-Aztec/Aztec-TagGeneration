data_and multiple rule bias in the comparison of classification rules motivation there is growing discussion in the bioinformatics community concerning overoptimism of reported results two approaches contributing to overoptimism in classification are i the reporting of results on datasets for which a proposed classification rule performs well and ii the comparison of multiple classification rules on a single dataset that purports to show the advantage of a certain rule results this article provides a careful probabilistic analysis of the second issue and the multiple rule bias resulting from choosing a classification rule having minimum estimated error on the dataset it quantifies this bias corresponding to estimating the expected true error of the classification rule possessing minimum estimated error and it characterizes the bias from estimating the true comparative advantage of the chosen classification rule relative to the others by the estimated comparative advantage on the dataset the analysis is applied to both synthetic and real_data using a number of classification rules and error estimators availability we have implemented in c code the synthetic data distribution model classification rules feature_selection routines and error estimation methods the code for multiple rule analysis is implemented in matlab the source_code is available atthree recent_articles in bioiformatics have lamented the difficulty in establishing performance advantages for proposed classification rules two statistically grounded sources of overoptimism have been highlighted one considers applying a classification rule to numerous datasets and then reporting only the results on the dataset for which the designed classifier possesses the lowest estimated error the optimistic bias from this kind of dataset picking is quantitatively analyzed in where it is termed to whom correspondence should be addressed reporting bias and where this bias is characterized as a function of the number of considered datasets a second kind of overoptimism concerns the comparison of a collection of classification rules by applying the classification rules to a dataset and comparing them according to the estimated errors of the designed classifiers this kind of bias which we will call multiple rule bias has been considered in boulesteix and strobl by applying a battery of classification rules to colon_cancer and prostate_cancer datasets and then examining the effects of choosing classification rules having minimum cross_validation error estimates whereas the thrust ofis to compare the sources of multiple rule bias in classification rules namely gene_selection parameter selection and classifier function construction our interest is in studying multiple rule bias as a function of the number of rules being considered in particular we are interested in the joint_distribution as a function of the number of compared rules between the minimum estimated error among a collection of classification rules and the true error for the classification rule having minimum estimated error as well as certain moments associated with this joint_distribution although different with regard to distributional specifics this approach is analogous to the approach taken in where the joint_distribution involved the minimum estimated error of the designed classifier over a collection of datasets and the true error of the designed classifier on the population corresponding to the dataset resulting in minimum estimated error this is a natural way to proceed because any bias ultimately results from inaccuracy in error estimation so that the behavior of the joint_distribution of interest and its moments are consequent to the joint_distribution of the error estimator and the true error owing to the methodology in it would have been impossible for them to study this joint_distribution because they never concern themselves with true errors only crossvalidation estimates hence when they compare a minimal error to a baseline error to arrive at a measure of optimistic bias they are comparing cross_validation estimates in characterizing multiple rule bias we begin with a more general framework than the one just described rather than simply considering multiple classification rules we consider multiple classifier rule models so that there are not only multiple classification rules but also multiple error estimation rules being employed we define a classifier rule model as a pair where is a classification rule including feature_selection if feature_selection is employed and is an error estimation rule the scenario in the preceding paragraph results where there is only a single error estimation rule page 
