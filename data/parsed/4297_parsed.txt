data_and smash a benchmarking toolkit for human_genome variant_calling motivation computational_methods are essential to extract action able information from raw_sequencing and to thus fulfill the promise of next_generation unfortunately computational_tools developed to call variants from human sequen_cing data disagree on many of their predictions and current methods to evaluate accuracy and computational performance are ad_hoc and incomplete agreement on benchmarking variant_calling methods would stimulate development of genomic processing_tools and facilitate communication among researchers results we propose smash a benchmarking methodology for evaluating germline variant_calling we generate synthetic datasets organize and interpret a wide_range of existing benchmark ing data for real genomes and propose a set of accuracy and computational performance_metrics for evaluating variant_calling methods on these benchmarking_data moreover we illustrate the utility of smash to evaluate the performance of some leading single_nucleotide indel and structural variant_calling availability_and we provide free and open_access online to the smash tool kit along with detailed documentation atnext generation sequencing is revolutionizing biological and clinical research long hampered by the difficulty and expense of obtaining genomic_data life_scientists now face the opposite problem faster cheaper technologies are beginning to generate massive_amounts of new sequencing_data that are overwhelming our technological capacity to conduct genomic_analyses computational processing will soon become the bottleneck in genome_sequencing research and as a result computational_biologists are actively developing new tools to more efficiently and accurately process human_genomes and call variants e g samtools gatk platypus http www well ox ac uk platypus breakdancer pindel and dindel unfortunately single_nucleotide snp callers disagree as much as of the time and there is even less consensus in the outputs of structural variant algorithms moreover reproducibility interpretability and ease of setup and use of existing software are pressing issues currently hindering clinical adoption indeed reliable benchmarks are required to measure accuracy computational performance and software robustness and thereby improve them in an ideal world benchmarking_data to evaluate variant_calling would consist of several fully_sequenced perfectly known human_genomes however ideal validation_data do not exist in practice technical_limitations such as the difficulty in accurately sequencing low complexity regions along with budget constraints such as the cost to generate high coverage sanger reads limit the quality and scope of validation_data nonetheless significant resources have already been devoted to generate subsets of benchmarking_data that are substantial enough to drive algorithmic innovation alas the existing data are not curated thus making it extremely_difficult to access interpret and ultimately use for benchmarking purposes owing to the lack of curated ground_truth data current benchmarking efforts with sequenced human_genomes are lacking the majority of benchmarking today relies on either simulated_data or a limited set of validation_data associated with real_world datasets simulated_data are valuable but do not tell the full story as variant_calling is often substantially easier using synthetic reads generated via simple generative_models sampled data as mentioned earlier are not well curated resulting in benchmarking efforts such as the genome in a bottle consortium and the comparison and analytic testing resource gcat http www bioplanet com gcat that rely on a single dataset with a limited quantity of validation_data rigorously evaluating predictions against a validation dataset presents several additional challenges consensus based evaluation approaches used in various benchmarking efforts may be misleading indeed to whom correspondence should be addressed in this section we first evaluate the impact of our ambiguity resolution algorithms we next illustrate the utility of smash by evaluating the performance of some leading snp indel and structural variant_calling the goal of these experiments is to highlight smashs functionality and to simplify the discussion we use the default_settings for all variant_calling and the same machine instance in all of our amazon elastic compute cloud ec experiments we note that improved accuracy and computational performance results may indeed be possible via parameter_tuning and optimizing to minimize ec instance footprints in all reported results evaluation is reported as described in section see section g in the supplementary_material for further implementation details hundreds of variant_calling have been proposed and the majority of these algorithms have been benchmarked in some form see detailed discussion in section a in the supplementary_material to the best of our knowledge none of these existing benchmarking methodologies accounts for noise in validation_data ambiguity in variant representation or computational_efficiency of variant_calling methods in a consistent and principled fashion given the rapid_growth of next_generation data the need for a robust and standardized methodology has never been greater the it industry serves as an illuminating case study in the context of benchmarking similar to next_generation the it industry has benefited greatly from the additional hardware resources provided by moores law however the industrys rapid progress also hinged on the agreement on proper metrics to measure performance as well as consensus regarding the best benchmarks to run to fairly evaluate competing systems prior to this industry wide agreement each company invented its own metrics and ran its own set of benchmarking evaluations making the results incomparable and customers suspicious of them even worse engineers at competing companies were unable to determine the usefulness of their competitors innovations and so the competition to improve performance occurred only within companies rather than between them once the it industry agreed on a fair playing field progress accelerated as engineers could see which ideas worked well and which did not and new techniques were developed to build on promising approaches similarly we believe that smash could help accelerate progress in the field of genomic variant_calling we have compiled a rich collection of datasets and developed a principled set of evaluation metrics that together allows for quantitative evaluation of variant_calling in terms of accuracy computational_efficiency and robustness ease of use via ability to run on aws moreover although smash currently focuses on benchmarking variant_calling for normal human_genomes we believe that the motivating ideas behind smash along with the tools developed as part of smash will be useful in devising analogous variant_calling benchmarking toolkits for human cancer_genomes and for the genomes of other organisms finally we view smash as a work in progress as the contents of smash reflect and are limited by existing technologies smash currently has limited ground_truth data for human_genomes and the validation_data across datasets are enriched in easier non repetitive_regions owing to underlying sequencing and chip biases like any benchmarking suite smash must evolve over time to stay relevant as new sources of validation_data become available e g the na knowledge_base m depristo personal communication or curated variants from illuminas platinum genome these datasets should be incorporated into smash existing datasets should also be updated to keep them fresh and prevent algorithms from overfitting to stale benchmarks and benchmarking datasets should be deprecated as new data_sources obviate their utility 
