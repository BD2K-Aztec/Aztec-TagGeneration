cross study validation for the assessment of prediction_algorithms motivation numerous competing algorithms for prediction in high_dimensional settings have been developed in the statistical and machine_learning literature learning algorithms and the prediction_models they generate are typically evaluated on the basis of cross_validation error estimates in a few exemplary datasets however in most applications the ultimate_goal of prediction modeling is to provide accurate_predictions for independent samples obtained in different settings cross_validation within exemplary datasets may not adequately reflect performance in the broader application context methods we develop and implement a systematic approach to cross study validation to replace or supplement conventional cross_validation when evaluating high_dimensional prediction_models in independent datasets we illustrate it via simulations and in a collection of eight estrogen_receptor microarray gene_expression where the objective is predicting distant_metastasis dmfs we computed the c index for all pair_wise combinations of training and validation datasets we evaluate several alternatives for summarizing the pairwise validation statistics and compare these to conventional cross_validation results our data_driven simulations and our application to survival_prediction with eight breast_cancer microarray_datasets suggest that standard cross_validation produces inflated discrimination_accuracy for all algorithms considered when compared to cross study validation furthermore the ranking of learning algorithms differs suggesting that algorithms performing best in cross_validation may be suboptimal when evaluated through independent_validation availability the survhd survival in high_dimensions package http www bitbucket org lwaldron survhd will be made available through bioconductor contact cross_validation and related resampling methods are de facto standard for ranking supervised_learning algorithms they allow estimation of prediction_accuracy using subsets of data that have not been used to train the algorithms this avoids over optimistic accuracy estimates caused by re substitution this characteristic has been carefully discussed in it is common to evaluate algorithms by estimating prediction_accuracy via cross_validation for several datasets with results summarized across datasets to rank algorithms this approach recognizes possible variations in the relative performances of learning algorithms across studies or fields of application however it is not fully consistent with the ultimate_goal in the development of models with biomedical_applications of providing accurate_predictions for fully independent samples originating from institutions and processed by laboratories that did not generate the training_datasets it has been observed that accuracy estimates of genomic prediction_models based on independent validation_data are often substantially inferior to cross_validation estimates in some cases this has been attributed to incorrect application of cross_validation however even strictly performed crossvalidation may not avoid over optimism resulting from potentially unknown sources of heterogeneity across datasets these include differences in design acquisition and ascertainment strategies hidden biases technologies used for measurements and populations studied in addition many genomics studies are affected by experimental batch_effects quantifying these heterogeneities and describing their impact on the performance of prediction_algorithms is critical in the practical implementation of personalized_medicine procedures that use genomic_information there are potentially conflicting but valid perspectives on what constitutes a good learning algorithm the first perspective is that a good learning algorithm should perform well when trained and applied to a single population and experimental_setting but it is not expected to perform well when the resulting model is applied to different populations and settings we call such an algorithm specialist in the sense that it can adapt and specialize to the population at hand this is the mainstream perspective for assessing prediction_algorithms and is consistent with validation procedures performed within studies however we argue that it does not reflect the reality that samples of convenience and uncontrolled specimen_collection are the norm in genomic biomarker_studies we promote another perspective a good learning algorithm should be generalist in the sense that it yields models that may be suboptimal for the training population or not fully representative of the dataset at hand but that perform reasonably well to whom correspondence should be addressed 
