sequence_analysis smallwig parallel compression of rna_seq wig files downloaded from contributions we developed a new lossless_compression method for wig data named smallwig offering the best known compression_rates for rna_seq and featuring random_access func tionalities that enable visualization summary_statistics analysis and fast queries from the compressed files our approach results in order of magnitude improvements compared with bigwig and ensures compression_rates only a fraction of those produced by cwig the key features of the smallwig algorithm are statistical data analysis and a combination of source coding methods that ensure high_flexibility and make the algorithm suitable for different applications furthermore for general_purpose file compression the compression_rate of smallwig approaches the empirical en tropy of the tested wig data for compression with random query features smallwig uses a simple block based compression scheme that introduces only a minor overhead in the compression_rate for archival or storage space sensitive applications the method relies on context mixing techniques that lead to further improvements of the compression_rate implementations of smallwig can be executed in parallel on different sets of chromosomes using multiple processors thereby enabling desirable scaling for future transcriptome big_data platforms motivation the development of next_generation has led to a dramatic decrease in the cost of dna rna_sequencing and expression_profiling rna_seq has emerged as an important and inexpensive technology that provides information about whole transcriptomes of various species and organisms as well as different organs and cellular communities the vast volume of data generated by rna_seq experiments has significantly_increased data storage costs and communication bandwidth requirements current compression tools for rna_seq such as bigwig and cwig either use general_purpose compressors gzip or suboptimal compression_schemes that leave significant room for improvement to substantiate this claim we performed a statistical_analysis of expression data in different transform domains and developed accompanying entropy coding methods that bridge the gap between theoretical and practical wig file compression_rates results we tested different variants of the smallwig compression algorithm on a number of integer and real floating point valued rna_seq wig files generated by the encode_project the results reveal that on average smallwig offers fold compression_rate improvements up to fold compression time improvements and fold decompression time improvements when compared with bigwig on the tested files the memory_usage of the algorithm never exceeded kb when more elaborate context mixing compressors were used within smallwig the obtained compression_rates were as much as times better than those of bigwig for smallwig used in the random query mode which also supports retrieval of the summary_statistics an overhead in the compression_rate of roughly was introduced depending on the chosen system parameters an increase in encoding and decoding time of and represents an additional performance loss caused by enabling random data access we also implemented smallwig using multi processor programming this parallelization feature decreases the encoding delay times compared with that of a single processor implementation with the number of processors used ranging from to in the same parameter regime the decoding delay decreased times availability_and the smallwig software can be downloaded from next_generation have resulted in a dramatic decrease of genomic_data sequencing time and cost as an illustrative example the hiseq x machines introduced by illumina in enable whole human_genome in less than h and at a cost of only http www illumina com systems hiseq x sequencingsystem ilmn a suite of other seq techniques has closely followed this development for a comprehensive_overview see http res illumina com documents products research reviews including the by now well documented rna_seq method rna_seq is a shotgun_sequencing technique for whole transcriptomes used for quantitative and functional_genomic in addition to generating sequence related information rna_seq methods also provide dynamic information about gene or functional rna activities as measured by their expression abundance values this makes rna_seq techniques indispensable for applications such as mutation discovery fusion_transcript detection and genomic medicine as a result the volume of data produced by rna_seq methods can be foreseen to increase at a much faster rate than moores law it is therefore imperative to develop highly_efficient lossless compression_methods for rna_seq the problem of dna and rna_sequence and expression compression has received much attention in the bioinformatics community compression_methods for whole genomes include direct sequence compression e g and reference based compression_schemes e g the former class of methods explores properties of genomic_sequences such as small alphabet size and large number of repeats the latter techniques use previously sequenced_genomes as references with which to compare the target_genome or sequencing_reads leading to dramatic reductions in compressed file sizes related similarity discovery based schemes are usually applied to a large collection of genomes and they achieve very small per genome compression_rates e g moreover recent work also includes the compressive genomics paradigm which allows for direct computation and alignment on compressed data the aforementioned methods and some information_theoretic techniques to biological data compression were reviewed in for every base_pair in the genome an rna_seq wig file contains an integer or floating point expression value human_transcriptome wig files may contain hundreds of millions of expression values which amounts to gb of storage space e g one of the subsequently_analyzed wig files randomly_chosen from the encode_project has a size of gb wig files are usually compressed by bigwig which basically performs gzip compression on straightforwardly preprocessed data unfortunately the bigwig format does not appear to offer significant data volume reductions and about of the tracks from the ucsc encode hg browser in bigwig format take up in storage space hoang and sung recently another compression suite termed cwig hoang and sung was implemented as an alternative to bigwig the cwig method outperforms bigwig in terms of compression_rate and random query time although it still relies on suboptimal compression techniques such as elias delta and gamma coding this work focuses on transform and arithmetic compression_methods for expression data in the wig format since wig files capture expressions of correlated rna_sequence blocks modeling these values as independent and identically distributed random_variables is inadequate for the purpose of compression hence we first perform a statistical_analysis of expression values to explore their dependencies correlations and then proceed to devise a new suite of compression algorithms for wig files since the wig format is not limited to rna_seq our compression_methods are also suitable for other types of dense data or quantitative_measurements such as gc_content values probability scores proteomic measurements and metabolomic information the main analytic and algorithmic contributions of our work are as follows i devising a new combination of run length and delta encoding that allows for representing the expression data in highly compact form as part of this procedure we identified runs of locations with the same expression value and then computed the differences of adjacent run values the resulting transformed sequences are referred to as run difference sequences and specialized statistical_analysis of difference sequences constitutes an important step towards identifying near optimal compression strategies ii analyzing the probability_distributions of the difference sequences and inferring mixture markov_models for the data as part of this step we estimated information_theoretic quantities such as the conditional entropy to guide us in our design and evaluation process more precisely we first fitted power_law distributions to the empirical probability_distributions of the difference sequences second we showed that strong correlations exist between adjacent run differences while there exists only a relatively small correlation between the sequences of run length differences and that of the corresponding run expression differences these findings provide a strong basis for performing separate compression of the run length and the expression information iii developing arithmetic encoders for compression of the difference sequences including options such as basic arithmetic coding and context mixing coding based on the work in in this step we were guided by the results of the statistical_analysis and performed alphabet size reduction in the difference sequences and subsequent run length and run expression compression with this step we were able to achieve fold improvements in the compression_rate when compared with bigwig as an illustration a typical wig file of size gb was compressed to roughly mb depending on the userdefined operational mode in comparison traditional gzip and the bigwig compressors produced files of sizes gb and gb respectively our new compression algorithm follows the standard requirements for expression data representation visualization by allowing random_access features via data blocking and separate block compression it also encodes data summary_statistics akin to bigwig data_formats furthermore smallwig has two implementation modes one of which runs on a single processor and another which uses multiple processors in parallel the parallelized version of the algorithm offers significant_savings in computational time with identical rate performance as the serial version the remainder of the article is organized as follows section provides the idea behind our sequence transformations and coding methods section contains our statistical_analysis a detailed_description of the smallwig algorithm is provided in section compression results and a comparative_study of compression_methods is given in section a discussion of our findings and concluding_remarks are given in sections and respectively we tested our compression algorithm on integer valued wig files with sizes ranging from to gb and on integer and real valued bedgraph files all wig files contain human_transcriptome rna_seq from the encode hg browser since smallwig is designed for wig files here we mainly focus on the file set a more detailed report on the performance of smallwig on both file sets can be found in supplementary_material we measured the performance of smallwig and other existing algorithms through the bullet compression_rate compression_ratio the compressed file size divided by the original file size bullet running time of i the encoding ii the decoding and iii the random query process shows the compression_rates achieved by various variants of smallwig compared with the rates of gzip bigwig and cwig through bedgraph the depicted entropy is under the independent run difference model with arithmetic coding our algorithm offers fold rate improvements compared with bigwig in fact the compressed file size of our running example is only of the original wig file furthermore the compression_rate is only larger than the empirical entropy and may be attributed to storing the empirical probabilities with context mixing one can further improve the compression_rate to times compared with bigwig for compression with random queries smallwig offers fold rate improvements compared with bigwig according to the report in hoang and sung the compression_rate of the state of the art cwig method is about times better than that of bigwig however we found that one can obtain an even better rate by first converting a wig file into a bedgraph file and then converting the bedgraph file to cwig with some simple additional processing bedgraph files are compact representations of wig files that fundamentally rely on run length coding our sequential wig bedgraph cwig pipeline performs about times better than bigwig the newly_introduced smallwig method still performs twice as well as the proposed modification of cwig for databases containing tb pb of wig files a fold reduction in file sizes may lead to exceptionally important storage cost_savings in we present the running time of smallwig encoding decoding schemes as well as those of gzip bigwig and cwig with arithmetic coding smallwig has a times smaller encoding and times smaller decoding time compared with that of bigwig arithmetic coding with random query has times smaller encoding time than bigwig context mixing algorithms are computationally_intensive compared with arithmetic coding and require significantly_longer running time to compare the effect of different block sizes used for random query on compression_rate and encoding decoding time we refer the reader to in the experiments the block sizes ranged from to to enable random query we introduced a overhead in compression_rate and a and overhead in encoding and decoding time respectively lists the random query time note that the start positions and for long queries the end positions of the queries were generated uniformly at random among all allowed chromosomal_locations for every chromosome for short queries the query length was fixed to so that one query falls within a single block in this case the query time corresponds to the time needed to retrieve the corresponding block one can see that smallwig is comparable in performance to bigwig for short queries and runs about three times_faster for long queries it is also comparable to cwig for both types cwig hoang and sung through bedgraph and smallwig methods which encompass arithmetic coding arithmetic coding on blocks of size and context mixing algorithms using lpaq to test cwig we constructed our own wig bedgraph cwig pipeline all presented results are averaged over sample files taken from encode hg a more detailed table is included in the supplementary_material smallwig parallel compression of rna_seq wig filesof queries moreover to facilitate visualization in the random query functions smallwig outputs the exact summary information together with the queried location expression pairs on the other hand the bigwig summary function only outputs information corresponding to the overlapped blocks but not to that of the exact queried region we observe that for all the tested files smallwig with arithmetic coding had a relatively small memory_usage as listed in in particular during most of the compression tests the memory_usage was less than kb with different user_defined parameters smallwig with context mixing had higher and more variable memory_usage ranging from kb to mb we note here that since gzip does not offer random_access and summary information its memory_usage is smaller than that of the other algorithms in we show the running time of parallel multiprocessor compression_methods the encoding time is decreased by times as the number of processors increases from to furthermore the decoding time is decreased by times the time does not decrease linearly since we used a uniform sequence partition procedure for individual chromosomes and chromosomes have largely different lengths moreover after every step in the algorithm e g sequence transformation empirical probability computation arithmetic coding some components of the pipeline have to pause until all processors have finished their computations and their information is aggregated we also tested smallwig on wig files that were generated from bedgraph files including integer valued as well as floatingpoint valued expressions the average compression_rates are shown in note that bedgraph already takes_into the run length transformations and hence the compression_rate improvements for these files are not as large as those for wig files for integer valued files smallwig is and times more efficient than bigwig and cwig respectively for floating point valued files smallwig is and times more efficient than bigwig and cwig respectively more details about these tests can be found in supplementary_material in what follows we describe the differences in compression strategies used by various methods and attempt to intuitively explain the improved_performance of smallwig compared with cwig and bigwig all three algorithmsbigwig cwig and smallwiguse run length encoding both cwig and smallwig use delta encoding moreover all three algorithms use blocks of a certain size for random query purposes bigwig and cwig only operate with fixed 
