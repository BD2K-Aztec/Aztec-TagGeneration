reference based compression of short_read using path encoding motivation storing transmitting and archiving data produced by next_generation is a significant computational_burden new compression techniques tailored to short_read are needed results we present here an approach to compression that reduces the difficulty of managing large_scale sequencing_data our novel approach sits between pure reference based compression and reference free compression and combines much of the benefit of reference based_approaches with the flexibility of de_novo encoding our method called path encoding draws a connection between storing paths in de_bruijn and context dependent arithmetic coding supporting this method is a system to compactly store sets of kmers that is of independent interest we are able to encode rna_seq using of the space of the sequence in raw fasta files which is on average more than smaller than competing approaches we also show that even if the reference is very poorly matched to the reads that are being encoded good compression can still be achieved availability_and source_code freely_available for download atthe size of short_read sequence collections is often a stumbling block to rapid analysis central repositories such as the nih sequence_read sra http www ncbi nlm nih gov sra are enormous and rapidly_growing the sra now contains petabases of dna and rna sequence_information and due to its size it cannot be downloaded in its entirety by anyone except those with enormous resources when select experiments are downloaded the local storage burden can be high limiting large_scale analysis to those with large computing_resources available use of cloud_computing also suffers from the data size problem often transmitting the data to the cloud cluster represents a significant fraction of the cost data sizes also hamper collaboration between researchers at different institutions where shipping hard disks is still a reasonable mode of transmission local storage costs inhibit preservation of source data necessary for reproducibility of published results compression techniques that are specialized to short_read can help to ameliorate some of these difficulties if data sizes can be made smaller without loss of information transmission and storage costs will correspondingly decrease while general compression is a long studied field biological sequence compression though studied somewhat before short_read e g is still a young field that has become more crucial as data sizes have outpaced increases in storage capacities in order to achieve compression beyond what standard compressors can achieve a compression v c the author published_by we have provided a novel encoding scheme for short_read that is effective at compressing sequences to of the uncompressed bit encoded size to do this we introduced the novel approach of encoding paths in a de_bruijn using an adaptive arithmetic encoder combined with a bit tree data_structure to encode start nodes see section for a description these two computational_approaches are of interest in other settings as well path encoding achieves better compression than both de_novo schemes and mapping based reference schemes because the reference for the human_transcriptome is small mb compared with the size of the compressed files the overhead of transmitting the reference is recovered after only a few transmissions in addition although it would be possible to shrink the reference using a custom format in our current_implementation the reference is intentionally chosen to be merely a gzipped version of the transcriptomea file that most researchers would have stored anyway path encoding is more general than reference based schemes because we have more flexibility in choosing how to initialize the statistical_model with the reference_sequence for example the reference could be reduced to simple context specific estimates of gc_content this will naturally lead to worse compression but will also eliminate most of the need to transmit a reference technology specific error models could also be incorporated to augment the reference to better deal with sequencing_errors in addition single_nucleotide snp data from a resource such as the hapmap project could be included in the reference to better deal with genomic_variation framing the problem using a statistical generative_model as we have done here opens_the to more sophisticated models being developed and incorporated another source of flexibility is the possibility of lossy sequence compression path encoding naturally handles with errors since they will have low_probability because they are typically seen only once or a few times in contrast to correct kmers that are more frequently seen while encoding a base that has low_probability in a particular context could be converted to a higher probability base under the assumption that the low_probability base is a sequencing_error implementation of this technique does indeed reduce file sizes substantially but of course at the loss of being able to reconstruct the input_sequence while lossy_compression may be appropriate for some analyses such as isoform_expression estimation and error_correction can be viewed as a type of lossy encoding because we are interested in lossless_compression we do not explore this idea more here an interesting direction for future_research is to explore the use of the reference to improve the encoding of the read heads using for example a huffman encoding like scheme another important direction for future work is to reduce the time and memory_requirements of the implementation of path encoding part of the reason for the higher computational_demands is use of dynamic arithmetic encoding which is needed because the probabilities of kmers need not be stationary for example in a file with many as in the first reads but many cs in the later reads the dynamic ac will adapt to this non stationary distribution leading to improved compression another direction for future work is to apply similar ideas to genomic reads where the reference is much larger a recent line of work e g aims at producing searchable compressed representations of sequence_information allowing sequence_search limits the type and amount of compression that can be applied and requires some type of random_access into the encoded sequences arithmetic encoding does not generally allow such random_access decoding because the constructed interval for a given symbol depends on all previously observed symbols however decompression with our path encoding scheme can be performed in a streamingmanner the encoded file is read once from start to finish and the decoder produces reads as they are decoded this would allow reads to be decoded as they are being downloaded from a central repository the other dimension of compressing short_read is storing the quality values that typically accompany the reads path encoding does not attempt to store these quality values as there are other more appropriate approaches for this problem path encoding can be coupled with one of these approaches to store both sequence and quality values in fact in many cases the quality values are unnecessary and many genomic_tools such as bwa and sailfish now routinely ignore them showed that quality values can be aggressively discarded and without loss of ability to distinguish sequencing_errors from novel snps thus the problem of compression of quality values is both very different and less important than that of recording the sequence_reads our main contribution is the design of a high_performing compression scheme we hope that our compression results will spur further reference based read compression work in the new direction that we propose here mapping free statistical compression with accompanying supporting pre_processing 
