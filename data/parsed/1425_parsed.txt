disk based compression of data from genome_sequencing motivation high_coverage data have significant yet hard to exploit redundancy most fastq compressors cannot efficiently compress the dna stream of large_datasets since the redundancy between overlapping reads cannot be easily captured in the relatively small main_memory more interesting solutions for this problem are disk based where the better of these two from cox et_al is based on the burrowswheeler_transform bwt and achieves bits_per for a gbp human_genome collection with almost fold coverage results we propose overlapping reads compression with minimizers a compression algorithm dedicated to sequencing_reads dna only our method makes use of a conceptually_simple and easily parallelizable idea of minimizers to obtain bits_per as the compression_ratio allowing to fit the gbp dataset into only gb of space availability_and http sun aei polsl pl orcom under a free license it is well known that the growth of the amount of genome sequencing_data produced in the last years outpaces the famous moores law predicting the developments in computer hardware confronted with this deluge of data we can only hope for better algorithms protecting us from drowning speaking about big_data management in general there are two main algorithmic concerns faster processing of the data at preserved other aspects like mapping quality in de_novo or referential assemblers and more succinct data representations for compressed storage or indexes in this article we focus on the latter concern raw_sequencing are usually kept in fastq format with two main streams the dna symbols and their corresponding quality_scores older specialized fastq compressors were lossless squeezing the dna stream down to about bpb bits_per and the quality stream to bpb but more recently it was noticed that a reasonable solution for lossy_compression of the qualities has negligible impact on further analyzes for example referential mapping or variant_calling performance this scenario became thus immediately practical with scores lossily compressed to about bpb or less note also that illumina software for their hiseq equipment contains an option to reduce the number of quality_scores even to a few since it was shown that the fraction of discrepant single_nucleotide grows slowly with diminishing number of quality_scores in illuminas casava package http support illumina com sequencing sequencing software casava ilmn it is easy to notice that now the dna stream becomes the main compression challenge even ifhigher order modeling or lz style compression can lead to some improvement in dna stream compression we are aware of only two much more promising approaches both solutions are disk based yanovsky creates a similarity graph for the dataset defined as a weighted undirected graph with vertices corresponding to the reads of the dataset for any two reads s and s the edge_weight between them is related to the profitability of storing s and the edit script for transforming it into s versus storing both reads explicitly for this graph its minimum_spanning mst is found during the mst traversal each node is encoded using the set of maximum exact_matches between the nodes read and the read of its parent in the mst as a backend compressor the popular zip is used recoil compresses a dataset of m illumina bp reads http www ncbi nlm nih gov sra srx with coverage below fold to bpb this is an interesting result but recoil is hardly scalable the test took about h on a machine with ghz intel celeron cpu and four hard disks more recently cox et_al took a different approach based on the burrowswheeler_transform bwt their result for the same dataset was bpb in less than min on a xeon x quad core ghz processor the achievement is however more spectacular if the dataset coverage grows for fold coverage of real human_genome sequence_data the compressed size improves to as little as bpb actually inthe authors report bpb but their dataset is seemingly no longer available and in our experiments we use a slightly different one allowing to represent the gbp of input_data in gb of space note that if the reference_sequence is available either explicit or can be reconstructed presumed in the terminology of c novas and moffat then compressing dna_reads is much easier and high compression_ratios are possible several fastq or sam bam compressors make use of a reference_sequence to name quip fastqz and fqzcomp in one of their modes slimgene cram goby deez and fqzip several techniques for compressing sam_files including mapping reads to a presumed reference_sequence were also explored in c novas and moffat in this article we present a new reference free compressor for fastq data overlapping reads compression with minimizers orcom achieving compression_ratios surpassing the best known solutions for the two mentioned human datasets it obtains the compression_ratios of and bpb respectively orcom is also fast producing the archives in about and min respectively using eight threads on an amd opteron ghz machine we tested our algorithm versus several competitors on real_and detailed in 
