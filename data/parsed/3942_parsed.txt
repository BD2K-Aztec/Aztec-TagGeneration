gene_expression reducing the algorithmic variability in transcriptome based_inference motivation high_throughput measurements of mrna abundances from microarrays involve several stages of preprocessing at each stage a user has access to a large number of algorithms with no universally agreed guidance on which of these to use we show that binary representations of gene_expressions retaining only information on whether a gene is expressed or not reduces the variability in results caused by algorithmic choice while also improving the quality of inference drawn from microarray_studies results binary representation of transcriptome_data has the desirable property of reducing the variability introduced at the preprocessing stages due to algorithmic choice we compare the effect of the choice of algorithms on different problems and suggest that using binary representation of microarray_data with tanimoto kernel for support_vector reduces the effect of the choice of algorithm and simultaneously improves the performance of classification of phenotypes a plethora of computational_methods for the statistical_analysis of high_throughput gene_expression is available to users interested in making inferences from transcriptomes the preprocessing stages involved in the analysis include background correction within and between array normalizations probe specific correction and summarization after the raw_data is processed it is subject to sophisticated machine_learning such as classification cluster_analysis and the modelling of time course data by means of dynamical_systems the preprocessing stages lead to quantifications of relative mrna abundances taking scanned images as input the choices made here have been shown to have an important effect on the results of statistical_inference approaches we review the effect of the choice of algorithms on different problems and suggest that using binary representation of microarray_data with tanimoto kernel for support_vector svm reduces the variability due to the to whom correspondence should be addressed choice of algorithm and simultaneously improves the performance of classification on transcriptome_data the most extensive study so_far that shows significant algorithmic variability is due to p c boutros who analysed an impressive different combinations of preprocessing algorithms and quantified the sensitivity and stability of expression levels while this and similar studies seek the best combination of algorithms on a small number of datasets they do not offer a generic solution for practitioners to select a combination that leads to reliable results in downstream inference our approach starting from the premise of seeking sensible numerical precisions to represent microarray_data is similar to the bar_code method advanced by zilliox and irizarry in our previous work we established that gene_expressions quantized to binary precision lead to minimal average loss in the quality of inference drawn from them in a range of applications such as classification clustering and the analysis of time course data any loss of information due to binarization was shown to be easily recovered using metrics of similarity between gene_expression that are best suited for high_dimensional binary spaces our results showed that the tanimoto metric successfully used in matching chemical fingerprints in the chemoinformatics literature when cast in kernel svm and spectral clustering frameworks is able to achieve performances often better than and never worse than using data to the high numerical precision with which it is often reported and archived as reviewed above gene_expression values obtained by different preprocessing algorithms yield different results we use a gene_selection problem to illustrate the impact of this variation we took the dataset of a breast_cancer study and computed the most discriminant genes that could be used as hypothetical markers for discrimination these genes were selected using the fishers ratio as criterion as is commonly done we notice substantial differences in the genes that were identified as carrying the most discriminant information in three different algorithmic combinations as shown in the confusion matrix of the three algorithmic combinations chosen here correspond to those achieving the minimum maximum and mean class prediction_performance of an svm classifier measured in terms of the area under the receiver_operating auroc the names of the genes selected are given in supplementary table s the selected genes between the best and the worst performing classifiers overlap by only about this observation motivates a systematic study to explore the variability in inference quality caused by the choice of algorithms in the preprocessing stages and possible approaches to reducing this variability as we report in the remainder of this article a binary representation with kernel classification in high_dimensional spacesis able to achieve a drastic_reduction in the variability caused by algorithmic choice in this article we address the variability in the results of inferences from transcriptome_studies that arises from the choice of preprocessing algorithms this variability has previously been shown to have a significant effect on the gene_expressions measured by microarrays our work reported here shows that the binary representation helps significantly in reducing this algorithmic choice induced variability in classification_problems when used in combination with a high_dimensional kernel method while previous_studies have largely focused on pointing out that such a variability exists by reference to how well measured expressions of a gene correlates with spiked in concentration or with page sds of the aurocs shown as box plots inare shown statistical significances using f test show levels of confidence at which our proposed method of binary tanimoto differs from the alternate approach an alternate measurement such as qpcr quantitative_polymerase we have focused on the quality of inference drawn in the context of classification_problems to the best of our knowledge comparisons of inference algorithms on microarray_data e g support_vector versus nearest_neighbour as predictors of phenotype of which there is a very large body of literature in the statistical and machine_learning communities are based on the application of one set of preprocessing algorithms often published by the original authors of a particular study given the variability we observe we believe there is room for scepticism of conclusions drawn from such studies 
