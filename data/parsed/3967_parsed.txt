genetics_and reliable abc model choice via random_forests motivation approximate_bayesian abc methods provide an elaborate approach to bayesian_inference on complex models including model choice both theoretical arguments and simulation_experiments indicate however that model posterior_probabilities may be poorly evaluated by standard abc techniques results we propose a novel approach based on a machine learning_tool named random_forests rf to conduct selection among the highly complex models covered by abc algorithms we thus modify the way bayesian model_selection is both understood and operated in that we rephrase the inferential goal as a classification_problem first predicting the model that best fits the data with rf and postponing the approximation of the posterior_probability of the selected model for a second stage also relying on rf compared with earlier implementations of abc model choice the abc rf approach offers several potential_improvements i it often has a larger discriminative_power among the competing_models ii it is more robust against the number and choice of statistics summarizing the data iii the computing effort is drastically_reduced with a gain in computation efficiency of at least and iv it includes an approximation of the posterior_probability of the selected model the call to rf will undoubtedly extend the range of size of datasets and complexity of models that abc can handle we illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population_genetics datasets availability_and the proposed methodology is implemented in the r package abcrf available on the cran approximate_bayesian abc represents an elaborate statistical_approach to model based_inference in a bayesian setting in which model likelihoods are difficult to calculate due to the complexity of the models considered since its introduction in population_genetics the method has found an ever increasing range of applications covering diverse types of complex models in various scientific fields see e g the principle of abc is to conduct bayesian_inference on a dataset through comparisons with numerous simulated_datasets however it suffers from two major difficulties first to ensure reliability of the method the number of simulations is large hence it proves difficult to apply abc for large_datasets e g in population_genomics where tens tohundred thousand markers are commonly genotyped second calibration has always been a critical step in abc implementation more specifically the major_feature in this calibration_process involves selecting a vector of summary_statistics that quantifies the difference between the observed data and the simulated_data the construction of this vector is therefore paramount and examples abound about poor performances of abc model choice algorithms related with specific choices of those statistics even though there also are instances of successful implementations we advocate a drastic modification in the way abc model_selection is conducted we propose both to step away from selecting the most probable model from estimated posterior_probabilities and to reconsider the very problem of constructing efficient summary_statistics first given an arbitrary pool of available statistics we now completely bypass selecting among those this new perspective directly proceeds from machine_learning methodology second we postpone the approximation of model posterior_probabilities to a second stage as we deem the standard numerical abc approximations of such probabilities fundamentally untrustworthy we instead advocate selecting the posterior most probable model by constructing a machine_learning classifier from simulations from the prior predictive distribution or other distributions in more advanced versions of abc known as the abc reference table the statistical technique of random_forests rf represents a trustworthy machine learning_tool well adapted to complex settings as is typical for abc treatments once the classifier is constructed and applied to the actual data an approximation of the posterior_probability of the resulting model can be produced through a secondary rf that regresses the selection error over the available summary_statistics we show here how rf improves upon existing classification methods in significantly_reducing both the classification error and the computational expense after presenting theoretical arguments we illustrate the power of the abc rf methodology by analyzing controlled experiments as well as genuine population_genetics datasets this article is purposely focused on selecting a statistical_model which can be rephrased as a classification_problem trained on abc simulations we defend here the paradigm_shift of assessing the best fitting model via a rf classification and in evaluating our confidence in the selected model by a secondary rf procedure resulting in a different approach to precisely estimate the posterior_probability of the selected model we further provide a calibrating principle for this approach in that the prior error_rate provides a rational way to select the classifier and the set of summary_statistics which leads to results closer to a true bayesian_analysis compared with past abc implementations abc rf offers improvements at least at four levels i on all experiments we studied it has a lower prior error_rate ii it is robust to the size and choice of summary_statistics as rf can handle many superfluous statistics with no impact on the performance rates which mostly depend on the intrinsic dimension of the classification_problem a characteristic confirmed by our results iii the computing effort is considerably_reduced as rf requires a much smaller reference table compared with alternatives i e a few thousands versus hundred thousands to billions of simulations and iv the method is associated with an embedded and error freereliable abc model choiceevaluation which assesses the reliability of abc rf analysis as a consequence abc rf allows for a more robust handling of the degree of uncertainty in the choice between models possibly in contrast with earlier and over optimistic assessments because of a massive gain in computing and simulation efforts abc rf will extend the range and complexity of datasets e g number of markers in population_genetics and models handled by abc in particular we believe that abc rf will be of considerable interest for the statistical processing of massive snp datasets whose production rapidly increases within the field of population_genetics for both model and non model_organisms once a given model has been chosen and confidence evaluated by abc rf it becomes possible to estimate parameter distribution under this single model using standard abc techniques or alternative_methods such as those proposed by 
