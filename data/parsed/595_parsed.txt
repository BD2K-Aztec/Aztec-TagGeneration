automated annotation of gene_expression image_sequences via non parametric factor_analysis and conditional_random motivation computational_approaches for the annotation of phenotypes from image data have_shown across many applications and provide rich and valuable information for studying gene function and interactions while data are often available both at high_spatial and across multiple time points phenotypes are frequently annotated independently for individual time points only in particular for the analysis of developmental gene_expression it is biologically sensible when images across multiple time points are jointly accounted for such that spatial and temporal dependencies are captured simultaneously methods we describe a discriminative undirected graphical_model to label gene_expression time series image data with an efficient training and decoding method based on the junction tree algorithm the approach is based on an effective feature_selection technique consisting of a non parametric sparse bayesian factor_analysis model the result is a flexible framework which can handle large_scale data with noisy incomplete samples i e it can tolerate data missing from individual time points results using the annotation of gene_expression across stages of drosophila embryonic_development as an example we demonstrate that our method achieves superior accuracy gained by jointly annotating phenotype sequences when compared with previous_models that annotate each stage in isolation the experimental_results on missing_data indicate that our joint learning method successfully annotates genes for which no expression data are available for one or more stages the use of high_throughput image_acquisition such as in phenotypic screens has been quickly increasing and thus provides a new source of data for computational_biologists microscopy of colored or fluorescent_probes followed by imaging is able to deliver spatial and temporal quantitative phenotype information such as gene_expression at high_resolution in addition expression_patterns can be documented and distributed over the internet as a valuable resource to the research_community recent_advances in throughput or long_term investment in specific projects have by now generated large collections of images such image databases are traditionally analyzed through direct inspection by human curators an example is the berkeley drosophila genome_project bdgp gene_expression pattern database in this dataset images are assigned to stage ranges within the embryonic stages defined by developmental features and annotated collectively in small groups using a controlled_vocabulary cv i e annotation terms this allows researchers to search image databases and compare spatial and temporal embryonic_development given the very diverse nature of imaging_technology samples and biological_questions computational_approaches have often been tailored to a specific dataset for example the image_based profiling of gene_expression via in situ_hybridization ish requires the development of accurate and automatic image_analysis systems for using such data to understand regulatory_networks and development of multicellular_organisms images are affected by multiple sources of noise due to experiments or microscopy incomplete or multiple embryos variance of probes across genes illumination artifacts making the extraction and registration of embryos non trivial peng and myers andintroduced an automatic image annotation framework using various high_dimensional feature representations and classifying frameworks principal_component pca wavelets gaussian_mixture support_vector svm quadratic_discriminant each image may show the embryo under different views lateral dorsal or ventral this is a challenge for gene_annotation as embryonic structures may be visible in only certain views yet recent_studies that incorporating images from multiple_views could consistently improve the annotation accuracy it is desirable to represent images in a way that takes_advantage of image_features and offers robustness to image distortions in contrast to such large feature_sets prone to high redundancy and high computational costs identified a set of basic expression_patterns in drosophila a set of welldefined clusters describing specific regions of embryo expression were determined from lateral views of early development as with the majority of described approaches this study involved a high_level of human intervention in selecting good images for training testing purposesa potential drawback considering the rapid increase in the size of ish image collections in contrast pruteanuproposed a new approach for automatic_annotation of spatial expression_patterns using a vocabulary of basic patterns that involved little to no human intervention this work provided a flexible unsupervised framework in competitively predicting gene_annotation terms while using only a small set of features to whom correspondence should be addressed the author published_by this is an open_access the terms of the creative_commons http creativecommons org_licenses which permits non commercial re use distribution and reproduction in any medium provided the original_work for commercial re use please_contact permissions_oup com a particular aspect that has been largely neglected by computational_approaches so_far is that data acquired from such experiments often span_multiple time points or conditions phenotypes are typically annotated stage by stage without jointly learning the salient temporal dependencies across multiple time points which should allow for an overall higher accuracy e g the annotation terms predicted for earlier_stages should inform the prediction at later stages furthermore many genes are annotated with more than one term from the vocabulary creating an additional dependency_structure between annotations within the same stage range in this article we address the automatic_annotation of drosophila embryo gene_expression sequences building on state of the art models from computer vision and machine_learning there are several challenges that need to be addressed when approaching this problem through computational_methods as we mentioned previously the image_acquisition process results in embryonic structures with multiple_perspectives shapes and locations moreover the shape position of the same embryonic structure may vary from image to image variation in morphology and incomplete knowledge of the shape and position of various embryonic structures have made the gene_annotation task more prohibitive we first show that a non parametric bayesian factor_analysis bfa approach the infinite factor model allows for an efficient and sparse feature representation of the drosophila gene_expression dataset then we propose a conditional_random crf to tackle the time evolving annotation task experiments show that the exploitation of dependencies across adjacent developmental_stages leads to annotation accuracy superior to existing drosophila gene_expression annotation approaches the proposed_framework also tackles the missing_data scenario for many genes one or more stage ranges are absent from the image collection in such cases human annotators would take into account the entire set of expression data from adjacent stages to successfully annotate the available images the challenge to automatize this process is novel and represents a step closer toward a fully_automatic gene annotation_pipeline these predictions can be later analyzed by biologists to assess the correctness of the image_acquisition and the level of interest for that particular gene finally for a given gene the described framework predicts the entire set of annotation terms simultaneously taking full advantage of the term dependencies that exist at the stage range level the rest of this article is organized as follows in section we focus on data description and introduce the sparse bfa crf framework experimental_results are reported in section followed by conclusions and future work in section expression is visualized by rna ish which provides an effective way of locating specific mrna_sequences by hybridizing complementary mrna binding oligonucleotides and a suitable dye the mrna_expression apparent in the captured in situ images was verified by independently_derived microarray time course analysis using affymetrix_genechip technology more details can be found at http insitu fruitfly org and in gene_expression were documented by taking low and high magnification images at multiple developmental_stages the low magnification digital_images were taken to capture groups of embryos to provide a permanent record of the hybridization in each well each slide was then further examined under higher magnification using a zeiss axiophot optical_microscope images were assigned to developmental_stage ranges following the sequence of events taking_place at specific times after fertilization using the stages defined in in this analysis we focused on the first hr of drosophila_development spanning embryonic stages and developmental_stages were skipped owing to predominant ubiquitous expression_patterns not of interest to our analysis any gene is represented on average by approximately images however the number of images per gene varies from to this variability reflects the bdgp strategy to document highly_dynamic complex and novel patterns while lowering the number of images documenting common expression_patterns among those there are images with noninformative patterns due to poor_quality staining washing or non tissue_specific expression maternal or ubiquitous images within the same window can show different patterns due to embryo orientation or the relatively long developmental time spanned by a stage range images are annotated with ontology_terms from a cv describing developmental expression_patterns this vocabulary has been developed and refined by flybase over the past_few allowing human curators to compare their findings with expression data assembled from the literature expansion of annotations to greater detail and thorough searches of datasets based on gene_ontology schema the annotations used throughout this project consisted of a subset of about of the annotation terms in the flybase cv many of which only apply to later stages of development as mentioned previously we use all available images in our approach i e including those taken with any embryo orientation lateral dorsal and ventral before extracting features we segmented and registered images using a previously described probabilistic segmentation approach based on statistical shape models this provides us with pixel images mostly containing a single embryo in a standard dorsal up anterior down orientation and no background in we show a particular gene_expression pattern across five developmental_stage ranges of interest the complexity and variability of the image data led to competitive but not perfect results in terms of precise embryo extraction as well as embryo orientation which increased the challenge of automatic gene_annotation we here use the average intensities in a down sampled fixed grid size of pixels as input_features for the entire collection of images within the bdgp dataset in this section we describe the application of a sparse bfa crf framework for automatic time course gene_expression pattern annotation our procedure starts by extracting sparse meaningful features sbfa from the entire collection of drosophila_embryos suitable for downstream temporal analysis based on a conditional_random approach we then use the estimated factor_loadings as observed variables in the crf framework so as to infer most likely annotation terms for previously_unseen images 
