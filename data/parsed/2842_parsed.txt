data_and measuring the wisdom of the crowds in network based gene function inference motivation network based gene function inference methods have proliferated in recent_years but measurable progress remains_elusive we wished to better explore performance trends by controlling data and algorithm implementation with a particular focus on the performance of aggregate predictions results hypothesizing that popular_methods would perform well without hand tuning we used well characterized algorithms to produce verifiably untweaked results we find that most state of the art machine_learning obtain gold_standard performance as measured in critical assessments in defined tasks across a broad range of tests we see close alignment in algorithm performances after controlling for the underlying data being used we find that algorithm aggrega tion provides only modest benefits with a increase in area under the roc auroc above the mean auroc in contrast data aggregation gains are enormous with an improvement in mean auroc altogether we find substantial evidence to support the view that additional algorithm development has little to offer for gene_function availability_and the supplementary_information contains a description of the algorithms the network data parsed from different biological data resources and a guide to the source_code available at http gillislab cshl edu supplements high_throughput often relies on computational_methods for functional inference and interpretation improving our knowledge of gene function in otherwise uncharacterized genes is one major task to which these computational_methods are put while this is often called gene_function when being treated as a machine learning_problem essentially the same methods underlie a variety of important biomedical_applications such as candidate disease_gene like many methods in machine_learning a major_concern for function inference methods is the degree to which their performance is robust and generalizes from benchmark tasks to novel data to some extent systematic problems with generalizing past gene_function to future performance have been recognized by the field these have led to critical assessments of function prediction e g the critical assessment of mus_musculus gene_function mousefunc and the critical assessment of function_annotation cafa where groups compete to predict gene function these assessments are intended to provide field wide benchmarks for comparative performance and thereby help determine which research directions may be more fruitful generally_speaking gene_function methods rely on i prior function assignments for genes ii data to characterize genes and iii an algorithm which learns the data features associated with the previous function assignments novel genefunction mappings are then predicted based on the learned data features this can be done either in a gene centric or function centricbecause individual laboratories approach this problem in quite different ways characterizing the field overall in critical assessment has been difficult it is hard to know what factors in one laboratorys implementation drives results indeed even in critical assessments the same laboratory may submit slightly altered versions of the same software and find their performance changes dramatically we solve the over training problem addressed by critical assessments in an alternate way by using verifiable implementations it can be checked that there are no adjustments or tweaks normally fine_tuning algorithms and data to work appropriately are necessary to obtain reasonable performance but this is precisely what may contribute to poor generalization based on performance trends in previous critical assessments we hypothesized that welldeveloped machine_learning would perform at a high_level if using gold_standard data resources that the performance assessment is not tuned to obtain artificially high performance can be ensured by using pre_existing and verifiable tests i e the critical assessments by having an in house representative of the field as a whole we are able to conduct additional experiments with greater control one of our principal interests is in exploring how aggregation improves_performance that it generally does so has been a finding common to previous assessments of function prediction and network_inference and is a frequent expectation for machine_learning in general however the factors central to this effect have not been well characterized likely because most laboratories will have an individualized approach which makes well controlled comparison difficult to overcome this we will first benchmark a set of machine_learning based upon data resources available from mousefunc to establish they are representative_samples of high_performing methods then we will systematically apply these algorithms on various types of saccharomyces_cerevisiae data as a sample of field wide properties and investigate how their combination improves_performance and on what factors this depends there is a simple and consistent trend in our results even algorithms of wholly distinct conception and design give quite similar results in all ways on the same data this may seem an obvious findingone cannot get blood from a stonebut it is in fact contrary to much of the published_literature summarizing our points of departure from the previous_literature the most similar previous analysis to ours concluded that its the machine that matters even where similarly using default implementations similarly it is the consensus that algorithm aggregation alone can provide_substantial benefits on the closely_related task of network_inference which would normally suggest that algorithms vary enough to benefit from aggregation supported by these general findings more focused assessments have also concluded that simply aggregating algorithms on the same data offers enormous benefits again on tasks closely_related to but not identical to function prediction e g mutation prioritization while we draw superficially contrary conclusion our results are actually surprisingly consistent with these claims within the range of performance of our algorithms on a given dataset aggregation has a very large influence thus in any competition in which data is held_constant we too would suggest aggregation as a potentially useful strategy it is only by examining variation across data resources that we see how comparatively modest these benefits are in support of this finding in critical assessments where developers may alter their data choice high_performing methods have tended to use large and diverse datasets as in the top performing cafa method where data_integration was one of the chief areas of novelty this suggests that comparative_assessment of data or even its integration may be more fruitful than the current algorithmic focus in fact it would then be important to hold methods somewhat constant fortunately our analysis suggests this is rather easy and that even where methods differ in basic design and conception they are quite similar in performance relative to variation caused by data because of this downstream results can be assumed to be the property of the data being studied rather than the inference_method being used to study it while it is true that good experimental_design can ensure over training does not occur if a custom method is necessary to get a dataset to work or the custom method works unusually well we suggest there is a heavy onus to show it does not reflect accidental over training in some way of course another reason for our performance trends is that all our algorithms performed well if some performed badly then the machine could be said to matter more we believe there may be a subtle selection_bias at play here which is difficult to evaluate formally our function inference task was more computationally_challenging than what might be considered a typical assessment for a targeted function or multiple functions in a targeted dataset while our algorithms are not customized many methods we initially tried simply did not operate readily in the large data and prediction space we wished to assess this may well have selected for general_purpose 
