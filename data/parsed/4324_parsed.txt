sofia a data_integration framework for annotating high_throughput datasets motivation integrating heterogeneous datasets from several sources is a common bioinformatics task that often requires implementing a complex workflow intermixing database access data filtering format conversions identifier mapping among further diverse operations data_integration is especially important when annotating next_generation data where a multitude of diverse tools and heterogeneous databases can be used to provide a large variety of annotation for genomic_locations such a single_nucleotide or genes each tool and data source is potentially useful for a given project and often more than one are used in parallel for the same purpose however software that always produces all available data is difficult to maintain and quickly leads to an excess of data creating an information overload rather than the desired goal oriented and integrated result results we present sofia a framework for workflow driven data_integration with a focus on gen_omic annotation sofia conceptualizes workflow templates as comprehensive workflows that cover as many data_integration operations as possible in a given domain however these templates are not intended to be executed as a whole instead when given an integration task consisting of a set of input_data and a set of desired output data sofia derives a minimal workflow that completes the task these workflows are typically fast and create exactly the information a user wants without requiring them to do any implementation work using a comprehensive genome_annotation template we highlight the flexibility extensibility and power of the framework using real_life case studies biological entity annotation is the process of retrieving the broader biological_context around a biological entity from available data_sources it is a crucial step in essentially all current biological analyses based on high_throughput experiments only by considering the biological_context within which an experiment took_place can the results be properly interpreted and a pertinent conclusion reached as increasingly more knowledge about biological_systems is discovered and stored in structured databases the effort required to integrate all relevant_information in projects producing comprehensive experimental_datasets becomes more and more involved providing investigators all available information can lead to information overload as such complete annotation sets contain much more data than is relevant to the investigated hypothesis there is a lack of fast and intuitive methods that i allow researchers to specify exactly the type of information that they think is best suited to their investigations and ii produce this information quickly and fully automatically data_integration for the life_sciences is by no means a new topic current approaches can be broadly categorized into three classes so_called data warehouses are relational databases that integrate a selected set of data into a common schema accessing the data in a data_warehouse requires either the ability to program complex queries usually in sql or the usage of specific point and click user_interfaces encapsulating such queries the latter solution is the only option when programming expertise is lacking but is inflexible and involves costly interface development a second class of data_integration systems are based on linked open data and semantic_web standards these offer more flexibility in terms of data modelling but require efforts comparable to data warehousing for building semantically integrated datasets both approaches perform data_integration prior to any concrete analysis which implies that they usually try to be as comprehensive as possible to cover unforeseen applications creating or updating this large integrated dataset is highly complex and time consuming increasing the danger of using outdated data j rg and more recently a third class of systems has emerged that are based on flexible integration workflows in these approaches data_integration is performed by starting a pipeline of steps that are defined in advance by a workflow developer results of these workflows are typically directly consumed by the user or by other tools and not meant to be materialized in a persistent maintained manner accordingly every analysis uses the most recent data available to be fast these workflows are specialized a drawback when no available workflow exactly meets the users requirements either a new workflow has to be developed or multiple workflows with potentially overlapping subtasks have to be executed yielding inflexibility and unnecessary computation what is lacking is a data_integration method that based on a formalized understanding of an application domain is able to automatically_determine the minimal complete_sequence of steps_required to fulfil a given user request starting from a given set of input_data sofia software for flexible integration of annotation aims at filling this gap using sofia workflow designers specify comprehensive workflow templates covering as much of a given application domain as possible much like defining the process used to populate a data_warehouse however these templates are not intended to be executed in their entirety instead they should be understood as a formalized knowledge_base of processes transforming various types of input_data into various types of annotations using background_knowledge when given a set of input_data e g results from wet_lab experimentation and a desired output e g gene names related to the experimental_data in a specific way sofia uses the template to infer a minimal set of actions necessary to produce the output from the inputs in case there is a unique way for doing so the process runs fully automatically we see that the sofia approach has the following important_advantages a it is fast as only the necessary steps are executed b it supports reproducibility as users can choose which data versions will be integrated c from a users perspective it is very simple to use as only inputs and outputs have to be specified d it relieves from the need to build and update a large comprehensive database for unforeseen applications e it clearly separates data provisioning from data transformation annotation steps which yields a clear workflow design we developed sofia specifically for performing multiple types of analysis in next_generation ngs such as variant annotation rna_seq analysis chip_seq analysis or epigenetic analysis common aspects of these experiments are that they i map experimental read outs to parts of a genomic_sequence ii require specific information on these genome parts to produce biologically_meaningful and iii multiple tools and databases exist for each step in such a pipeline the design of sofia directly targets these requirements in this paper we present our framework for data_integration describe a template that is readily provided with the system and demonstrate the flexibility and power of sofia by case studies in variant annotation for a breast_cancer dataset comparison of the impact of using different gene models and an analysis of translation efficiencies using e coli protein expression data we developed sofia a framework that models biological entity annotation as an integration workflow that associates provided input_data from high_throughput experiments to requested annotations with little effort on the part of the user sofia is freely_available and comes with a comprehensive template for annotating genomic_data which we demonstrate here with three real_life use cases annotating sequence_variants comparing gene model consequences and calculating sequence_features that purportedly estimate translational_efficiency we strongly believe that supporting individualized data annotation is an important yet often neglected matter even when using the same type of data two different groups may have different annotation needs depending on their overall research goal for instance while some groups may seek information from drug response databases such as drugbank or context specific genetic dependencies such as achilles others may be more interested in the population_genetics which requires comparing their variant set to those from projects like the genomes_project genomes_project consortium et_al or dbsnp a third group might study the mechanistic effects of variants for particular types of diseases which requires them to integrate data from diseases specific sequencing panels like the cancer_genome or the international cancer genome consortium supporting all such needs with a single up to date data_warehouse requires complex schemas costly maintenance operations and continuous systems re engineering to properly integrate novel datasets and types into the schema the same extensions in sofia would require only programming a series of data transformations which are later only executed on demand 
