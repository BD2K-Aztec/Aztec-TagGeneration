data_and performance reproducibility index for classification motivation a common practice in biomarker_discovery is to decide whether a large laboratory experiment should be carried_out based on the results of a preliminary study on a small set of specimens consideration of the efficacy of this approach motivates the introduction of a probabilistic measure for whether a classifier showing promising results in a small sample preliminary study will perform similarly on a large independent sample given the error estimate from the preliminary study if the probability of reproducible error is low then there is really no purpose in substantially allocating more resources to a large follow on study indeed if the probability of the preliminary study providing likely reproducible results is small then why even perform the preliminary study results this article introduces a reproducibility index for classification measuring the probability that a sufficiently small error estimate on a small sample will motivate a large follow on study we provide a simulation_study based on synthetic distribution models that possess known intrinsic classification difficulties and emulate real_world scenarios we also set up similar simulations on four real_datasets to show the consistency of results the reproducibility indices for different dis tributional models real_datasets and classification_schemes are empirically calculated the effects of reporting and multiple rule biases on the reproducibility index are also analyzed availability we have implemented in c code the synthetic data distribution model classification rules feature_selection routine and error estimation methods the source_code is available at http gsp tamu edu publications supplementary yousefi a supplementary simulation_results are also included perhaps no problem in translational genomics has received more attention than the discovery of biomarkers for phenotypic discrimination to date there has been little success in developing clinically useful biomarkers and much has been said concerning the lack of reproducibility in biomarker_discovery in particular recently a report concerning comments made by us food_and fda drug division head janet woodcock stated janet woodcock drug division head at the fda this week expressed cautious optimism for the future of personalized drug_development noting that we may be out of the general skepticism phase but were in the long slog phase the major barrier to personalized_medicine as woodcock sees it is coming up with the right diagnostics the reason for this problem is the dearth of valid biomarkers linked to disease_prognosis and drug response based on conversations woodcock has had with genomics researchers she estimated that as much as of published biomarker associations are not replicable this poses a huge challenge for industry in biomarker identification and diagnostics development she said evaluating the consistency of biomarker discoveries across different platforms experiments and datasets has attracted the attention of researchers the studies addressing this issue mainly revolve around the reproducibility of signals for example lists of differentially_expressed their significance scores and rankings in a prepared list they try to answer the following question do the same genes appear differentially_expressed when the experiment is re run and the references therein suggest several solutions to this and related_questions our interest is different a prototypical reproducibility paradigm arises when a classifier is designed on a preliminary study based on a small sample and based on promising reported results a follow on study is performed using a large independent data sample to check whether the classifier performs well as reported in the preliminary study many issues affect reproducibility including the measurement platform specimen handling data_normalization and sample compatibility between the original and subsequent studies these may be categorized as laboratory issues note that here we are not talking about the issue of providing access to data and software for follow_up studies on published results one can conjecture mitigation of these issues as laboratory technique improves however there is a more fundamental methodological issue namely error estimation in particular inaccurate error estimation can lead to overoptimism in reported results the typical analysis proceeds in the following fashion i based on the data a feature_set is chosen from the ii a classifier is designed with feature_selection perhaps being performed in conjunction with classifier design and iii the classification error is measured by some procedure using the same sample data upon which feature_selection and classifier design have been performed given no lack of reproducibility owing to laboratory issues if the error estimate is sufficiently deemed small and a follow on study with independent data specimens is carried_out can we expect the preliminary error estimate on a sample of to be reproduced on a test sample of size since the root_mean rms error between the true and estimated errors for independent test data error estimation is bounded by ffiffiffiffi m p where m is the size of the test sample a test sample of insures rms so that the test sample estimate can be taken as the true error there are two fundamental related_questions dougherty i given the reported estimate from the preliminary study is it prudent to commit large resources to the follow on study in the hope that a new biomarker diagnostic will result ii prior to that is it possible that the preliminary study can obtain an error estimate that would warrant a decision to perform a follow on study a large follow on study requires substantially more resources than those required for a preliminary study if the preliminary study has a very low_probability of producing reproducible results then there is really no purpose in doing it we propose a reproducibility index that simultaneously addresses both questions posed earlier our focal_point is not that independent validation_data should be usedthis has been well argued for instance in the context of bioinformatics to avoid overoptimism rather the issue addressed by the reproducibility index is the efficacy of small sample preliminary_studies to determine_whether a large validating study should be performed we set up a simulation_study on synthetic models that emulate real_world scenarios and on some real_datasets we calculate the reproducibility index for different distributional models and real_datasets and classification_schemes we consider two other scenarios i multiple independent preliminary_studies with small samples are carried_out and only the best results minimum errors reported and ii multiple classification_schemes are applied to the preliminary study with small samples and only the results minimum errors of the best classfier are reported a decision is made for a large follow on study because the reported errors show very good performance show that there is a poor statistical relationship between the reported results and true classifier performance in these scenarios namely there is a potential for significant optimistic reporting bias or multiple rule bias these two biases can substantially impact the reproducibility index 
