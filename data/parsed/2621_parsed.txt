journaled string tree a scalable data_structure for analyzing thousands of similar genomes on your laptop motivation next_generation ngs has revolutionized biomedical_research in the past_decade and led to a continuous stream of developments in bioinformatics addressing the need for fast and space efficient solutions for analyzing ngs_data often researchers need to analyze a set of genomic_sequences that stem from closely_related or are indeed individuals of the same species hence the analyzed sequences are similar for analyses where local changes in the examined sequence induce only local changes in the results it is obviously desirable to examine identical or similar regions not repeatedly results in this work we provide a datatype that exploits data paral lelism inherent in a set of similar sequences by analyzing shared regions only once in real_world experiments we show that algorithms that otherwise would scan each reference sequentially can be speeded up by a factor of availability the data_structure and associated tools are publicly_available at http www seqan de projects jst and are part of seqan the c template library for sequence_analysis next_generation ngs has revolutionized biomedical_research in the past_decade and led to a continuous stream of developments in bioinformatics addressing the need for fast and space efficient solutions for analyzing ngs_data especially since the sequencing efficiency of modern ngs_technologies outpaced the improvement of storage capacities which directly leads to a growing economical issue as storing and sharing the generated information is now bounded by the available storage and network resources the same technology led to the generation of comprehensive catalogs for genetic_variations of the human and other organisms as well e g moreover the rapid advances in ngs made ambitious sequencing endeavors like the genomes_project systematic studies of cancerous genomes the international cancer genome or most eagerly the announced goal of the personal genome_project to sequence human_genomes possible those resources then provide detailed information about the genetic_diversity of entire populations which will be important to the societal health_sector to acquire better understandings of the correlation between clinical conditions and phenotypes and their corresponding genotypes as a result two major challenges need to be tackled the first challenge clearly is to compress the available sequence_data to relieve disk and network resources owing to the high redundancy of sequences originating from the same or related organism referential sequence compression has been proven to be especially efficient for these kinds of data e g more recentlyexploited cross sequence correlations to achieve profitable compression_ratios for the data of the genomes_project the second challenge however is to devise algorithms and data_structures that can handle the massive_amounts of available data to incorporate those information in existing analyzing pipelines clearly one solution would be to apply the algorithms sequentially to each sequence contained in the database but the runtimes scale linearly to the number of sequences included thus it is desirable to analyze the data in succinct form to archive runtimes proportional to the compressed size this paradigm is also known as compressive genomics the fm_index and the compressed suffix_array for example are succinct representations of indices that can be searched efficiently yet indexing thousands of genomes with these data_structures would still exceed currently available memory capacities by far in the past_years this problem was subject in several publications here the focus was moved from considering each sequence individually to exploiting similarities among the sequences to reduce the overall memory_footprint and to gain substantial speedups opposed to the sequential case presented compression accelerated blast and blat both tools to search patterns in a non redundant sequence library approximatively these methods however require the compressed sequence library to be generated in a computationally_intensive preprocessing phase used as input a more general format consisting of a reference_sequence and a set of variants for a collection of sequences which is a common representation of the data produced by large sequencing endeavors such as the genomes_project subsequently they built an index over the reference_set exploiting the high to whom correspondence should be addressed the author published_by all_rights for permissions please_e journals permissions_oup com similarities yet the read_mapper bwbble was the first practical tool capable of mapping reads against thousand genomes simultaneously in their approach they indexed a multi genome reference_sequence with the fm_index and adapted the burrows_wheeler aligner bwa to work on the indexed multi genome to construct the multi genome they needed a context size to determine the sequence_context left and right of genomic_regions harboring insertions_or thus the entire multi genome and the accompanied index must be reconstructed on changes of the context size however there exists a plethora of algorithms for sequence_analysis that are sequential in nature that means they scan the sequences to be analyzed from left to right and perform some computation e g compute an approximate matching or alignment of a query_sequence to a reference_sequence or scan sequences in the context of a hidden_markov hmm e g de there are many more applications such as filtering and verification algorithms in read_mappers or searching with position_specific after reading a character of the string the algorithm will change its internal_state and possibly report some results gave an algorithm to solve the pattern_matching with hamming_distance for a set of extremely similar sequences in their model they assumed that each sequence differs in around positions to every other sequence within the set to find all occurrences of a pattern within all sequences they searched first the reference_sequence and used then some auxiliary tables to check if at the reported positions the pattern can be found for the other sequences too besides the impractical error_model for real_biological they did not provide any experiments to evaluate their method thus to the best of our knowledge we provide for the first time a general solution to generically speed up a large class of algorithms when working on sets of similar strings we will mostly address the reduction of execution time and space of such algorithms by providing a data type called journaled string tree jst that can be traversed similarly to a simple for loop over all sequences while exploiting the high similarity the algorithm must simply be able to store its state when asked continue its computation from a stored state when presented with a new character and work locally i e when scanning a sequence its current state solely depends on a fixed size window of last seen characters also called context our approach is based on a reference based compression of shared sequence parts with some additional bookkeeping for example if lets say mb of a set of genomes is shared and we want to compute a semiglobal alignment on all sequences we will execute the alignment only once on this stretch of mb for regions that exhibit differences a corresponding sequence_context is constructed on thefly and then examined we can show that our approach exhibits speedups of times when analyzing a set of sequences of chromosome two haplotypes of sequences from the genomes_project and a reference_sequence compared with running the algorithms sequentially while using only gb of space this speedup includes all overheads the speedup in searching alone is up to a factor of 
