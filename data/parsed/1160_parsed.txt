conveyor a workflow engine for bioinformatic_analyses motivation the rapidly increasing amounts of data available from new high_throughput methods have made data_processing without automated pipelines infeasible as was pointed_out in several publications integration of data and analytic resources into workflow systems provides a solution to this problem simplifying the task of data analysis various applications for defining and running workflows in the field of bioinformatics have been proposed and published e g galaxy mobyle taverna pegasus or kepler one of the main aims of such workflow systems is to enable scientists to focus on analysing their datasets instead of taking_care for data management job management or monitoring the execution of computational tasks the currently available workflow systems achieve_this but fundamentally differ in their way of executing workflows results we have developed the conveyor software library a multitiered generic workflow engine for composition execution and monitoring of complex workflows it features an open extensible system architecture and concurrent program execution to exploit resources available on modern multicore cpu hardware it offers the ability to build complex workflows with branches loops and other control structures two example use cases illustrate the application of the versatile conveyor engine to common bioinformatics problems availability the conveyor application including client and server are available atworkflows have become an important aspect in the field of bioinformatics during the last years e g applications like galaxy taverna pegasus and kepler offer an easy way to access local and remote resources and perform automatic analyses to test_hypotheses or process data in many cases they have become a reasonable alternative to write simple software_tools like perl scripts especially for users without an in depth computer science background libraries like ruffus add workflow to whom correspondence should be addressed functionality to programming_languages providing methods to define and build workflow within own applications a workflow is built from several linked steps that consume inputs process and convert data and produce results the most simple workflows are linear chains of steps to convert input_data to the required output more complex setups may also include loops branches parallel and conditional processing reading from various sources and writing different outputs in various formats for creating reusable workflow components a processing step may be composed of nested processing_steps allowing the user to build complex pipelines for higher_level analysis many workflow engines act as a wrapper using existing command_line utilities or enact and orchestrate existing web_services as basic modules for their processing_steps as a result adding new processing_steps by wrapping existing applications often does only require little or no programming effort of course this comes at a price passing data between processing_steps depends on a common data format especially in the case of distributed_processing nodes most analysis tools available as command_line applications or web_services are consuming simple text formats e g the fasta_format for dna and amino_acid these formats are in turn used by the workflow engines to exchange data between processing nodes integrating other processing nodes or input sources requires explicit data conversion prior to processing this often leads to the loss of information e g gene features annotated in embl or genbank entries cannot retain all qualifiers after converting them to fasta_format an analysis done byshows that most tasks used in publicly_available taverna workflows are dedicated to data conversion to some extent this problem is solved by meta_information provided with types that allow the definition of type hierarchies and interfaces the biomoby data type management is an example for an ontology based_approach to data type handling nonetheless it requires extra efforts by the developer and or maintainer other attempts to define common data types like bioxsd orwere made but none of them has been successfully adopted by the community yet the situation is even worse if legacy data from applications are to be integrated into a workflow accessing data e g stored in a relational_database or available by a local application only requires special processing_steps passing the data between distributed_processing nodes may not be possible at all another problem arises from the nature of web_services used in processing_steps although they offer an elegant and easy way to provide and consume useful services users have to be aware of the pitfalls of web_services if they rely on them for an analytical workflow a service may become unavailable without prior noticethe use cases shown in the previous section demonstrate the usefullness of the conveyor system to common problems both use cases were processed with the set of available escherichia_coli genomes accession_numbers nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc ac nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc nc by concatenating their genbank formatted files available at the ncbi web_site the workflows in both use cases were executed on a single host with four intel_xeon e cpus running at ghz each cpu provides six physical cores and six additional virtual cores by hyper threading summing up to cores managed by the operation system the system is equipped with gb system ram and gb swap conveyor was configured to use a threadbased processing model with one thread per workflow node and a process based processing model for external applications the number of parallel running processes were set to one two four eight and sixteen each setup was run five times execution time was measured using the time command reporting the overall time real the accumulated cpu time of all processes user and the time spend in the kernel sys the complete benchmark data is available as supplementary_material as presented in the preceding sections the conveyor system offers a comprehensive and versatile system for data analysis although it is designed to work in any field of application the use cases presented in the former sections clearly prove its fidelity to the field of bioinformatics the unique design especially the powerful object model for both data and nodes allows the system to fill the gap between web_service based_approaches and writing custom software 
