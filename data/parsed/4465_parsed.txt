succinct data_structures for assembling large_genomes motivation second generation sequencing_technology makes it feasible for many researches to obtain enough sequence_reads to attempt the de_novo of higher_eukaryotes including mammals de_novo not only provides a tool for understanding wide scale biological_variation but within human biomedicine it offers a direct way of observing both large_scale structural_variation and fine_scale sequence_variation unfortunately improvements in the computational feasibility for de_novo have not matched the improvements in the gathering of sequence_data this is for two reasons the inherent computational_complexity of the problem and the in practice memory_requirements of tools results in this article we use entropy compressed or succinct data_structures to create a practical representation of the de_bruijn assembly_graph which requires at least a factor of less storage than the kinds of structures used by deployed methods moreover because our representation is entropy compressed in the presence of sequencing_errors it has better scaling behaviour asymptotically than conventional approaches we present results of a proof_of assembly of a human_genome performed on a modest commodity server availability binaries of programs for constructing and traversing the de_bruijn assembly_graph are available from http www genomics a central problem in sequence bioinformatics is that of assembling genomes from a collection of overlapping short fragments thereof these fragments are usually the result of sequencing the determination by an instrument of a sampling of subsequences present in a sample of dna the number length and accuracy of these sequences varies_significantly between the specific technologies as does the degree of deviation from uniform sampling and all these are constantly_changing as new technologies are developed and refined nonetheless it is typically the case that we have anywhere from hundreds of thousands of sequences several hundred bases in length to hundreds of millions of sequences a few tens of bases in length with error_rates between and depending on the technology to whom correspondence should be addressed the two main techniques used for reconstructing the underlying sequence from the short fragments are based on overlap_layout models and de_bruijn models the former was principally used with older sequencing_technologies that tend to yield fewer longer_reads and the latter has become increasingly popular with second_generation which yield many more shorter sequence_fragments irrespective of the technique it has been shown e g bythat the problem of sequence_assembly is computationally hard and as the correct solution is not rigorously defined all practical assembly techniques are necessarily heuristic in nature it is not our purpose here to discuss the various assembly techniqueswe restrict our attention to certain aspects of de_bruijn assemblywe refer the reader tofor a fairly comprehensive_review of assemblers and assembly techniques space consumption is a pressing practical problem for assembly with de_bruijn based_algorithms and we present a representation for the de_bruijn assembly_graph that is extremely compact the representations we present use entropy compressed or succinct data_structures these are representations typically of sets or sequences of integers that use an amount of space bounded closely by the theoretical minimum suggested by the zero order entropy of the set or sequence these representations combine their space efficiency with efficient access in some cases query operations can be performed in constant time and in most cases they are at worst logarithmic succinct data_structures are a basic building_block shows more complex discrete data_structures such as trees and graphs that can be built using them some of the tasks for which they have used include web graphs xpath indexing partial sums and short_read we have created a set of programs that construct and manipulate the de_bruijn assembly graph_representation we have described these do not constitute a complete assembler but represent the kinds of traversal and manipulation of the graph that are required to build an assembler the proof_of assembly procedure is as follows with each step being performed by a separate program that takes an on disk representation of the data and produces a new on disk representation of the data extract mers forwards and reverse complements from the sequence_reads and sort them into lexographic order the result of the sort operation is a list from which we can extract mer count pairs from which we construct the the sparse array for the graph_structure and the succinct representation of the counts on large_datasets this can be done in parts and the resulting partial graphs are merged to form the complete graph perform a left to right traversal of the list of edges counts and discard low_frequency edges which almost certainly correspond to errors perform depth first traversal to read of non branching paths within the graph to report as contigs the first step demonstrates the feasibility of building the graph_representation the second that it is possible to do trivial processing efficiently the third that graph traversal can be done to produce a modified representation in this case eliminating paths in the graph that probably correspond to errors and the fourth that meaningful contigs can be obtained a more detailed_description of these steps including pseudo code is provided in section of the supplementary page the reported time for the abyss assembly was h compared with our elapsed time of h it is not clear fromwhether the reported time is aggregate time or elapsed wall time though the latter seems more likely materials we believe that this proof_of demonstrates the feasibility of our method though a complete assembler would need to do significantly more processing on the graph e g bubble removal should use read coherence to resolve local ambiguities and should make use of pairing information to resolve repeats we have run this proof_of assembly pipeline on the sequence_data from a yoruban individual from sample number na with k the assembly was performed using a single computer with ghz opteron cores and gb ram the size of the graph edges and counts at the stages of the pipeline are shown in each step produces a set of files containing the representation of the graph these files are then brought into memory by the program for the next step using memory mapped i o the complete graph at the end of the first step is gb which is larger than the gb ram on the machine but the next step removing low_frequency edges does a sequential pass over these structures to produce a new smaller set so although the process virtual size is considerably larger than main_memory the accesses have extreme locality so the overall behaviour is efficient report results of assembling the same data with abyss in we reproduce the results reported there for the assembly not using the pairing information from the reads along with the results from our proof_of assembly importantly we have included the scope of the computing_resources used in both cases unsurprisingly our pipeline lacking bubble elimination and read coherent disambiguation of branches mostly produces only short contigs curiously the longest contig at about kb does not match the reference human_genome at all but is an exact match in to the epsteinbarr virus which is an artifact of the preparation of the cell_line from which the sequence_data were obtained that this is the longest contig is unsurprising since viral_sequences are not diploid like the human_genome and therefore are less prone to bubbles due to heterozygosity and viral_sequences tend to contain far less repetition than the human_genome and will therefore have much less branching in their de_bruijn graph_representation we have claimed that the number of bits per edge should be monotonically decreasing with the number of edges this is clearly not the case in the results in the graph containing all the edges present in the sequence_data uses more bits per edge the analysis in section gives a lower_bound for the number of bits required for the graph for the billion edges in our complete graph this suggests that about bits per edge or gb in total are required from we see that for the complete graph bits are required this translates to about bits or gb of space used beyond the the theoretical minimum as discussed in section of the supplementary_materials this is an artifact of our implementation which could be eliminated but in absolute terms is very minor to put it in perspective bits per edge is dramatically less than the bits required for a pointer and even a hashing based_approach would require at least bits per edge other entropycompressed bit vector representations may bring the space usage of the graph closer to the theoretical minimum we have presented a practical and efficient representation of the de_bruijn assembly_graph and demonstrated the kind of the operations that an assembler needs to perform but of course there is much more to doing de_novo with de_bruijn methods than we have presented a combinatoric number of eulerian paths exist in the de_bruijn assembly_graph among which true paths must be identified this is the eulerian superpath problem described by this is usually done in the first instance by using the sequence_reads to disambiguate paths in the second instance this is done by using paired sequence_reads e g pairedend and mate_pair sequence_reads in a process usually called scaffolding the algorithms described in the literature can either be implemented directly on our representation or in most cases adapted 
