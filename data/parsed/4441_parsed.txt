sequence_analysis string_graph construction using incremental hashing motivation new sequencing_technologies generate larger amount of short_reads data at decreasing cost de_novo sequence_assembly is the problem of combining these reads back to the original genome_sequence without relying on a reference_genome this presents algo rithmic and computational challenges especially for long and repetitive genome_sequences most existing_approaches to the assembly problem operate in the framework of de_bruijn yet a number of recent works use the paradigm of string_graph using a variety of methods for storing and processing suffixes and prefixes like suffix_arrays the burrowswheeler_transform or the fm_index our work is motivated by a search for new approaches to constructing the string_graph using alternative yet simple data_structures and algorithmic concepts results we introduce a novel hash based method for constructing the string_graph we use incremental hashing and specifically a modification of the karprabin fingerprint and bloom filters using these probabilistic methods might create false_positive edges during the algorithms execution but these are all detected and corrected the advantages of the proposed approach over existing_methods are its simplicity and the incorporation of established probabilistic techniques in the context of de_novo genome sequen_cing our preliminary implementation is favorably comparable with the first string_graph construction of simpson and durbin but not with subsequent improvements further research and optimizations will hopefully enable the algorithm to be incorporated with noticeable performance improvement in state of the art string graph_based assemblers availability_and a beta version of all source_code used in this work can be downloaded fromde novo sequence_assembly namely reconstructing an unknown genome_sequence from a set of overlapping sequence_reads is an important problem in bioinformatics there has been an extensive research in this area in the past_two yielding several efficient methods for the task of sequence_assembly yet nextgeneration_sequencing pose_challenges to current assemblers read sets produced by modern sequencing_machines contain up to hundreds of millions short_reads as a consequence there is an ongoing need for new assembly approaches which should provide a significant decrease both in memory_consumption and running time the first assembly paradigm to be commonly used was the overlap_layout olc one used in several assemblers such as celera and tigr the first stage of these assemblers aims to construct an overlap_graph representing the sequence_reads in this graph reads are represented as nodes and two nodes are connected by an edge if and only if the corresponding reads overlap constructing this graph is one of the biggest problems of the olc paradigm as this stage is both time and memory intensive one possible solution for this problem was recently_introduced in the leap assembler which uses compact data_structures to represent the overlap_graph the de_bruijn paradigm which was first proposed by is more space efficient compared with the olc paradigm as it merges reads from different instances of a repeat in this approach reads are broken into k_mers which serve as the nodes in a de_bruijn thus reads that come from the same repeat but from different locations in the genome share the same path in the de_bruijn however this approach might increase the ambiguity of assembling short repeats several short_read assemblers have implemented this approach e g euler velvet and abyss in recent_years several improvements of the de_bruijn method have led to a significant decrease in the memory_consumption needed for de_bruijn assemblers saved considerable memory_usage by not recording read locations and pairedend information used sparse bit arrays to store an implicit representation of the de_bruijn took advantage of the redundancy in the reads set and constructed roughly an equivalent de_bruijn by storing only one out of g nodes where g an efficient assembly both time and memory wise of a human_genome was reported by who used bloom filters to represent the de_bruijn as well as additional data_structures for avoiding false positive_nodes these results were further improved by the usage of cascading bloom filters a different framework is based on the string_graph where the edges of the overlap_graph are partitioned to two different types irreducible edges which are retained and transitive edges which are removed the notion of transitive edges removal was first introduced in while the term string_graph was first defined in it was used in the celera_assembler developed the string_graph assembler sga further implementing a new algorithm that outputs the string_graph directly without the need to first construct the overlap_graph and only then to remove transitive edges the string_graph approach has the advantage of repeats sharing the same path without the need to break the reads into k_mers as in the de_bruijn approach the readjoiner rj assembly improved on the sga assembler in terms of time and memory complexity this was achieved by first producing a relevant subset of all overlaps between read_pairs using matches between smaller strings as a filter and then outputting the set of irreducible edges by applying a traversal algorithm on a graph representing the sorted set of candidate overlaps in this article we present a different approach for the construction of the string_graph which resemble the sga algorithm of simpson and durbin but relies on different theory we apply hash functions to efficiently store access and process prefixes and suffixes of reads this method relies on computing hash values modulo a large prime for all prefixes and suffixes of the reads our algorithm deals solely with these hash values except during a verification process performed on the reads themselves the probabilistic_method might introduce false_positive and methods to overcome them are also detailed the algorithm is relatively easy to implement as it simplifies the task of identifying irreducible edges by using probabilistic techniques such as incremental rolling hash and bloom filters to the best of our knowledge this is the first incorporation of these two techniques together in the genome_assembly context we hope and expect these probabilistic techniques will lead to a simplification and improved_performance of state of the art assemblers as well right now our initial results improve on the first version of the sga string_graph construction method described in simpson and durbin further optimizations may substantially reduce the required computational_resources as was the case with the latest_version of the sga assembler at this point the method is only a proof of a new concept and not a complete assembler however it can be smoothly combined with other assemblers that are based on the string graph_representation of reads we have implemented the algorithm in c the program receives a file containing reads in a fasta_format and produces the set of corresponding irreducible edges we performed two sets of tests all conducted on a machine with a ghz intel_xeon e core processor with gb ram running a bit linux operating system using only a single core the first set of tests aimed to check the correctness of the implemented algorithm in these tests we have created simulated genomes with different lengths and different frequencies of repetitive zones these genomes were then randomly_sampled to create read files the output of these tests was compared with the expected set of irreducible edges obtained by an inefficient na ve algorithm that was applicable owing to the relatively small_size of the simulated_datasets the second set of tests was conducted using simulated_data from human_chromosomes and after removing sequence gaps from those chromosomes we generated baselong reads randomly at an average coverage of reads that appear more than once including reverse complements were removed we then applied our algorithm to construct the string_graph of these four different sets of reads with a minimum overlap of bases the time required for these executions and the memory consumed are presented in in addition this table includes the time and memory_performance of three other algorithms sga sga corresponding to present sga code with the basic index and overlap options of simpson and durbin sga sga corresponding to latest sga using more advanced features and rj corresponding to readjoiner see the supplementary_material for a specification of the commands executed in our comparative_study sga reflects a current_implementation and the results we report here seem better than the original ones reported in simpson and durbin our executions were all obtained using a single thread sga uses a number of optimizations both in the indexing and overlapping phase which dramatically improved the memory_consumption we remark that in our current_implementation all data are stored in ram throughout the entire execution this differs from the three other tools where the index is stored on disk the performance of our algorithm compares_favorably with that of sga its memory_consumption is of the memory required for sga whereas its execution time is only regarding sga and rj these two are far more memory_efficient than our tool of our memory_consumption however ih is slightly faster than sga rj isstring graph construction using incremental hashing substantially faster than the other three tools fold more efficient we observed no false_positive edges were created during the executions on the above_mentioned data this indicates that in practice the verifications could be safely ignored see section as our algorithm is a proof_of we expect that future optimizations will lead to an improved_performance much like the way sga improves on sga depicts the effect of different expected false_positive of the bloom_filter on the overall performance the falsepositive rate determines the number of hash functions used and the number of bits allocated for the bloom_filter if the falsepositive rate is low e g the number of bits in the data_structure and the number of hash functions must be large implying high space and time consumption when we increase the false_positive we observe a continuous improvement in both memory and time with peak performance for fp from this point on the performance of the algorithm deteriorates this occurs because too many hash values are incorrectly reported to be in the bloom_filter so the program consumes more memory and runs longer we note that can be considered high in many applications however false_positive in the bloom_filter part typically does not cause long false branching as opposed to collisions in the ih part which tend to propagate longer our main contribution in this work is the introduction of a novel and efficient method for the construction of the string_graph from a set of sequence_reads probabilistic methods are still not widely used in the domain of assembly apart from the usage of bloom_filter by the combination of the probabilistic nature and the usage of easily computed hash functions yield a rather efficient and fast index an encouraging feature of our algorithms is that no false_positive edges have reached the verification stage in all four chromosomes tested we argued that this desired property is expected to occur for larger instances as well at this stage our tool is a proof_of and not a full fledged assembler to assemble contigs the string_graph can be traversed and processed a strength of our approach as well as of sga is the possibility to access a large number of reads by referring only to a single entitythe hash value of a common prefix in sga this will be a range of consecutive indexes this implies that the number values in each ol prefixes set is bounded by the length of the read rather than the number of overlapping reads the hash functions used must be incremental to enable us not to store the set of ol prefixes in memory the overlap extension phase deals mainly with hash values and would otherwise be much less memory and time efficient in addition the incremental nature of the hash function enable indexing all reads in time linear in the total length of the reads we made two assumptions in the design of this algorithm error_free data and equal length reads the first assumption can be dealt with by using existing algorithms that preprocess the reads and use statistical_methods to correct sequencing_errors as for the second assumption modifications can be done in the algorithm to not rely on reads having the same length for example we consider an irreducible edge to belong to a read whose extension process is terminated first if some reads are shorter this condition is violated and we need to revert to checking the overlap lengths as well the verification process we currently perform requires accessing the reads during the construction of the string_graph we observed that false_positive edges hardly ever appear under the current choice of modulus m in the kr fingerprint and the parameters of the bloom_filter this should enable us to remove all verifications thus making it possible not to store and access the original reads after the indexing phase resulting in a somewhat lower memory_consumption our present implementation uses a separate bloom_filter for every suffix length to reduce the number of possible collisions it may be possible to use a single bloom_filter for all lengths by properly adjusting the values of the relevant_parameters this could potentially lead to a substantial saving in memory there are additional aspects that could be improved for example we have used a version of the kr fingerprint but other ih functions d lemire and o kaser submitted for publication may perform even better in addition many parts of our algorithm can be parallelized the impact of the hash function moduli and the bloom_filter parameters on the performance can be further investigated we believe that we have not yet reached the optimal choice of parameter_values which can save more memory and running time even at the expanse of an increased number of false_positive which are subsequently detected memory can be slightly saved by representing bases in reads using bits_per instead of a single character however this will not effect the peak memory used the preliminary_results shown here make us believe that the hash based_approach can yield even better results in the future the performance of our algorithm is favorably comparable with the first implementation of the fm indexbased assembler by simpson and durbin a number of newer assemblers are based on string graphs such as edena sga leap and readjoiner these assemblers differ in the data_structures and algorithms used to construct the string_graph and in particular how they compute suffixprefix matches their reported performances are substantially better than the performance of our initial_implementation however we believe that our algorithm is of interest due to its simplicity and the probabilistic techniques that are incorporated in it and that improvements as outlined above can make it competitive with state of the art string_graph algorithms more generally we expect that probabilistic approaches can play a key factor in improving other string_graph based_approaches 
