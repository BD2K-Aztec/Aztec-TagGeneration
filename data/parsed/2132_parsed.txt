structural_bioinformatics a high_throughput and highly_parallel open_source molecular_simulation toolkit motivation molecular_simulation has historically been a low_throughput technique but faster computers and increasing amounts of genomic and structural data are changing this by enabling large_scale automated simulation of for instance many conformers or mutants of biomolecules with or without a range of ligands at the same time advances in performance and scaling now make it possible to model complex biomolecular interaction and function in a manner directly testable by experiment these applications share a need for fast and efficient software that can be deployed on massive scale in clusters web_servers distributed computing or cloud resources results here we present a range of new simulation algorithms and features developed during the past_years leading up to the gromacs software_package the software now automatically handles wide classes of biomolecules such as proteins nucleic_acids and lipids and comes with all commonly used force_fields for these molecules built in gromacs supports several implicit_solvent models as well as new free_energy algorithms and the software now uses multithreading for efficient parallelization even on low end systems including windows based workstations together with hand tuned assembly kernels and state of the art parallelization this provides extremely_high performance and cost efficiency for high_throughput as well as massively_parallel simulations availability gromacs is an open_source and free software available from http www gromacs org although molecular_dynamics of biomolecules is frequently classified as computational_chemistry the scientific roots of the technique trace back to polymer chemistry and structural_biology in the s where it was used to study the physics of local molecular propertiesflexibility distortion and stabilizationand relax early x_ray structures of proteins on short time scales molecular_simulation in general was pioneered even earlier in physics and applied to simplified hard_sphere systems the field of molecular_simulation has developed tremendously since then and simulations are now routinely performed on multi microsecond scale where it is possible to repeatedly fold small proteins predict interactions between receptors and ligands predict functional properties of receptors and even capture intermediate states of complex transitions e g in membrane_proteins this classical type of single long simulation continues to be important as it provides ways to directly monitor molecular_processes not easily observed through other means however many current studies increasingly rely on large sets of simulations enabled in part by the ever increasing number of structural_models made possible by sequencing and structural_genomics as well as new techniques to estimate complex molecular_properties using thousands of shorter simulations mutation studies can now easily build models and run short simulations for hundreds of mutants model_building web_servers frequently offer automated energy_minimization and refinement and free_energy are increasingly being used to provide better interaction_energy estimates than what is possible with docking in these scenarios classical_molecular based on empirical_models have a to whom correspondence should be addressed the author published_by all_rights for permissions please_e journals permissions_oup com significant role to play as most properties of interest are defined by free_energies which typically require extensive sampling that traditional quantum_chemistry methods can not provide for large systems these developments would not have been possible without significant research_efforts in simulation algorithms optimization parallelization and not least ways to integrate simulations in modeling pipelines the emergence of standardized packages for molecular_modeling such as charmm gromos amber namd and gromacs has been important as these have helped commoditize simulation and molecular_modeling research and made the techniques available to life_science application researchers who are not specialists in simulation development all these packages have complementary strengths and profiles for the gromacs molecular_simulation toolkit one of our primary long_term development goals has been to achieve the highest possible simulation efficiency for the small to mediumsize clusters that were present in our own research_laboratories as computational_resources are typically limited in those settings it is sometimes preferable to use throughput approaches with moderate parallelization that yield whole sets of simulations rather than maximizing performance in a single long simulation however in recent_years we have combined this with optimizing parallel scaling to enable long simulations when dedicated clusters or supercomputers are available for select critical problems during the past_years since gromacs we have developed a number of new features and improvements that have led up to release of the software and significantly_improved both performance and efficiency for throughput as well as massively_parallel applications many tasks that only a decade_ago required exceptionally large dedicated supercomputing resources are now universally accessible and sometimes they can even be run efficiently on a single workstation or laptop however contemporary low end machines are now parallel computers ranging from to cores on a laptop and up to cores on workstations and require parallel programs to use all resources in a single job here we present the work and features that have gone into gromacs including development to make the code fully portable and multithreaded on a wide_range of platforms features to facilitate high_throughput simulation and not least more efficient tools to help automate complex simulations such as free_energy with another long_term goal of commoditizing affinity prediction as well high end performance in gromacs has also been improved with new decomposition techniques in both direct and reciprocal_space that push parallelization further and that have made microsecond simulation timescales reachable in a week or two even for large systems using only modest computational_resources 
