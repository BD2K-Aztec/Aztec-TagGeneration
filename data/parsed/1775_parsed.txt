fastlsu a more practical approach for the benjamini hochberg fdr controlling procedure for huge scale testing problems motivation we address a common problem in large_scale data analysis and especially the field of genetics the huge scale testing problem where millions to billions of hypotheses are tested together creating a computational challenge to control the inflation of the false_discovery as a solution we propose an alternative algorithm for the famous linear step up procedure of benjamini and hochberg results our algorithm requires linear time and does not require any p value ordering it permits separating huge scale testing problems arbitrarily into computationally_feasible sets or chunks results from the chunks are combined by our algorithm to produce the same results as the controlling procedure on the entire set of tests thus controlling the global false_discovery even when p values are arbitrarily divided the practical memory_usage may also be determined arbitrarily by the size of available memory availability_and r code is provided in the supplementary_material in many fields the substantially increased scale of data available has resulted in a significant increase in the size of multiple_hypotheses testing problems in genetics in particular typical gwas_studies consist of snps while eqtl studies newly advanced methylation studies and imaging_studies usually start with tests these testing problems are hugescale as opposed to large_scale used byto describe studies consisting of hundreds to thousands of hypotheses it is preferable to control the false discovery proportion rather than the number_of for a huge scale testing problem therefore the fdr or the pfdr approaches are favored and both tend to offer larger more powerful sets of results than those yielded by the conservative fwer control these huge scale multiple_hypotheses testing problems create numerous computational challenges when many tests say of the order are performed with all of the p values of more or less equal importance as a result some simpler testing procedures such as rigid p value thresholds may be used that sacrifice power and correctness alternatively tests may be separated or chunked into smaller sets or chunks that are more computationally_feasible notes that the problem of separating hypotheses tests has not received great attention and warns of some pitfalls in chunking p values but focuses on grouping tests that share a biological property rather than arbitrary computationally_feasible chunks cai and sun and later propose alternative solutions to efrons grouping problem but do not address the problem of arbitrary computationally_feasible chunking we confront the computationally_feasible chunking problem for thebenjaminihochberg false_discovery we show on data from strangers hapmap study that if results from separate tests are not combined correctly there is considerable inflation of type_i offer an explication for this occurrence and propose our algorithm as a solution consider a huge scale testing problem of size m where our goal is to select exactly r significant tests of the r significant discoveries exactly v tests will be false_discoveries i e truly nonsignificant tests that are declared significant a common approach in multiple_hypotheses testing problems is to control the family_wise fwer prv the probability of selecting at least one false discovery for huge scale testing a more favorable alternative is to control the false discovery proportion fdp v maxr the proportion of truly false tests among the significant r some will prefer to control the positive fdr pfdr efdpjr the expectation of the fdp when significant tests are selected while others will opt to control the false_discovery fdr the expectation of the fdp e fdp the fdr is always of a potentially smaller magnitude than the fwer and of the pfdr fdr pfdr prr yet in reality for huge scale testing fwer fdr and sometimes pfdr fdr therefore both fdr and pfdr control approaches tend to offer larger more powerful sets of results than those that might be offered by the conservative fwer control for a further discussion about fwer fdr and pfdr refer to farcomeni in huge scale testing when the m p values are partitioned into chunks it is challenging to control the fwer pfdr or fdr over the entire collection of m p values controlling these error_rates on a per chunk basis if not done correctly may interfere with the overall results by introducing more false_discoveries although the same difficulty arises for the fdr and pfdr control it is easier to illustrate this for the fwer consider for instance an example of fwer control using the bonferroni approach by collecting all p values less than a m assuming that the number m of p values happen to be very large so that m should be divided_into k chunks each of of size m i so that m p k i m i applying bonferroni in chunks of size m i will tend to select more significant results than applying it over the entire set of m p m i p values since a m is less than a m i in the case of fwer control using a fixed bound of a m for all the chunks is theoretically preferred but often yields no significant results a stricter constant cut_off on all sets of tests as suggested by dudbridge and gusnanto for gwas was developed based on results from simulated gwas however such an ad_hoc approach eliminates from the entire multiple_hypotheses testing problem any knowledge of the actual significance_level a used 
