genome_analysis bias correction for selecting the minimal error classifier from many machine_learning models_motivation supervised machine_learning is commonly_applied in gen_omic research to construct a classifier from the training data that is generalizable to predict independent testing data when test datasets are not available cross_validation is commonly used to estimate the error_rate many machine_learning are available and it is well known that no universally best method exists in general it has been a common practice to apply many machine_learning and report the method that produces the smallest cross_validation error_rate theoretically such a procedure produces a selection_bias consequently many clinical studies with moderate sample_sizes e g n risk reporting a falsely small cross_validation error_rate that could not be validated later in independent cohorts results in this article we illustrated the probabilistic_framework of the problem and explored the statistical and asymptotic properties we proposed a new bias correction_method based on learning curve_fitting by inverse power_law ipl and compared it with three existing_methods nested cross_validation weighted mean correction and tibshirani tibshirani procedure all methods were compared in simulation datasets five moderate_size real_datasets and two large breast_cancer datasets the result showed that ipl outperforms the other methods in bias correction with smaller variance and it has an additional advantage to extrapolate error estimates for larger_sample a practical feature to recommend whether more samples should be recruited to improve the classifier and accuracy an r package mlbias and all source files are publicly_available in the past_two fast development in bioinformatics was accompanied by the rapid production of high_throughput such as gene_expression genotyping and various types of next_generation data such high_dimensional usually come with small_sample and a large_number features also known as large p small n problem and pose many new challenges in statistical_learning and data_mining in the content below we focus on machine_learning of gene_expression data but the concept and theoretical issues also apply to other high_throughput genomic e g copy_number dna_methylation or proteomic_data in gene_expression analysis it is of great interest to predict or diagnose a disease status e g classify cases versus controls or treatment responders versus non responders because no universally best machine_learning exists in general to fulfill this task multiple models are often constructed with different combinations of features genes different machine_learning as well as different tuning parameters in the methods to choose among such a large number of classifiers models it is common practice to select the model with the smallest cross_validation error_rate called the minimal error classifier mec and report its associated error_rate the mec error_rate is however generally downward biased and an overly optimistic estimator of the true optimal classification error_rate this is because taking the minimum of cross_validation error_rates where the estimates are random_variables will inevitably yield a downward bias such a selection_bias has great adverse_impact in many biomedical pilot_studies with moderate sample_sizes e g n the problem however has often been overlooked in applications for example one can examine the small pilot_data using popular machine_learning and simultaneously choose among many different numbers of features and tuning parameters in each method this easily increases the number of tested classifiers to several hundreds and selects the mec with a falsely small error_rate because of the selection_bias when the model proceeds to a large cohort validation for translational_research it will likely fail in the molecular taxonomy of breast_cancer international_consortium metabric example that will be demonstrated in section we will show that the mec bias can mistakenly reduce the error_rate from to an overly optimistic in early_and stage_classification i e a error_rate bias many researchers have recognized this problem dupuy and simon recommended to report the estimates to whom correspondence should be addressed y the authors wish it be known that in their opinion the first two authors should be regarded_as for all the classification algorithms not just the minimal error_rate some proposed comparing the minimal error_rate with the median error_rate from the original datasets with permuted class_labels these suggestions however did not provide a real solution proposed an approach to estimate the bias by a multivariate_gaussian assumption between the minimal estimated error_rate and the true minimal error_rate the gaussian assumption is however generally questionable and the method may not be accurate with a small_sample in this article three applicable bias correction methods proposed in the literature will be compared with our proposed inverse power_law ipl method nested cross_validation nestedcv tibshirani tibshirani procedure tt and weighted mean correction methods wmc wmcs tibshirani and tibshirani proposed a simple bias estimation method that is computationally_efficient and could be calculated through a traditional k fold_cross they claimed that the bias is only an issue when p n where p is the number of genes and n is the number of samples the nestedcv proposed by varma and simon introduced another outer loop of cross_validation so that the model_selection stage is wrapped in the training samples of the outer loop this double loop procedure which amounts to nested double leave_one loocv is computationally_expensive with complexity of o n wmc wmcs was proposed as a smooth analytical alternative to nestedcv based on subsampling which yielded a competitive estimate compared with nestedcv at a much lower computational price theoretically according to the tt method does not apply subsampling in the bias correction and its estimation target is the conditional error_rate conditional on the given samples the nestedcv and wmc wmcs and the ipl method we will propose target on the unconditional error_rate by repeated subsampling in addition both nestedcv and wmc wmcs methods target on the error_rate of a wrapper algorithm multiple algorithms and or rules to decide which one shall be used which is slightly different from the mec error_rate we discuss in this article we however compare all methods side_by because biologically they all conceptually aim to correct mec bias from many machine_learning this article is structured as follows we first illustrate the mec bias by a d toy example and discuss its asymptotic theory and statistical_properties the performance of the nestedcv wmc wmcs and tt will be examined a subsampling based ipl method will be proposed for the bias correction and compared with the three existing_methods in both simulated_and in real_data evaluation we will use five gene_expression geo datasets and two large breast_cancer datasets the cancer_genome tcga and metabric 
