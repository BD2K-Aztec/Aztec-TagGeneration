the impact of incomplete knowledge on the evaluation of protein_function a structured output learning perspective motivation the automated functional_annotation of biological macro molecules is a problem of computational assignment of biological_concepts or ontological terms to genes and gene_products a number of methods have been developed to computationally annotate genes using standardized nomenclature such as gene_ontology go however questions remain about the possibility for development of accurate methods that can integrate disparate molecular data as well as about an unbiased evaluation of these methods one important concern is that experimental annotations of proteins are incomplete this raises questions as to whether and to what degree currently available data can be reliably used to train computational_models and estimate their performance accuracy results we study the effect of incomplete experimental annotations on the reliability of performance evaluation in protein_function using the structured output learning framework we provide theoretical_analyses and carry out simulations to characterize the effect of growing experimental annotations on the correctness and stability of performance_estimates corresponding to different types of methods we then analyze real_biological by simulating the prediction evaluation and subsequent re evaluation after additional experimental annotations become available of go term predictions our results agree with previous observations that incomplete and accumulating experimental annotations have the potential to significantly impact accuracy assessments we find that their influence reflects a complex_interplay between the prediction algorithm performance metric and underlying ontology however using the available experimental_data and under realistic assumptions our results also suggest that current large_scale evaluations are meaningful and almost surprisingly reliable assigning function to gene_products is of primary importance in biology yet with the overwhelming abundance of sequence_data in the post genomic era only a small fraction of gene_products have been annotated experimentally therefore it has become important in computational_biology to predict function based on sequence structure and other data when experimental annotations are unavailable with the development of a large number of function prediction_methods there is a need for unbiased assessment of these methods community_based challenges such as mousefunc and the critical assessment of functional_annotation cafa have emerged to address this problem through the prediction of ontological annotations the objective in the cafa challenge for example is to predict a set of gene_ontology go terms associated with a given protein where go is a hierarchical knowledge representation of functional descriptors terms organized in a large directed_acyclic to evaluate the performance of function prediction_methods properly a set of metrics needs to be established at the same time an important challenge we face when assessing the performance of these methods is that because of the incremental nature of scientific discovery our knowledge of any given proteins function is likely to be partial therefore a function prediction that is originally assessed as a false_positive may be discovered to be a true_positive at a later stage and similarly a prediction that is initially assessed as a true negative may later be discovered to be a false_negative this problem of incomplete_data has led to doubts regarding the reliability of evaluations of protein_function prediction_algorithms the problem of incomplete_data in the training and assessment of classifiers has been recognized both in computational_biology and machine_learning huttenhower et_al have concluded that the effect of missing annotations can produce misleading evaluation results because the classifiers are differentially impacted this results in the re ranking of classifiers upon reevaluation at a time when more experimental annotations are available similarly studied the problem in the framework of asymmetric class label noise in which negative_examples contain some mislabeled data_points however both studies only considered a binary_classification scenario e g when a predictor is developed for a particular term in the ontology here we study the effect of incomplete experimental annotations on the quality of performance assessment of protein_function prediction_methods to that end we consider protein_function as a structured output learning_problem in which a classifier is expected to output a totality of interdependent go terms for a given sequence we consider both topological and information_theoretic metrics and analytically derive under what conditions the initial performance evaluation will underestimate or overestimate the true accuracy then we provide simulations to characterize the impact for different types of predictors finally we analyze experimental protein_function data by simulating the cafa experiment our results regarding the potential impact of incomplete_data on correctness of evaluation largely agree with previous_studies however under to whom correspondence should be addressed the author published_by this is an open_access the terms of the creative_commons http creativecommons org_licenses which permits non commercial re use distribution and reproduction in any medium provided the original_work for commercial re use please_contact permissions_oup com realistic assumptions we provide evidence that the impact of missing_data on reliable_evaluation is surprisingly small as a result this study provides confidence that large_scale performance evaluation of protein_function predictors is useful in addition our study raises concerns about potentially different conclusions that can be reached when protein_function is studied as a series of binary classifiers versus using a structured output learning formulation figures and show the performance of each method in the prrc and ru mi planes separately for each of the three classifications in go here the incomplete_data evaluations for each method are shown as solid gray curves on the other hand color_coded curves are used to visualize the performance on incomplete_data solid curves full circles and complete data dotted curves empty circles on the subsets of proteins that acquired new annotations in the periods from to green to blue and to red note that the complete data evaluations on the entire set of proteins are not shown as the curves would overlap solid gray curves tables summarize the performance evaluation using f max and s min after a year period during which new annotations were allowed to accumulate in the swiss_prot database the results shown_inand and tables provide evidence that the effects of incomplete annotations are ontologyspecific metric specific and algorithm specific the impact on f max was relatively small within percentage_points on the entire dataset and within percentage_points when the evaluation was restricted to the proteins that accumulated new annotations the impact on s min on the other hand was larger and suggests that the initial evaluations are consistently overestimating the quality of performance the overall change of s min in the year period was within bit of information for molecular_function and cellular_component classifications and within bits for biological_process when the evaluation was restricted to the subset of proteins that accumulated new annotations this difference became more significant within bits for molecular_function and cellular_component and within bits for biological_process 
