systems_biology an integrated strategy for prediction uncertainty analysis motivation to further our understanding of the mechanisms_underlying biochemical_pathways mathematical_modelling is used since many parameter_values are unknown they need to be estimated using experimental_observations the complexity of models necessary to describe biological_pathways in combination with the limited amount of quantitative data results in large parameter_uncertainty which propagates into model_predictions therefore prediction uncertainty analysis is an important topic that needs to be addressed in systems_biology modelling results we propose a strategy for model_prediction uncertainty analysis by integrating profile likelihood analysis with bayesian_estimation our method is illustrated with an application to a model of the jak_stat signalling_pathway the analysis identified predictions on unobserved variables that could be made with a high_level of confidence despite that some parameters were non identifiable availability_and source_code is available at mathematical_modelling is used to integrate hypotheses about a biochemical_network in such a manner that such networks can be simulated in addition to the formulation and testing of biochemical_properties computational_models are used to predict unmeasured behaviour despite great advances in measurement techniques the amount of data is still relatively scarce and therefore parameter_uncertainty is an important research_topic we focus on biochemical_networks modelled using ordinary_differential odes such models consist of equations which contain parameters p inputs u t and state_variables x t in many cases these systems are only partially observed which means that measurements y t are performed on a subset or a combination of the total number of states n in the model this results in a mapping from an internal_state to an output additionally these measurements are hampered by noise moreover many techniques used in biology e g western_blotting necessitate the use of scaling and offset parameters q for ease of notation we define which lists all the parameters that should be defined in order to simulate the model to whom correspondence should be addressed considering m time series of length n i with additive independent gaussian_noise we can obtain for the probability_density of the output data in this equation y t represents the true system with true parameters t whereas i j indicates the sd of a specific datapoint and k serves as a normalization constant in maximum_likelihood mle the goal is to find model_parameters for which the probability_density most likely produced the data in mle one attempts to maximize the likelihood_function l y d whose formula is identical to a second formalism commonly_applied to inferential problems is known as bayesian_inference in contrast to mle bayesian_inference does attach a notion of probability to the parameter_values applying bayes theorem to the parameter_estimation problem we obtain since the probability of the data does not depend on the parameters it merely acts as a normalizing constant the posterior probability_distribution is given by normalizing the likelihood multiplied with the prior to a unit area whereas mle tends to focus on estimating best fit parameters the bayesian methodology attempts to elucidate posterior parameter probability_distributions in this article we provide a strategy for uncertainty analysis consisting of multiple_steps by performing these steps sequentially we show how to avoid problems associated with the different techniques we illustrate our approach using a model of the jak_stat the model is based on a number of hypothesized steps see the supplementary_material for model equations first erythropoietin epor activates the epor receptor which phosphorylates cytoplasmic stat x this phosphorylated_stat x dimerizes x and is subsequently imported into the nucleus x here dissociation and dephosphorylation occurs which is associated with a delay similar to the implementation given in the original article the driving input_function was approximated by a spline interpolant while the delay was approximated using a linear_chain approximation we used data from the article by for parameterization and inference observables were the total concentration of stat and the total concentration of phosphorylated_stat in the cytoplasm both reported in arbitrary_units thereby requiring two additional scaling parameters s and s the initial cytoplasmic concentration of stat is unknown while all other forms of stat are assumed zero at the start of the simulation the vector of unknown parameter_values consists of the elementsin order to investigate the existence of multiple_modes we first performed a large_scale search using mcmm with initial parameters and their associated wrsss dots note that all of the optimized parameter_sets shown are acceptable with respect to the lr ratio right model_predictions from parameter_sets taken from location a b and c for two measured outputs as well as one unmeasured internal_state based on a log uniform random_sampling between the ranges and n after optimization samples are either accepted or rejected based on the lr bound based on the best fit value the resulting distribution and associated wrss are shown in it is clear that there are at least three local_minima in the likelihood although all three modes describe the data adequately they show different prediction results for the unobserved internal_states of the model subsequently a pl analysis was performed in order to increase confidence that no acceptable regions of parameter_space were missed we started pls from each mode detected using the mcmm method step subsequently we merged these profiles and verified whether they covered the full span of acceptable parameter_sets obtained in step based on the pl shown in the top panel of it can be concluded that the model based on first principles is structurally non identifiable from scatter plots of the pls shown in the supplement it was determined that the parameters x s and s were structurally_related and therefore unidentifiable analogously to we specify a gaussian prior nm nm for the initial condition which is comparable to assuming that the initial concentration was measured with this accuracy in order to check whether the prior affects the profiles in the desired manner one can compute new profiles using map estimation by incorporating the prior in the procedure in our case the gaussian prior constrains both the initial condition as well as both scaling_factors see we can also observe that parameter p is practically non identifiable at in the case of jak_stat at least three priors are required to render the model identifiable for all levels of significance as the name suggests priors based on prior belief are preferred however in many cases little is known beforehand regarding the parameters of a system for the initial condition we specify a gaussian priorin this article we proposed a new strategy for prediction uncertainty analysis by performing pl analysis we were able to specify sufficient priors to ensure that the posterior_distribution was proper and could be sampled from using mcmc a sample of parameter_sets proportional to the probability density of that parameter set was obtained the strategy enables a comprehensive analysis on the effect of parameter_uncertainty on model_predictions and enables the modeller to relate these effects to the model_parameters given a sufficient amount of data such an analysis should be relatively insensitive to the assumed priors as observed in the case of jak_stat however it can be seen that even for a small model identifiability can be problematic it is important to realize that in such cases the choice of priors will affect the outcome of the analysis furthermore most priors are not re parameterization invariant and therefore uniform priors do not reflect complete ignorance although seemingly uninformative a uniform prior in untransformed parameter_space implies that extremely_large rates have an equal a priori probability of occurring than slow rates in our case for the completely unknown kinetic_parameters we assumed a uniform prior in logarithmic space for positively defined parameters a uniform_distribution in logarithmic space corresponds to an uninformative prior such a prior gives equal probability to different orders_of scales an approximate scale_invariance of kinetic_parameters has indeed been observed in biological_models note that in a bayesian_analysis there is no such thing as not specifying a prior our strategy can be used to gain insight into prediction uncertainty note however that aside from the computational_model and the prior distributions the noise model also affects the resulting posterior_distribution it should be stressed that investigating what kind of noise model to use when and subsequently determining the appropriate likelihood_function for this noise model is important practical solutions to non additive noise can usually be found one example would be a multiplicative noise model which is often associated with non negative data where data preprocessing such as taking the logarithm of both the model and data can help alleviate problems if the likelihood_function truly becomes intractable then one can resort to approximate bayesian_methods where rather than computing the likelihood_function one computes a distance_metric between simulations with simulated noise and data when the goal of prediction uncertainty analysis is model falsification then one could opt for an approach based on interval analysis b in these works uncertainty analysis is reformulated into a feasibility problem using this approach regions of parameter_space that cannot describe the data can systematically be determined an attractive aspect of these methods is that these methods provide guarantees on finite parameter searches but have up to this point only been performed on small_scale models different approaches for prediction uncertainty analysis based on optimization are proposed in such methods are useful for probing consistent behaviour termed core predictions among multiple parameter_sets even in the non identifiable case however they do not result in a probabilistic assessment of the prediction uncertainty probing consistent behaviour is also the main focus of a workflow proposed by for classifying consistent model behaviours and hypotheses several steps in the proposed approach are computationally_challenging and require many model evaluations because of this model_simulation time is a primary_concern many packages including ours have been able to attain significant simulation speed ups by compiling simulation code reducing model evaluation time by up to two orders_of potters wheel copasi sloppy cell additionally new computational platforms such as general_purpose programming on the graphical processing unit are being explored in conclusion our strategy enables the modeller to account for parameter_uncertainty when making model_predictions in the case of a fully identifiable model we can work with uninformative priors and overconfident conclusions that could result from a model described by a single_parameter set can be avoided regarding nonidentifiable models a practical approach can be adopted where the dependence with respect to the assumed prior distributions can be determined a posteriori note that though this makes computing the posterior_distribution feasible such an approach underestimates the parameter_uncertainty performing the analysis and obtaining a sample from the posterior takes considerably more computational effort than determining a single_parameter set however once such a sample is obtained the results can be used for a wide_array of model analysis techniques which more than warrants the additional computational time invested relations within this posterior_distribution and also its relation to the posterior predictive 
