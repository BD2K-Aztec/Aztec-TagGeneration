benchmark analysis of algorithms for determining and quantifying full_length mrna splice_forms from rna_seq motivation because of the advantages of rna_sequencing rna_seq over microarrays it is gaining widespread popularity for highly_parallel gene_expression analysis for example rna_seq is expected to be able to provide accurate identification and quantification of full_length splice_forms a number of informatics packages have been developed for this purpose but short_reads make it a difficult problem in principle sequencing_error and polymorphisms add further complications it has become necessary to perform studies to determine which algorithms perform best and which if any algorithms perform adequately however there is a dearth of independent and unbiased benchmarking studies here we take an approach using both simulated and experimental benchmark data to evaluate their accuracy results we conclude that most methods are inaccurate even using idealized data and that no method is highly_accurate once multiple splice_forms polymorphisms intron signal sequencing_errors alignment errors annotation errors and other complicating_factors are present these results point to the pressing need for further algorithm development availability_and simulated_datasets and other supporting_information can be found atwe first fix some terminology for our purposes a gene is a collection of transcripts also called splice_forms a transcript is a collection of exons an exon is a contiguous span of genomic_coordinates two splice_forms of the same gene can and usually do have some of the same exons exons which overlap but have different start and or end location are also possible two different splice_forms may share all of their exons as long as at least one of them differs by their start end coordinates typically for any given gene in any given cell some of its splice_forms are expressed and others are absent one of the primary_goals of high_throughput rna_seq is to accurately identify the full_length structure of the transcripts that are present and their relative_abundances so that researchers can focus on the most relevant splice_forms in their system of interest this is a very difficult problem however most human and mouse genes have many exons left and are annotated with multiple splice_forms we observe that of genes may express at least two forms under normal conditions see right as methods improve and the number of tissues that are deep sequenced increases the annotation will only get more complex most algorithms which rely on a genome alignment can be run in two possible modes depending on whether or not they utilize transcript model annotation in the first mode the algorithm determines which of the annotated forms are expressed and then further uses the annotation as a guide to determine novel forms the second mode attempts to determine the forms of the expressed transcripts from scratch with no transcript annotation provided the latter is a considerably harder problem annotation is never perfect to model this for dataset t we hid roughly of the expressed transcripts at random and replaced each one with a different unexpressed transcript of the same gene unexpressed transcripts that are called as expressed constitute the false_positives for ep and er also half of the expressed transcripts were hidden however those datasets follow a realistic spectrum ofbenchmark analysis of algorithmsexpression so that approximately one third of the transcripts are not expressed therefore it is not necessary to introduce further unexpressed transcripts as was done for t supplementary tables s s give a summary of all of the available figures although some algorithms perform well with perfect data and a single splice form they tend to have difficulty predicting multiple splice_forms represents the most ideal case the top row represents the correctly annotated transcripts in this case a false_positive is an annotated transcript that is not expressed but is called expressed and a false_negative is an annotated and expressed transcript that is not called expressed no algorithm should be expected to do this well in practice but it gives a bound on the accuracy even in this case several algorithms are out of the comfort zone in some if not most cases having differing exon start ends splicing category iii gave the algorithms the most difficulty across the board note that the methods were given the perfect alignment so the category iii issues are not due to alignment artifacts in the lower panel we see the accuracy for incorrectly annotated transcripts here a false_positive is an inferred un annotated transcript that is not expressed and a false_negative is any of the of hidden transcripts that are expressed that were not called as expressed most methods perform well on the genes with one transcript but the accuracy rates for unannotated transcripts are low across the board when the data are aligned with tophat or star the error_rates naturally increase supplementarys s s as expected the de_novo methods performed_worse than the genome alignment guided methods supplementary we conclude that although use without annotation is a common and intended application the error_rates of all algorithms on real_data are high ep and er represent more realistic datasets because they contain the full complexity of the ensembl annotation and they span a range of expression levels similar to real_data additionally er contains polymorphisms in the form of substitutions and indels sequence error and intron signal because in practice algorithms will not have access to perfect annotation the gene models provided were modified models whereby of the expressed transcripts were hidden shows the results when the algorithms are provided with perfect alignments the top row represents properly annotated transcripts and the bottom row represents hidden transcripts the statistics are stratified by depth of coverage in this case on correctly annotated transcripts cufflinks and stringtie stay in the comfort zone in most cases with cufflinks tending to have better precision and stringtie better recall it is notable that often precision goes down with depth of coverage the reason for this is that with more reads most algorithms find more ways to go wrong without annotation very few data_points of any category are in the comfort zone supplementary only cufflinks and stringtie appear to be potentially viable there will of course also be other un modeled biases and factors in real_data such as position specific_biases so these bounds on the accuracy are certainly quite conservative the results when tophat and star are used instead of a perfect alignment are given in the supplementary figures s s again the de_novo methods underperform the alignment_based supplementary we turn next to the ivt data to obtain information on the impact of the other effects of real sequencing that we could not model in this case since this is real sequence_data we could not provide an error_free alignment the annotation given was of the known structure of the transcripts for the sake of comparison from these gene models we also generated ideal simulated_data based on these gene models rightmost panel apparently polya selection caused more problems across the board as compared with ribozero unfortunately however the vast_majority of rna_seq is being generated with the polya selection protocol these results are accuracy results for simulated dataset t for the methods which utilize a reference_genome this represents the most ideal case where all genes are highly_expressed there are no polymorphisms and there are no alignment errors splicing is divided_into three types the only cases where precision was above in the first two types are when there is a single splice form the analysis was run with gene_annotation provided an order of magnitude worse than for the er data which speaks to the complications introduced by un modeled factors for the genes with more than one splice form the results were worse still note that we get a different separation of methods in the simulated_data right panel compared with the real_data left and center panels this indicates comparisons based on simulated_data do not necessarily indicate which methods to prefer to assess the quality of the inferred quantified fpkm values we computed the pearson_correlation between the true fpkm and the inferred values the true fpkm was determined not from the theoretical intensity of the transcripts but from the actual true number of reads that came from each transcriptexonic reads only we filtered out extreme outliers we also removed all cases where the true expression is zero but the algorithm gave it positive expression or where the true expression is positive and the algorithm gave it zero we call these on off errors if we do not do this then all correlations are very low and uninformative we separated out how often the on off errors occur and graphed them separately we did not report outlier statistics since there was only a handful but some were extreme e g cufflinks gives an fpkm value in the ranges for one gene that is not expressed at all these results provide objective and conservative bounds on the error_rates of transcript inference and quantification algorithms and found that none of them can be considered highly_accurate one has to decide if chasing differential splicing is worth the potential disadvantages of dealing with many false_positives in the downstream_analysis our experience is that most groups are after evident effects meaning moderately_high fold_change of moderately to highly_expressed and well annotated_genes however many such groups follow the path of transcript_level analysis because it has been presented as the standard thing to do some groups certainly must worry about alternate splicing however most groups would find what they are looking for by performing a much more straight_forward gene level analysis and a significant portion of those who need more than a gene level analysis wouldfind exon_intron junction level analysis sufficient therefore employing the current state of transcript_level analysis should only be done with considerable forethought transcript structure_determination either de_novo or with annotation is a challenging_problem to assess how the most recent versions of algorithms perform we devised a set of tests using simulated and in vitro transcribed rna_seq we generated one clean dataset t simulated from genes with one to five splice_forms per gene t used paired_end base per end sequencing as this is ideal data it has a perfect representation of all bases no sequencing or mapping errors and no polymorphisms we also simulated a realistic dataset er from ensembl transcripts we generated this dataset with polymorphisms error_rates and intron signal transcripts follow a distribution of intensities at rates consistent with what is typically observed in quality data in practice finally we used in vitro transcription data for full_length human cdnas where mapping errors uneven base representation and polymorphisms are unavoidable with all of these datasets we know the truth making them informative for evaluating the performance of transcript assembly algorithms on perfect data t with a single splice form as in most algorithms perform reasonably well with fairly high_precision all algorithms seem to be optimized to detect exon_skipping events while of the three types of splicing investigated variable_length exons present the greatest challenge detecting truncated genes on the other hand has high_precision across the board but the recall of many algorithms suffered with clean data and just two forms per gene the error_rates for all algorithms go up considerably if one must do transcript_level analysis then cufflinks and stringtie are among the best performers in summary all algorithms designed to delineate transcript forms tend to make many false_discoveries even on perfect data the ivt data ivt are perhaps the most informative control dataset because rather than modeling errors alignment polymorphisms base representation it simply has them as they occur naturallyyet we still know the ground_truth in terms of what the true transcript models are error_rates with this data are considerably_higher even though of genes had only a single form put simply on this real benchmark data with of transcripts having only a single form all algorithms predict at least as many false transcripts as true ones and at best roughly or more of the true ones were missed in the ivt data we also had a limited subset of genes with more than one transcript on this limited set of real_data performance was yet worse it is instructive to look at what aspects of the popular algorithms need improvement in order to increase their accuracy cufflinks uses junction and coverage_information to determine local alternative_splicing however it often assembles the local information into global transcripts in all possible ways that are consistent with the short_reads and this is the source of many of the false_positives because the integrity of the transcripts is lost in this approach it is not clear what advantage it could have over a more local analysis such as at the exon_intron junction level another problem is that it does not effectively sort out exon signal from intron signal in most rna_seq there is some amount of signal at almost every base of almost every intron this intron signal is often assembled into long exons resulting in more false_positives as seen in the er dataset perhaps reading_frame information could be used to limit this type of artifact there is also generally a smattering of junctions that connect exons to the middle of introns yet these probably do not represent real exon exon_junctions the challenge is for algorithms to find a way to ignore the intron and spurious junction signal to get at the clean transcript models scripture uses an even more liberal approach by essentially ignoring junction information altogether and instead taking a peak finding approach to identify putative exons and then connecting them in all possible ways this approach also suffers from the fact that there is insufficient_information to connect local inferences into global inferences and so it constructs a large_number by joining local effects to make full_length transcripts we have also observed that scripture tends to find many peaks in introns where the signal was generated via a uniform model and so should not have any significant peaks the extreme overcalling of forms makes it unclear how to utilize the output of scripture in a practical way the de_novo methods such as trinity are trying to solve a much harder problem by not using the information coming from the genome_sequence if no genome exists then this approach might provide information that can be informative however for model_organisms and many others for which a reference_genome is available and which have some degree of quality community annotation it is hard to see any benefit of using a de_novo approach we have seen inregions that present a particular challenge to transcript_level analysis indeed it is not even clear that these are true splicing_events and not just artifacts of sequencing or alignment so the first task in using rna_seq to perform transcript_level analysis should be to determine what part of these challenging regions represents true biology that is worth quantifying and what part of it is artifactual in a survey of random papers from pubmed that perform rna_seq studies and for which some method of feature quantification was employed we found transcript_level inference is quite popular being done more than half the time in particular cufflinks is cited times and trinity is cited times a couple methods are so new that we do not expect citations yet such as stringtie many other papers do not perform transcript inference but still attempt to do transcript_level analysis via methods which simply assign quantifications to a fixed set of annotated transcripts in particular out of the publications surveyed htseq is cited times rsem times simple counting times and clcbio genomics workbench times only a small minority of groups work with exon or gene level analysis however our results indicate that such an analysis may often be much more practical and efficient investigators using rna_seq want to know which transcripts are present and their expression levels full_length transcript reconstruction from short_read has emerged as a potential solution to the problem however short_reads are noisy and fundamentally lack the information necessary to build globally accurate transcripts despite this several algorithms have gained widespread usage underscoring the importance of more research into this problem most likely a satisfactory solution will involve an evolution not just in the algorithms but in the nature of the data it is likely this problem will not improve to the point of being practical until much longer_reads are available and until the ribosomal depletion protocols improve finally it remains possible that some keen insight into how to identify and effectively utilize signals in the genome could emerge that helps to solve this problem regardless benchmarking studies such as those presented here will remain a critical component to realizing these important goals 
