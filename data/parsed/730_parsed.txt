genome_analysis bigdatascript a scripting language for data pipelines motivation the analysis of large biological_datasets often requires complex processing pipelines that run for a long time on large computational infrastructures we designed and implemented a simple script like programming_language with a clean and minimalist syntax to develop and manage pipeline execution and provide robustness to various types of software and hardware failures as well as portability results we introduce the bigdatascript bds programming_language for data_processing pipelines which improves abstraction from hardware resources and assists with robustness hardware abstraction allows bds pipelines to run without modification on a wide_range of computer architectures from a small laptop to multi core servers server farms clusters and clouds bds achieves robustness by incorporating the concepts of absolute serialization and lazy processing thus allowing pipelines to recover from errors by abstracting pipeline concepts at programming_language level bds simplifies implementation execution and management of complex bioinformatics_pipelines resulting in reduced development and debugging cycles as well as cleaner code availability_and bigdatascript is available under open_source at http pcingola github io bigdatascript processing large_amounts of data is becoming increasingly important and common in research environments as a consequence of technology improvements and reduced_costs of highthroughput experiments this is particularly the case for genomics research programs where massive parallelization of microarray and sequencing based assays can support complex genome_wide experiments involving tens or hundreds of thousands of patient samples with the democratization of high_throughput and simplified access to processing resources e g cloud_computing researchers must now routinely analyze large_datasets this paradigm_shift with respect to the access and manipulation of information creates new challenges by requiring highly_specialized skill such as implementing data_processing pipelines to be accessible to a much wider audience a data_processing pipeline referred as pipeline for short is a set of partially ordered computing tasks coordinated to process large_amounts of data each of these tasks is designed to solve specific parts of a larger problem and their coordinated outcomes are required to solve the problem as a whole many of the software_tools used in pipelines that solve big_data genomics problems are cpu memory or i o intensive and commonly run for several hours or even days creating and executing such pipelines require running and coordinating several of these tools to ensure_proper data flow and error control from one analysis step to the next for instance a processing pipeline for a sequencingbased genome_wide may involve the following steps i mapping dna_sequence obtained from thousands of patients to a reference_genome ii identifying genetic changes present in each patient genome known as calling variants iii annotating these variants with respect to known gene_transcripts or other genome landmarks iv applying statistical_analyses to identify genetic_variants that are associated with differences in the patient phenotypes and v quality_control on each of the previous steps even though efficient tools exist to perform each of these steps coordinating these processes in a scalable robust and flexible pipeline is challenging because creating pipelines using general_purpose computer languages e g java python or shell scripting involves handling many low_level process synchronization and scheduling details as a result process coordination usually depends on specific features of the underlying systems architecture making pipelines difficult to migrate for example a processing pipeline designed for a multi core server cannot directly be used on a cluster because running tasks on a cluster requires queuing them using cluster specific commands e g qsub therefore if using such a language programmers and researchers must spend significant efforts to deal with architecture specific details that are not germane to the problem of interest and pipelines have to be reprogrammed or adapted to run on other computer architectures this is aggravated by the fact that the requirements change often and the software_tools are constantly_evolving in the context of bioinformatics there are several frameworks to help implement data_processing pipelines although a full comparison is beyond the scope of this article we mention a few that relate to our work i snakemake k oster and written as a python domain specific language dsl which has a strong influence from make command just as in make the workflow is specified by rules and dependencies are implied between one rules input_files and another rules output_files ii ruffus a python_library uses a syntactic mechanism based on decorations this approach tends to spread the pipeline structure throughout the code making maintenance cumbersome which is also written as a python_library expresses pipelines as graphs drawn using ascii characters although visually rich the authors acknowledge that this representation is harder to maintain than the traditional code iv bpipe is implemented as a dsl on top of groovy a java virtual_machine jvm based language bpipe facilitates reordering removing or adding pipeline stages and thus it is easy for running many variations of a pipeline v nextflow www nextflow io another groovy based dsl is based on data flow programming paradigm this paradigm simplifies parallelism and lets the programmer focus on the coordination and synchronization of the processes by simply specifying their inputs and outputs each of these systems creates either a framework or a dsl on a pre_existing general_purpose programming_language this has the obvious benefit of leveraging the languages power expressiveness and speed but it also means that the programmer may have to learn the new general_purpose programming_language which can be taxing and take time to master some of these pipeline tools use new syntactic structures or concepts e g nextflows data flow programming model or leafs pipeline drawings that can be powerful but require programming outside the traditional imperative model and thus might create a steep learning_curve in this article we introduce a new pipeline programming_language called bigdatascript bds which is a scripting language designed for working with big_data pipelines in system architectures of different sizes and capabilities in contrast to existing frameworks which extend general_purpose languages through libraries or dsls our approach_helps to solve the typical challenges in pipeline programming by creating a simple yet powerful and flexible programming_language bds tackles common problems in pipeline programming by transparently managing infrastructure and resources without requiring explicit code from the programmer although allowing the programmer to remain in tight_control of resources it can be used to create robust pipelines by introducing mechanisms of lazy processing and absolute serialization a concept similar to continuations that helps to recover from several types of failures thus improving robustness bds runs on any unix like environment we currently provide linux and os x pre compiled binaries and can be ported to other operating_systems where a java runtime and a go compiler are available unlike other efforts bds consists of a dedicated grammar with its own parser and interpreter rather than being implemented on top of an existing language our language is similar to commonly used syntax and avoids inventing new syntactic structures or concepts this results in a quick to learn clean and minimalistic language furthermore creating our own interpreter gives better control of pipeline execution and allows us to create features unavailable in general_purpose language most notably absolute serialization this comes at the expense of expressiveness and speed bds is not as powerful as java or python and our simple interpreter cannot be compared with sophisticated just in time execution or jvm optimized bytecode execution provided by other languages nonetheless in our experience most bioinformatics_pipelines rely on simple programmatic constructs furthermore in typical pipelines the vast_majority of the running time is spent executing external programs making the executing time of the pipeline code itself a negligible factor for these reasons we argue that bds offers a good trade_off between simplicity and expressiveness or speed to illustrate the use of bds in a real_life scenario we present an implementation of a sequencing_data analysis_pipeline this example illustrates three key bds properties architecture independence robustness and scalability the data we analyzed in this example consist of high_quality short_read coverage of a human_genome corresponding to a person of european_ancestry from utah na downloaded from illumina platinum genomes http www illumina com platinumgenomes the example pipeline we created follows current best practices in sequencing_data analysis which involves the following steps i map reads to a reference_genome using bwa ii call variants using gatks haplotypecaller and iii annotate variants using snpeff and snpsift the pipeline makes efficient use of computational_resources by making sure tasks are parallelized whenever possible shows a flowchart of our implementation while the pipelines source_code is available at include bio seq directory of our projects source_code https github com pcingola bigdatascript architecture independence we ran the exact same bds pipeline on i a laptop computer ii a multi core server cores gb shared ram iii a server farm servers cores each iv a core cluster and v the amazon aws cloud_computing infrastructure for the purpose of this example and to accommodate the fact that running the pipeline on a laptop using the entire dataset would be prohibitive we limited our experiment to reads that map to chromosome thethe script is executed from a terminal the go executable invokes main bds written in java performs lexing parsing compilation to ast and runs ast c when the task statement is run appropriate checks are performed d a shell script task sh is created and a bds exec process is fired e bds exec reports pid executed the script task sh while capturing stdout and stderr as well as monitoring timeouts and os signals when a process finishes execution the exit status is logged architectures involved were based on different operating_systems and spanned about three orders_of in terms of the number of cpus from to and ram from gb to tb bds can also create a cluster from a server farm by coordinating raw ssh connections to a set of computers this minimalistic setup only requires that the computers have access to a shared disk typically using nfs which is a common practice in companies and university networks in all cases the overhead required to run the bds script itself accounted for ms per task which is negligible compared with typical pipeline runtimes of several hours robustness to assess bdss robustness we ran the pipeline on a cluster where of the nodes have induced hardware failures as opposed to software failures which are usually detected by cluster management systems hardware node failures are typically more difficult to detect and recover from in addition we elevated the cluster load to to make sure the pipeline was running on less than ideal_conditions as shown in the pipeline finished successfully without any human intervention and required only more time than in the ideal case scenario because bds had to rerun several failed tasks this shows how bds pipelines can be robust and recover from multiple failures by using lazy processing and absolute serialization mechanisms scalability to assess bdss scalability we ran exactly the same pipeline on two datasets that vary in size by several orders_of i a relatively small dataset chromosome subset gb that would typically be used for development testing and debugging and ii a high depth whole_genome dataset over coverage roughly tb we introduced bds a programming_language that simplifies implementing testing and debugging complex data analysis_pipelines bds is intended to be used by programmers in a similar way to shell scripts by providing glue for several tools to ensure that they execute in a coordinated way shell scripting was popularized when most personal_computers had a single_cpu and clusters or clouds did not exist one can thus see bds as extending the hardware abstraction concept to data center level while retaining the simplicity of shell scripting bds tackles common problems in pipeline programming by abstracting task management details at the programming_language level task management is handled by two statements task and wait that hide system architecture details leadingnotes running the same bds based pipeline a sequence variant_calling and analysis_pipeline on the same dataset chr but different architectures operating_systems and cluster management systems notes the same sample pipeline run on dataset of gb reads mapping to human_chromosome and tb whole_genome data_set computational times vary according to systems resources utilization factor and induced hardware failures to cleaner and more compact code than general_purpose languages bds also provides two complementary robustness mechanisms lazy processing and absolute serialization a key feature is that being architecture agnostic bds allows users to code test and debug big_data analysis_pipelines on different systems than the ones intended for full scale data_processing one can thus develop a pipeline on a laptop and then run exactly the same code on a large cluster bds also provides mechanisms that eliminate many boilerplate programming tasks which in our experience significantly reduce pipeline development times bds can also reduce cpu usage by allowing the generation of code with fewer errors and by allowing more efficient recovery from both software and hardware failures these benefits generally far outweigh the minimal overhead incurred in typical pipelines 
