data_and a user oriented web crawler for selectively acquiring online content in e health research motivation life stories of diseased and healthy_individuals are abundantly available on the internet collecting and mining such online content can offer many valuable insights into patients physical and emotional_states throughout the pre diagnosis diagnosis treatment and post_treatment stages of the disease compared with those of healthy_subjects however such content is widely dispersed across the web using traditional query based search_engines to manually collect relevant materials is rather labor_intensive and often incomplete due to resource constraints in terms of human query composition and result parsing efforts the alternative option blindly crawling the whole web has proven inefficient and unaffordable for e health researchers results we propose a user oriented web crawler that adaptively acquires user desired content on the internet to meet the specific online data source acquisition needs of e health researchers experimental_results on two cancer related case studies show that the new crawler can substantially accelerate the acquisition of highly_relevant online content compared with the existing state of the art adaptive web crawling technology for the breast_cancer case study using the full training_set the new method achieves a cumulative precision between and after h of execution till the end of the h long crawling session as compared with the cumulative precision between and using the peer method for the same time period for the lung_cancer case study using the full training_set the new method achieves a cumulative precision between and after h of execution till the end of the h long crawling session as compared with the cumulative precision between and using the peer method using the reduced training_set in the breast_cancer case study the cumulative precision of our method is between and whereas the cumulative precision of the peer method is between and for the lung_cancer case study using the reduced training_set the cumulative precisions of our method and the peer method are respectively between and versus between and these numbers clearly show a consistently superior accuracy of our method in discovering and acquiring user desired online content for e health research availability_and the implementation of our user oriented web crawler is freely_available to non commercial users via the following web_site http bsec ornl gov adaptivecrawler shtml the web_site provides a step_by guide on how to execute the web crawler implementation in addition the web_site provides the two study datasets including manually labeled ground_truth initial seeds and the crawling results reported in this article the internet carries abundant and ever enriching user generated content on a wide_range of social cultural political and other topics life stories of patients are no exception to this trend collecting and mining such personal content can offer many valuable insights on patients experiences with respect to disease symptoms and progression treatment management side_effects and effectiveness as well as many additional factors and aspects of a patients physical and emotional_states throughout the whole disease cycle the breadth and depth of understanding attainable through mining this voluntarily contributed web content would be extremely expensive_and to capture via traditional data_collection mechanisms used in clinical studies despite the merits and rich availability of user generated patient content on the internet collecting such information using conventional query based web search is labor_intensive for the following two reasons first it is not clear what are the right queries to use to retrieve the desired content accurately and comprehensively for example a general query such as breast_cancer stories would pull up over million results using google web search wherein only a selected portion usually small such as of the whole search result set may meet the researchers specific needs manually examining and selecting the qualified search results require extensive human effort second clinical_researchers have specific requirements regarding the usergenerated disease content they need to collect query based search_engines cannot always support such requirements let us assume that a researcher wants to collect the personal stories of two groups of female breast_cancer patients those who have had children and those who have not with much manual effort the researcher might be able to obtain some stories of the first group but so_far no off_the general_purpose search_engine that we are aware of allows users to retrieve information that does not carry undesirable content i e the support of negative queries given the steadily growing volume of patient generated disease specific online content it is highly_desirable to minimize to whom correspondence should be addressed y the authors wish it to be known that in their opinion the first two authors should be regarded_as the manual intervention involved in source acquisition and subsequent mining processes although an extensive collection of automatic or largely automatic text_mining algorithms and tools exists for analyzing social_media content limited efforts have been dedicated to developing automatic or largely automatic content acquisition tools and methods for obtaining online patient generated content meeting certain e health research needs and requirements to meet this challenge in the e health research_community as well as the broader bioinformatics communities we propose a user oriented web crawler which can acquire user generated content satisfying particular content requirements with minimum intervention we use cancer as the case study to demonstrate the value and impact of the proposed web crawling technology comparing the design of our newly_proposed adaptive crawler with that of the peer method presented in barbosa and freire there exist four key aspects of similarities between the two approaches as follows i both methods are designed with an online learning capability which automatically learns on thefly to capture characteristics of promising crawling paths or destination web regions that lead to the most rewarding discoveries of online content relevant to the current crawling needs ii both methods are equipped with a randomized link visitation strategy where the likelihood of selecting a certain link from the candidate url pool for visit in the next crawling step is probabilistically modulated by the estimated reward that the link can lead to the discovery of relevant content iii both methods are equipped with some self assessment and self critic module that can autonomously determine the relevance of any harvested web_page with respect to the crawling needs for selective output of crawling results and iv both methods extract text features and select the most reliable subset of candidate features to construct the predictive estimator on the crawling utility of a link the key differences between the two methods include i to forecast the utility of a link u i our method introduces a lightweighted learner wpu i that predicts the utility score before crawling the web content pointed to by u i according to the information provided by the snippet associated with u i such snippet is always available for search results returned by a typical search_engine such as google and yahoo the snippet includes u i s url a running head text of u i and a brief piece of selected text from the search result associated with u i in comparison the adaptive link learner introduced in the peer method only examines the text information_encoded in a links url when performing the link utility estimation the extended scope of textual information available from the snippet of a link allows our predicting function to be able to estimate the link utility more accurately as confirmed by the experimental_results which results in better accuracy in targeted web crawling ii in addition to the lightweighted learner wpu i our method also carries a more powerful web_page utility assessment function wp that measures the utility of a web_page wp with respect to the information need after the web_page is crawled based on the output of the function wp we dynamically retrain the function of wpu i so that the particular machine_learning model_selection and configuration are optimized on the fly according to all the cumulative quality_assessment scores produced by the function wp since the beginning of the current crawling session this novel two tier online learning framework which was not present in the peer method allows our method to be able to train a more tailored and task optimized link utility predictor wpu i in an autonomous fashion iii the peerboth crawling methods were initialized using the same set of labeled samples to explore the influence of the training sample_size we randomly_selected a subset of the available positive_and training samples and repeated the aforementioned comparable analysis c and d method uses a specialized classifier for recognizing online searchable forms as a critic to judge the relevance of their online crawling results this feedback mechanism is only applicable for the particular crawling needs to discover web_pages of searchable forms as hidden entries to online_databases in contrast when assessing the utility of a crawled online content web_page our method comprehensively considers the content words in the main_body of an html file words in the heading and subtitles of an html file and the anchor text embedded in an html file benefited by this more generic design of the self assessment mechanism which is coupled by a corresponding more advanced text feature_extraction selection and predictive_modeling implementation our adaptive crawler is able to detect online content meeting a much wider spectrum of users needs for a more generic scope of applications iv our new crawler design explicitly_models the confidence and reliability of its link utility prediction_performance during the execution of online web crawling and considers such uncertainty during its planning of adaptive strategies for the current crawling task this uncertainty modeling feature is missing from the design of the peer method v our crawler design explicitly_models the time required to access a given web_page for balancing the time spent between developing more carefully_planned crawling strategies versus executing more operations of web_page harvesting with less deliberated crawling strategies such feature is also absent from the design of the peer method to better understand the behavior characteristics of the two crawling methods we conducted some further investigative analysis to comparatively examine crawling results obtained by each crawler for the two case studies reported in this article first we examined the overlap between crawling results harvested by the two crawlers using three metrics which assess the amount of common content between two crawling result sets s and s on the levels of key_words documents and sites respectively for the key word level overlap between s and s we first extracted the key_words from each result set using the rake algorithm next we ranked the two lists of key_words according to each key_words term frequency inverse document frequency value among its corresponding crawling result set we then counted the number of common key_words included in the top_ranked key word_lists of s and s as the key word level overlap between s and s reports results of the estimated key word level overlap between the two sets of crawling results where both crawlers are trained with the full set of example search results shows that the key word level overlap remains roughly in the same value range between and even when estimated using different sizes of top_ranked key_words we then chose a representative window_size for the top_ranked key_words and estimated the key word level overlap throughout the entire crawling sessions the results are shown in according to which we can see the overlap between the two crawling result sets also stably remains within the same value range of and during the progression of the crawling processes for the document level overlap we use the cosine distance to measure pairwise document similarity we then estimated the overlap between s and s as thethe peer method appears to collect online content richer in medical terms whereas the adaptive crawler appears to be able to harvest cancer patient stories that expose more abundant details in lifestyle and emotionally related matters it is noted the latter type of crawling results acquired by our crawler agrees better with the true information collection needs of e health researchers in both case studiesone about gathering life stories of breast_cancer patients and the other about smoking of lung_cancer patients we speculate this better alignment of crawling results with researchers information acquisition needs is achieved by the more content sensitive adaptive crawling mechanism of our new crawler benefited by its more advanced selective online content detection and acquisition algorithm proposed in this article we propose a user oriented web crawler that can adaptively acquire social_media content on the internet to meet the specific online data source acquisition needs of the end_user we evaluated the new crawler in the context of cancer epidemiological_research with two case studies experimental_results show that the new crawler can substantially accelerate the online usergenerated content acquisition efforts for cancer researchers than using the existing state of the art adaptive web crawling technology 
