selecting a classification function for class_prediction with gene_expression data motivation class predicting with gene_expression is widely used to generate diagnostic and or prognostic_models the literature reveals that classification_functions perform differently across gene_expression the question which classification function should be used for a given dataset remains to be answered in this study a predictive_model for choosing an optimal function for class_prediction on a given dataset was devised results to achieve this gene_expression data were simulated for different values of gene_pairs correlations sample_size genes variances deferentially expressed genes and fold changes for each simulated dataset ten classifiers were built and evaluated using ten classification_functions the resulting accuracies from different simulation scenarios by ten classification_functions were then modeled using a linear mixed_effects on the studied data characteristics yielding a model that predicts the accuracy of the functions on a given data an application of our model on eight real_life datasets showed_positive between the predicted and expected accuracies conclusion the here presented predictive_model might serve as a guide to choose an optimal classification function among the studied functions for any given gene_expression data availability_and the r source_code for the analysis and an r package sprefuged are available at bioinformatics online microarray gene_expression has become a widely used tool to identify particular disease subpopulations and to perform diagnostic and prognostic predictions in clinical practice they are used in diagnostic and prognostic analyses while in preclinical_studies toxicogenomics they involve predicting the toxicity of compounds in animal_models with the goal of speeding up the evaluation of toxicity for new drug_candidates though class_prediction analysis is a common practice the question that remains to be addressed is given the wide availability of classification_functions nowadays which classification function do we use for a particular dataset classification_functions have been shown to perform differently across gene_expression moreover the maqc ii initiative has pointed_out that classification function is one of the variables that explains the variabilitybetween gene_expression class prediction_performance while substantial amount of information is known about the characteristics of classification_functions and class_prediction building procedures little is known about which data characteristics have impact on the performance of a class_prediction model for instance diagonal linear_discriminant dlda assumes no covariances and hence no correlations between variables and might fail if the data is highly_correlated on the other hand linear_discriminant lda assumes a common covariance_matrix for the classes and thus to some extent accounts for correlations in addition pernalized regressions like ridge lasso elastic_net are capable to handle correlated variables support_vector svm though commonly understood as a method of finding the maximum margin hyperplane may also be seen as a regularization function estimation problem corresponding to a hinge loss function with a quadratic penalty as that of ridge_regression and it has been shown by that if a group of non distinct variables are selected as input variable set its training time lengthened and the errors become bigger on the other hand tree based_methods are by nature designed to capture interactions between variables while neural_networks might capture other complex structures within a given dataset given the above observations it is obvious that the performance of these functions depends on the characteristics of the data in question despite this the literature on how to choose a classification function for a given dataset is sparse a common practice is comparing several classification_functions and selecting the one with the minimum error_rate but this has been pointed by tibshirani and tibshirani and varma and simon to lead to selection_bias as such some experimenters adhere to one or a few classification_functions irrespective of the dataset disease or medical question being addressed while others choose a classification function for their datasets by affinity or familiarity without taking_into the characteristics of such data a simulation_study byshows that correlation is one of the data characteristics that affect the performance of most probabilistic classification_functions in addition showed that correlation structures differ across gene_expression data of different etiological diseases the study byshows that microarray_gene characteristics like log_fold of expression values number of deferentially expressed genes and pairwise_correlations between genes are associated to the accuracy of classification_functions however this study was conducted in real_life gene_expression where the magnitude and or direction of association might have been confounded by unobserved data characteristics in this study we aim to provide a guideline for making a choice of a classification function for a binary class prediction_problem based on observed magnitudes and directions of the data characteristics using accuracy as a measure of evaluation we investigate the effect of sample_size proportion of deferentially expressed de genes genes variances log fold changes pairwise_correlations between de and noisy genes on the accuracy of classification_functions using extensive simulations the remainder of this article is organized as follows methodology to simulate data classification_functions considered and the building and evaluation of class prediction_models are presented in section section contains a predictive summary of the results of class prediction_models for different simulated scenarios section provides an application of our predictive_model from the simulated results on real_life microarray gene_expression and section_presents a discussion to evaluate the predictive_ability of the here presented random_effects regression_model on real_life data eight affymetrix geneclassification in gene expressionexpression datasets of the non cancerous datasets described in one of our previous_studies were used these datasets were selected to include a variety of array platforms both class balance and class imbalance number of de probesets as well as various sample_sizes three of these datasets were preprocessed without filtering while the other five were preprocessed and filtered as described by we quantified the data characteristics studied and presented onas follows i sampsize by counting the samples in the study ii propde by ranking the probesets using limma and computing the proportion of de probesets based on a log_fold cutoff of if the number of de is or otherwise iii variance was determined as the mean of the variances of all the probesets iv log fc computed as the mean log fold changes of the de probesets v decorr as the mean of the elements of the upper lower triangular of the correlation_matrix of the de probesets and vi othercorr was computed as the standard_deviation sd of the elements of the upper lower triangular of the correlation_matrix of non de probesets this matrix was computed from all non de probesets if they were less than or a sample of from these non de probesets otherwise these data characteristics were standardized using the mean and sd of the respective variables from the simulated_data and our model was used to predict the accuracies for all classification_functions in each dataset supplementary we then built and evaluated classifiers using the classification_functions by splitting the data into learning set and test_set with stratification and a fold inner cross_validation on the learning set for parameters optimization this step was repeated a hundred times each time predicting the accuracies of classification_functions on the learning set using the random_effects and also recording the expected observed accuracies on the test_set these predicted and observed accuracies over the repetitions are respectively presented on supplementaryand b to compare the predicted to observed accuracies and considering that we are interested in the ordering of performance i e determining an optimal function for a given data we used the ranked base spearman_correlation between the average predicted accuracies and the average observed accuracies the results of this comparison for each dataset are presented on the positive correlation values on this figure indicate agreement between our predicted and observed accuracies though these correlations are not very high in some datasets our model more or less determined an optimal classification function for all the datasets except for uc where ridge_regression and svm emerged first instead of fourth as predicted i e sensitivity nevertheless the model was able to rule out on which classification s will perform poor on a given dataset with approximately certainty as expected the performance of the functions deteriorate on cf small_sample and low proportion of de probesets dia high class imbalance and small fold changes uc low proportion of de probesets and small fold changes uc large variances and high correlations and uc low proportion of de probesets fromand supplementaryand b one sees that except on the uc data our models accuracies are less than or equal to observed accuracies the model performs well on dataset with large_sample and balanced classes uc uc and uc it attained its lowest performance on dia where there is high class imbalance and hence few samples of the small class in the learning set and on hiv and cf datasets with small_sample we hypothesized that the performance of classification_functions on gene_expression data depends on sample_size proportion of de genes genes variances log fold changes between de genes and magnitude of the pairwise correlation within de genes and non de genes and showed their association to the accuracies of ten often used and clinically_relevant classification_functions using simulations additionally we built a predictive_model to determine an optimal classification function among the studied functions using the simulation_results an application of the predictive_model on eight non cancerous real_life gene_expression predicted optimal function s for seven out of the eight and was able to rule out function s that will perform poor on almost all the datasets this model may serve as a guide for choosing a classification function for a given gene_expression data classification_functions have been shown to perform differently across gene_expression and data characteristics have been shown to differ across datasets and are associated to the performance of classification_functions while sufficient_knowledge is available on the properties of most classification_functions and procedures to build class prediction_models using gene_expression data have been outlined by little is known about data characteristics that accounts for the variability in the performance of classification_functions and how to use these characteristics to choose an optimal classification function for a specific dataset as such most researchers adhere to specific classification function s or randomly choose a classification for their class prediction_models irrespective of the disease or data under study a common practice is to evaluate several classification_functions and select the one with smallestmisclassification error but this leads to selection_bias in this study we outlined data characteristics together with clinically_relevant and often used classification_functions and investigated their effects on classification_performance using simulation_studies based on these simulation_studies we provided a guide for choosing an optimal classification function for a specific dataset using the datas characteristics and the studied classification_functions through a linear random_effects predictive_model as a metamodel one would expect it to explain close to of the variance in the simulated_data but our predictive_model accounts for approximately of the variability in the simulated_data the remaining unexplained variance may be associated to sampling variability stemming from the several random covariance_matrices used to generate both learning and test_sets as well as the different learning and test_sets generated at each iteration although we used different classification_functions and evaluated these functions using accuracy our simulation_results confirm the findings of kim and simon that classifiers tend to have poor_performance on highly_correlated data our results also agree with those ofthat correlations the absolute log fold changes and the number of de probesets are associated to the accuracy of a class_prediction model in addition these results specify clearly the directions of the association and point out the effects of other data characteristics like sample_size genes variances that were not previously_identified most importantly we have provided a predictive_model that can serve as a guide to choose a classification function for a given dataset and its application on eight real_life datasets both filtered and unfiltered indicated a good predictive_ability of the model although our model was reasonably good in its prediction on real_life data we want to point out that it might have failed in some datasets because of the following reasons i most of the eight non cancerous datasets had small_sample and splitting these datasets to learning and test_sets yielded even smaller_sample of the learning sets and hence might have led to poor estimates of the characteristics under study and ii the observed accuracies might not be the true accuracies because of the few bootstrap samples it could have been better if we had the means to perform several bootstraps but due to the small_sample the number of independent bootstrap samples is limited the fact that our predictions were most often slightly_lower than the observed accuracies for almost all classification_functions might indicate the general trend that the performance of a model usually decreases on an independent dataset hence our models predictions might reflect expected accuracies on independent datasets in the simulated_data we assumed exponential and normal_distributions for the variances and pair_wise correlation of non de genes respectively these distributional assumptions might be violated in some datasets as such it will be worth trying different distributions also we used accuracy as a measure of evaluation by minimizing the loss function but in clinical_applications probabilities are more informative than simple yes or no predictions because they quantify theuncertainty of a prediction as such it is worth evaluating these data characteristics on probabilistic classification_functions where by the log likelihood_function is optimized this might possibly provide a predictive_model that will be most useful in clinical_applications despite these limitations our model was found to work well with data containing reasonably large and balanced sample_sizes n as such our results apply to balanced class data for data with class imbalance some classification_functions will have deteriorating performance for which several solutions are proposed however this topic is outside the focus of the current study in summary our results serve as a guide to use data characteristics to choose an optimal classification function for a given dataset 
