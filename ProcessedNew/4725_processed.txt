transformations for the compression of fastq quality scores of next-generation sequencing data motivation: the growth of next-generation sequencing means that more effective and efficient archiving methods are needed to store the generated data for public dissemination and in anticipation of more mature analytical methods later. this article examines methods for compressing the quality score component of the data to partly address this problem. results: we compare several compression policies for quality scores, in terms of both compression effectiveness and overall efficiency. the policies employ lossy and lossless transformations with one of several coding schemes. experiments show that both lossy and lossless transformations are useful, and that simple coding methods, which consume less computing resources, are highly competitive, especially when random access to reads is needed.next-generation sequencing (ngs) offers new directions in genome science by allowing entire genomes to be sequenced at lower costs. given its rapid growth, the problem of economically storing and quickly restoring sequencing data is becoming a concern for both researchers and operators of data centers. the most notable examples of data repositories is the sequence read archives (sra) by the international nucleotide sequence database collaboration (insdc) at ncbi, ebi and ddbj, which helps in disseminating publicly funded data . while the storing of raw data is infeasible, their hope is to be able to store at least the bases and their corresponding quality scores. on the other hand, as the amount of data continues to rise, it is conceivable that the burden of storing such data will gradually shift to research laboratories, hospitals or even individuals. effective means of storing sequencing data will be needed in these cases as well, even though the resources available will differ greatly. to whom correspondence should be addressed.the above trends suggest that economical representation of sequencing data is important. in response, compression of dna sequences has been an active research topic for many years. in contrast, relatively little attention has been devoted to quality scores. the set of quality scores is far larger than the four dna nucleotides; this has the potential to make the problem more difficult than sequence compression. whenever the economical representation of quality scores was taken into account, the scores were considered together with all other components. hence, it is not clear how well the quality scores component can be compressed. note that the little attention given to quality scores does not mean that they are useless. in fact, they are slowly becoming a necessary part of many data analyses. they can be used to trim reads at either end [see the galaxy tool for an example] or be used for read mapping, as demonstrated by the maq software . other future applications may also be possible. in this article, we address the issue of economical representation of quality scores as a stand-alone component. this separation allows us to focus on the topic at hand, and does not limit us from combining our results with other works that are devoted to compressing dna sequences alone. to have a clear understanding, and to supply different levels of economy trade-off, we break the process of economical representation into three independent and optional components: lossy transformation, lossless transformation and coding (or compression). rather than advocating a single method for each component, we will explore various options.this section reports the main experiment results for the dataset srr032209. additional results, including those for the dataset srr070788_1 are provided in the supplementary material. all experiments were conducted on a set of 2.53 ghz 8-core intel xeon e5540 with 12 gb of ram and hyper-threading.demonstrates the compression effectiveness, as compression ratios, achieved by the described lossless transformations with various block sizes. the most noticeable feature in the figure is the near immobility of the by-symbol curves, and the variability of the by-value curves across the four graphs. as anticipated, in general, the lossless transformations improve compression ratio for the by-value methods, and have almost no effects on the by-symbol compression schemes. moreover, all the by-symbol compression schemes are not effective when the block size k is small. the effect of lossless transformation on the by-value compression schemes is tremendous. in fact, these compression schemes are unsuitable for the original q-scores, as shown in. it is understandable, as the minimal q-scores of 33 is already a large value for these methods which reserve short code words to small values. while we can take off 33 from all q-scores before encoding to improve compression effectiveness, that simple operation is not considered here because it cannot do better than the minshifting transformation.and c clearly show the difference in performance between minshifting and freqordering. the former has a very low cost in terms of block headers, and performs well when k = 1, but gets worse when k grows since the impact of the minimal value lessens. on the contrary, while freqordering spends more for the block headers, the absolute cost is stabilized when k is large enough, making freqordering the clear winner. it should page: 633 628635be noted, however, that the combination (k = 1, minshifting, binary) is the simplest fromand c, but achieves a very impressive compression ratio. this combination can be a good choice when random access is a need. finally,shows that gaptranslating is the best overall lossless transformation for the by-value coding schemes, with the only exception of binary. it is interesting to see that the compression ratio is almost stable across different k. moreover, the compression ratio attained by interp is very close to that achieved by the by-symbol compression schemes with large k. the solid black lines indicate the zero-order self-information for each transformed dataseta limit that can be improved upon through block creation or more complex models. based on, we will further consider four representative coding schemes: gamma, interp, huffman and libbzip2. effects of lossy transformations: compression ratio achieved with different levels of granularity of the lossy transformation logbinning for the dataset srr032209 is shown in. as one would expect, the more stringent the lossy transformation, the better the compression ratio. with k = 1, the combination of (gaptranslating, interp) is the clear winner. with k = 256 or 16 384, although libbzip2 with no lossless transformation is the best, interp and gamma with gaptranslating can achieve fairly close compression ratios. in general, when a relative mapping accuracy of 99 is acceptable, the lossy transformation logbinning with =5 can be applied. at this setting, various combinations of lossless transformations and coding methods can achieve compression) times averaged over three trials for srr032209. lossless transformation gaptranslating and no lossy transformation for (a) k = 1 and (b) k = 256; and with no lossless transformation and lossy transformation logbinning at =5 for (c) k = 1 and (d) k = 256. ratios of around 1 bit per quality scorea dramatic progress in comparison to the level of 2.5 bits per quality score achieved in the absence of any lossy transformation.in this work, we have considered how to economically represent quality scores in ngs data as a combination of three main components: lossy transformation, lossless transformation and coding. of them, the first two are optional, but they can considerably affect the whole process. by separating these components, we were able to see the effects each of them can bring, and also to identify the best settings in order to achieve good system performance. in our study, we proposed three lossy transformations, introduced or made use of three lossless transformations, investigated several by-value coding schemes and considered more conventional bysymbol methods. moreover, we demonstrated how data blocking can affect both compression ratios and running times. finally, we also proposed a method to assess the usability, or worthiness, of any lossy transformation by employing the read mapping tool maq. we found that while full-fledged compression systems such as libbzip2 are widely accepted for economical representation of ngs, they are not the best choice for quality scores. here, simple codes such as the static code gamma and parameterized codes interp and golomb, when accompanied by the lossless transformation gaptranslating, are highly competitive: they can achieve similar levels of compression while using less time. in particular, unlike their counterparts that are effective only with large block sizes, the simple codes have the distinguished feature of being unaffected by this factor. this is an important point because it essentially removes the block size parameter from the process, and allows simple codes to greatly outperform their counterparts when small block sizes are in use. note that small block sizes offer a number of advantages which we have not explored. first, peak memory usage is reduced. second, since blocks are coded independently, errors in the compressed data stream can be isolated easily. third, random access in compressed data can be supported at the additional low cost of an index before each block. finally, independent blocks mean that our findings would apply to higher coverage datasets. as expected for the lossy transformations, compression ratios positively correlate with the number of distinct quality scores. of the three proposed transformations, logbinning is the most effectiveit achieves excellent relative mapping performance even when employing only a few distinct quality scores. with this choice, compression ratio is improved by around six times, while processing time, when coupled with by-symbol coding schemes, is reduced significantlyall suggesting that lossy transformations are useful. our results for srr032209 are supported by those of srr070788_1, which appear in the supplementary material. even though the reads are longer, the relative performance of the transformation and compression methods remain the same. our view is that as ngs data continues to grow, lossy and lossless transformations can work in tandem. for example, ngs data can be compressed losslessly and kept in off-line storage (such as tape backup) while lossy versions of the data can be shared between users and research laboratories for daily use. employing lossy transformations requires consideration since such changes are more easily noticeable and difficult to assess compared with images and video data our evaluation with maq is meant to address this issue. in the future, we plan to reorder the reads, as was done byfor the sequence bases, so that each block possesses reads that have similar quality score patterns. with respect to running time, we intend to parallelize some of our methods across blocks. furthermore, the effect of lossy transformations on other applications of ngs data, such as rna-seq and snp calling, also needs to be evaluated. our implementation, dubbed qscores-archiver, is available from http://www.cb.k.u-tokyo.ac.jp/asailab/members/ rwan under the lesser general public license version 3 or later to allow users to combine it with their fastq compression systems.  
