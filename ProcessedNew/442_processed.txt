gene expression application of the bayesian mmse estimator for classification error to gene expression microarray data motivation: with the development of high-throughput genomic and proteomic technologies, coupled with the inherent difficulties in obtaining large samples, biomedicine faces difficult small-sample classification issues, in particular, error estimation. most popular error estimation methods are motivated by intuition rather than mathematical inference. a recently proposed error estimator based on bayesian minimum mean square error estimation places error estimation in an optimal filtering framework. in this work, we examine the application of this error estimator to gene expression microarray data, including the suitability of the gaussian model with normal inverse-wishart priors and how to find prior probabilities. results: we provide an implementation for non-linear classification, where closed form solutions are not available. we propose a methodology for calibrating normal-inverse-wishart priors based on discarded microarray data and examine the performance on synthetic high-dimensional data and a real dataset from a breast cancer study. the calibrated bayesian error estimator has superior root mean square performance, especially with moderate to high expected true errors and small feature sizes. availability: we have implemented in c code the bayesian error estimator for gaussian distributions and normalinverse-wishart priors for both linear classifiers, with exact closed-form representations, and arbitrary classifiers, where we use a monte carlo approximation. our code for the bayesian error estimator and a toolbox of related utilities are available atclassification is a major constituent of bioinformatics, in particular, phenotypic discrimination, which can be accomplished via many different data types, such as gene expression, protein expression or sequence data. the misclassification error of a classifier quantifies its predictive capacity, the key aspect of any scientific model. to whom correspondence should be addressed.thus, accuracy of the error estimation represents the salient epistemological issue in classification, model validity . the main measure of error estimation accuracy is the root mean square (rms) error of the estimator,where tru and est are the true and estimated errors of the classifier and e is expectation with respect to the random sampling procedure. given a large data sample, the data can be split between training and test data, the classifier designed on the training data and classifier error estimated on the test data. in this scenario, there is a satisfactory distribution-free bound, rms 1/2 m, where m is the size of the test sample . however, when the sample is small, splitting the data is unacceptable because the classifier will be trained on too small a set, thereby resulting in poor classifier design. thus, in small sample settings (the concern of this article), a classifier is trained and its error estimated on the same data. a number of training data-based error estimators have been proposed in the past and we will consider several in this article. perhaps the one most commonly employed in bioinformatics is cross-validation. in this method, the data are partitioned into k folds (subsets); at each state of the procedure, one fold is held out, a surrogate classifier trained on the remaining folds and its error estimated on the held-out fold. the error of the classifier (originally trained on the full sample) is estimated by the average surrogate errors on the left-out folds. in the special case k = n, the sample size, each held-out fold consists of one point and the error method is termed leave-one-out. for leave-one-out, there is only one partition of folds; however, when k n evaluating all combinations of partitions is computationally prohibitive. hence, in this case partitions are randomly chosen to make the estimation. although cross-validation is close to being unbiased if k is not too small, it tends to have a large variance for small samples (braga) and also to be poorly correlated with the true error , the two combining to create a large rms for small samples [for a review of error estimation performance, see. a natural question arises: can cross-validation be used for small samples or, equivalently, are there small-sample cases in which the rms of cross-validation is sufficiently small so that it can be considered a valid error estimator? to answer this question, one might first ask if it is possible to use distribution-free bounds. notpage: 1823 18221831only are there very few cases in which such bounds are known, but also when they are known they are so loose as to be useless in practice. for instance, consider the following leave-one-out rms bound for the k-nearest neighbor classification rule with random tie breaking :  
