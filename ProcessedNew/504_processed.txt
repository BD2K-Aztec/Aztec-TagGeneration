a simple yet accurate correction for winner's curse can predict signals discovered in much larger genome scans downloaded from motivation: for genetic studies, statistically significant variants explain far less trait variance than sub-threshold association signals. to dimension follow-up studies, researchers need to accurately estimate true effect sizes at each snp, e.g. the true mean of odds ratios (ors)/regression coefficients (rrs) or z-score noncentralities. navenave estimates of effect sizes incur winners curse biases, which are reduced only by laborious winners curse adjustments (wcas). given that z-scores estimates can be theoretically translated on other scales, we propose a simple method to compute wca for z-scores, i.e. their true means/noncentralities. results:wca of z-scores shrinks these towards zero while, on p-value scale, multiple testing adjustment (mta) shrinks p-values toward one, which corresponds to the zero z-score value. thus, wca on z-scores scale is a proxy for mta on p-value scale. therefore, to estimate z-score noncen-tralities for all snps in genome scans, we propose fdr inverse quantile transformation (fiqt). it (i) performs the simpler mta of p-values using fdr and (ii) obtains noncentralities by back-transforming mta p-values on z-score scale. when compared to competitors, realistic simulations suggest that fiqt is more (i) accurate and (ii) computationally efficient by orders of magnitude. practical application of fiqt to psychiatric genetic consortium schizophrenia cohort predicts a non-trivial fraction of sub-threshold signals which become significant in much larger supersamples. conclusions: fiqt is a simple, yet accurate, wca method for z-scores (and ors/rrs, via simple transformations).genome-wide association studies (gwas) represent a powerful and widely used tool for detecting associations between genetic variants and complex traits. in such studies, researchers directly assay and statistically impute genotypes for one to several million single nucleotide polymorphisms (snps), respectively. the gwas paradigm has been very successful in identifying genetic variants associated with a range of phenotypes . however, as seen in gwas of psychiatric disorders , a considerable portion of the predicted genetic contribution is contributed by numerous moderate signals which are not deemed significant at accepted genome-wide levels, i.e. suggestive signals. with the advent of large-scale, wholeexome and-genome sequencing studies, the field will likely see an exponential increase in the number of such suggestive signal. to successfully dimension future studies, e.g. for detecting as significant a certain number of signals which are only suggestive in current cohorts, there is a need for statistical methods that accurately estimate unbiased effect-sizes for suggestive signal snps and, even, all variants from genome scans (henceforth denoting not only extant gwas and whole-exome sequencing but emergent wholegenome sequencing studies as well). given that effect size estimates such as z-scores, ors and rrs can be theoretically translated from one scale to another, in this paper we concentrate on estimating the true means/noncentrality of z-scores at each snp. when estimating the true effect sizes, it is well established that the largest signals are generally the most affected by the bias known as winners curse , i.e. their apparent effect size is (sometimes much) larger than their true values. this is due to statistics with the largest magnitude having an extreme value distribution , as opposed to the gaussian distribution we commonly assume for a random snp z-score. by incorrectly assuming a gaussian distribution, navenave estimators of extreme statistics have a tendency to overestimate the magnitude of these statistics . in statistical genetics, researchers proposed a multitude of methods to perform winners curse adjustment (wca) for studies with one-stage (discovery) and twostage (discovery and replication) studies . for z-scores, these wca estimates, i.e. their noncentralities or true means, are obtained by shrinking the z-scores towards their null value of zero. however, a majority of these methods are only designed to handle mostly significant signals. recently, two new tools for estimating the mean/noncentrality of all statistics in genome scans were proposed. the first, the empirical bayes (eb) method based on tweedies formula , was adapted from a general purpose statistical method. because it employs empirical estimates of the density/histogram (120 bins by default) of scan statistics, it is well suited for the large number of statistics from a genome scan (albeit less suited to instances in which the number of statistics is much smaller). in the context of genome scans, this method was used by, who found that the empirical histogram is less precise in the extreme tails off the distribution, where tail adjustment (ta) methods provide better accuracy. based on these observations, the authors proposed an interesting adaptive combination of eb and ta which, at the cost of increased computational burden, combines the best attributes of both methods. the second of these new tools is a computationally efficient, soft threshold method which adjusts statistics such that their sum of squares do not overestimate the true mean. because this method does not use empirical density estimation, it is applicable even to a small number of statistics. similar to wca, navenave use of snp p-values as a measure of association for snps will overestimate their statistical significance. this is due to the fact that, due to the large number of tests, many snps will attain very low p-values even under the null hypothesis of no association between trait and genotypes. thus, to be used in assessing genome-wide significance of snps, individual p-values need to be first adjusted for multiple testing . after multiple testing adjustment (mta), the adjusted p-values are much larger than the original ones, i.e. they are shrunken towards the null value of one. given that wca shrinks z-scores towards zero and mta shrinks the p-values towards one (which corresponds to a value of zero on the z-score scale), we argue that wca for effect sizes is very similar in spirit with, if not downright the homologue of, mta for p-values. thus, mta can be considered, if not identical to, a very good proxy for the wca for p-values. to accurately estimate the wca of z-scores from a genome scan, i.e. their true means/noncentralities, we propose a novel twostep method, which is inspired by the strong similarity between mta and wca. first, we perform a mta for p-values, e.g. by using a false discovery rate (fdr) approach . second, we estimate the noncentrality of z-scores by back-transforming the adjusted p-value on the z-score scale using an inverse gaussian cumulative distribution function (cdf). when compared to competing methods, we show that the proposed procedure has very good performance in terms of (i) squared error loss, (ii) fraction of the variability in true means of univariate statistics explained and (iii) computational efficiency. a practical application of this approach shows that, due to their good performance, the proposed estimators can be used to predict with reasonable accuracy the number and location of subtreshold signals that are likely to become significant in much larger cohorts.among eb-n methods, we tested eb-1, eb-10, eb-50 and eb-100. eb-10 and eb-50 have intermediate performance between eb-1 and eb-100, with eb-10 closer to eb-1 and eb-50 closer to eb-100 (data not shown). consequently, we present only the results for eb1 and eb-100. measures of prediction accuracy for all of the above methods are assessed based on the simulated and estimated noncentralities for all snps in genome scans. under h 0 , i.e. the surrogate for underpowered studies, fiqt has the best mean square error (mse) performance everywhere, except for the (very small) region of extremely low p-values, where eb-1 slightly outperforms it . among the remaining methods, eb1 performs best over the entire parameter space and, as expected, mle has the largest mse. we note that, in marked contrast to the alternative hypothesis results that follow, eb-1 thoroughly outperforms eb-100. under the alternative, h a , fiqt has better mse performance for settings with moderate to large number of signals and larger sample sizes . its performance improvement over competitors is sometimes substantial, e.g. for large sample sizes and medium number of signals. eb-1 does not outperform fiqt under any h a scenarios. eb-100 only nominally outperforms fiqt at smaller sample sizes. surprisingly, even though it was designed only as a tail bias adjustment, ta performs reasonably well. under certain scenarios, e.g. large sample sizes, it outperforms ebs for statistics with nominally significant p-values and even slightly outperforms fiqt for a very narrow range of moderately small p-values. the better performance of fiqt is mostly due to the lower variance of this estimator [in supplementary material (sm)], because, when compared with eb (and especially ta) methods, the bias is often somewhat larger . (however, fiqt conservativeness at low p-values (negative bias in s2), opens the possibility of future improvement which take into account the local ld of statistics, as. alternative hypothesis mse of z-scores noncentrality estimates (seefor background/notation). c s relative to mhh sample size; c c-number of causal signals relative to the 180 significant mhh signals alluded in discussion.) when measuring accuracy by r 2 , i.e. the explained variability of the z-score noncentralities, fiqt practically outperforms all other methods , albeit eb-100 only nominally so. due to its very simple computation, fiqt has much faster running times than competitors. when compared to the next most accurate method, eb-100, the proposed method is faster by more than four orders of magnitude [in supplementary material (sm)]. fiqt is also faster than the less accurate eb-1 by almost two orders of magnitude (data not shown). practical application. given that discovery phase of pgc-scz2 has around four times the sample size of its pgc-scz1 homolog, then the z-score noncentrality in pgc-scz2 are expected to be twice as large as the pgc-scz1 fiqt estimates (seefor relationships between these estimators and pgc-scz1 statistics). by computing, under a gaussian distribution assumption, the p-values associated with estimated pcc_scz2 noncentralities, we predict 46 regions (supplementary) to attain significance in pgcscz2, as opposed to only 11 present in pgc-scz1. the 46 regions were obtained by clustering together predicted significant signals within 250 kb. of these significant regions, a very high number, 34 ($75), overlap the 105 independent chromosomal regions reported by pgc-scz2. a total of 18 predicted pgc-scz2 regions overlap the extended mhc region (2533 mb) on chromosome 6p from the actual pgc-scz2 findings, as opposed to only 5 reported by pgc-scz1. of the overall of 34 overlapping regions, 16 are in loci outside mhc regions, as opposed to just 6 reported by pgcscz1.  
