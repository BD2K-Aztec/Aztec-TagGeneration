sequence analysis kmc 2: fast and resource-frugal k-mer counting motivation: building the histogram of occurrences of every k-symbol long substring of nucleotide data is a standard step in many bioinformatics applications, known under the name of k-mer counting. its applications include developing de bruijn graph genome assemblers, fast multiple sequence alignment and repeat detection. the tremendous amounts of ngs data require fast algorithms for k-mer counting, preferably using moderate amounts of memory. results: we present a novel method for k-mer counting, on large datasets about twice faster than the strongest competitors (jellyfish 2, kmc 1), using about 12 gb (or less) of ram. our disk-based method bears some resemblance to mspkmercounter, yet replacing the original minimizers with signatures (a carefully selected subset of all minimizers) and using (k, x)-mers allows to significantly reduce the i/o and a highly parallel overall architecture allows to achieve unprecedented processing speeds. for example, kmc 2 counts the 28-mers of a human reads collection with 44-fold coverage (106 gb of compressed size) in about 20 min, on a 6-core intel i7 pc with an solid-state disk. availability and implementation: kmc 2 is freely available at http://sun.aei.polsl.pl/kmc.one of common preliminary steps in many bioinformatics algorithms is the procedure of k-mer counting. this primitive consists in counting the frequencies of all k-long strings in the given collection of sequencing reads, where k is usually more than 20 and has applications in de novo assembly using de bruijn graphs, correcting reads and repeat detection, to name a few areas. more applications can be found, e.g. in, with references therein. k-mer counting is arguably one of the simplest (both conceptually and programmatically) tasks in computational biology, if we do not care about efficiency. the number of existing papers on this problem suggests, however, that efficient execution of this task, with reasonable memory use, is far from trivial. the most successful of early approaches was jellyfish , maintaining a compact hash table (ht) and using lock-free operations to allow parallel updates. the original jellyfish version [as presented in marais and] required more than 100 gb of memory to handle human genome data with 30-fold coverage. bfcounter employs the classic compact data structure, bloom filter (bf), to reduce the memory requirements due to preventing most single-occurrence k-mers (which are usually results of sequencing errors and for most applications can be discarded) from being added to an ht. although bf is a probabilistic mechanism, bfcounter applies it in a smart way, which does not produce counting errors. dsk and kmc are two disk-based algorithms. on a high level, they are similar and partition the set of k-mers into disk buckets, which are then separately processed. dsk is more memory frugal and may process human genome data in as little as 1.1 gb of ram , whereas kmc is faster but typically uses about 1116 gb of ram. turtle (bears some similarities to bfcounter. the standard bf is there replaced with its cache-friendly variant and the ht is replaced with a sorting and compaction algorithm (which, accidentally, resembles a component of kmc), apart from adding parallelism and a few smaller modifications. finally, mspkmercounter is another disk-based algorithm, based on the concept of minimizers, described in detail in the next section. in this article, we present a new version of kmc, one of the fastest and most memory efficient programs. the new release borrows from the efficient architecture of kmc 1 but reduces the disk usage several times (sometimes about 10 times) and improves the speed usually about twice. in consequence, our tests show that kmc 2 is the fastest (by a far margin) algorithm for counting k-mers, with even smaller memory consumption than its predecessor. there are two main ideas behind these improvements. the first is the use of signatures of k-mers that are a generalization of the idea of minimizers . signatures allow significant reduction of temporary disk space. the minimizers were used for the first time for the k-mer counting in mspkmercounter, but our modification significantly reduces the main memory requirements (up to 35 times) and disk space (about 5 times) when compared with mspkmercounter. the second main novelty is the use of (k, x)-mers (x 0) for reduction of the amount of data to sort. simply, instead of sorting some amount of k-mers, we sort a much smaller portion of (k x)-mers and then obtain the statistics for k-mers in the post-processing phase.the implementation of kmc 2 was compared against the best, in terms of speed and memory efficiency, competitors: jellyfish 2 [which is significantly more efficient than the version described in ], dsk , turtle , mspcounter , kanalyze and kmc 1 . each program was tested for two values of k (28 and 55) and in two hardware configurations: using conventional hard disks (hdd) and using a solid-state disk (ssd). we used several datasets of varying size; two of them are human data with large coverage. the experiments were run on a machine equipped with an intel i7 4930 cpu (6 cores clocked at 3.4 ghz), 64 gb ram and 2 hdds (3 tb each) in raid 0 and single ssd (1 tb). some of the experiments were also run on a single hdd (5 tb). the programs were run with the number of threads equal to the number of virtual cores (6 2 12), to achieve maximum speed. in the experiments, we count only k-mers with counts at least 2, since the k-mers with a single occurrence in a read collection most likely contain erroneous base(s). as in some applications, all k-mers may be needed, we ran a preliminary kmc 2 test in such setting, with a ssd. we found out that the overhead in computation time is only up to 3 (mainly caused by increased i/o). the comparison, presented in tables 24 and supplementary tables s1 and s2, includes total computation time (in seconds), maximum ram use and maximum disk use. ram and disk use are given in gbs (1gb 10 9 b). time is wall-clock time in seconds. a test running longer than 10 h was interrupted. other reasons for not finishing a test were excessive memory consumption (limited by the total ram, i.e. 64 gb) or excessive disk use (over 650 gb, chosen for our 1 tb ssd disk; note that the largest input dataset, homo sapiens 2, occupies 312.9 gb on the same disk). jellyfish 2 was tested twice, in the default and the bf-based mode with exact counts. unfortunately, in the latter experiments, the amount of memory in our machine was often not enough, and this is why jellyfish 2-bf results are shown only for two datasets. several conclusions can easily be drawn from the presentedthe slowest; for this reason, kanalyze was tested only on the ssd. kanalyze also uses a large amount of temporary disk space, which was the reason we stopped its execution on the two human datasets (for k 28 only, as kanalyze does not support large values of k). mspkc, on the other hand, theoretically allows the parameter k to exceed 32, but in none of our datasets, it finished its work for k 55; for the smallest dataset (f.vesca), it failed probably because of variable-length reads, on the other datasets, we stopped it after more than 10 h of processing. the only asset of kanalyze and mspkc we have found is their moderate memory use. dsk is not very fast either. still, it consistently uses the smallest amount of memory (6 gb was always reported) and is quite robust, as it passed all the tests. jellyfish 2 in its default mode is not very frugal in memory use, and this is the reason on our machine it passed the test for k 55 only for two datasets (f.vesca and m.balbisiana). still, for k 28, it passed all the tests, being one of the fastest programs, often outperforming kmc 1. turtle is rather fast as well (slower than jellyfish though), but even more memory hungry; we could not have run it on the two largest datasets. turtle and jellyfish are memory-only algorithms, all the other ones are disk based. this is the reason why changing hdd to a much faster ssd does not affect the performance of these two counters significantly (yet it is non-zero due to faster input reading from the ssd). kmc 2 on the ssd was tested three times for each k: with standard memory use (12 gb) and with memory use reduced to 6 gb (suggested limit) and to 4 gb (strict limit). we note that reducing the memory even to 4 gb only moderately increases the processing time. it is worth to note that both kmc 2 and dsk can be run with even lower memory limits, i.e. about 1 gb but it comes at a price of speed drop. for experiments we, however, chose larger settings, as 4-6 gb of ram seems to fit even low-end machines. kmc 2 with its standard memory use is a clear winner in processing time, on the human datasets being about twice faster than jellyfish 2 or kmc 1. these speed differences concern the ssd experiments, as on the hdd the gap diminishes (but is still significant). this can be explained by i/o (especially reading the input data) being the bottleneck in several phases of kmc 2 processing. it is worth examining how switching a conventional disk to a ssd affects the performance of disk-based software. it might seem natural that the biggest time reduction (in absolute time, not percentage gain) should be seen in those programs which use more disk space. to some degree it is true (e.g. kmc 1 gains more than kmc 2), but dsk is a counter-example: e.g. on h.sapiens 2, it gains as much as 13 006 s, which is almost seven times the reduction for kmc 1, seemingly surprising as dsk uses less disk space. yet, a probable explanation is that dsk works in several passes, so its total i/o is actually quite large for large datasets.interestingly, for disk-based algorithms, the disk use of kmc 2 is typically reduced when switching from k 28 to k 55. this can be explained by a smaller number of k-mers per read, and in case of kmc 2 also by a smaller number of super k-mers per read. to check if the ssd disk, with about 500 mb/s read/write performance, may still be a bottleneck, we ran kmc 2 also in the inmemory mode. the memory consumption then grows to about the sum of memory and disk use in the standard setting, yet the processing time improves, by about 20 for g.gallus and 3 for m.balbisiana. this shows that even with the ssd disk, the performance is (somewhat) hampered by i/o operations. we also measured how the input format (raw, gzipped) and media (one or two hdds in raid 0, ssd) affects the performance of our solution on the largest dataset, h.sapiens 2 . as expected, using the ssd reduces the time by 2540 and reading the input from compressed form also has a visible positive impact. we note in passing that replacing gzip with, e.g. bzip2 (results not shown here) would not be a wise choice, since the improvement in compression cannot offset much slower bzip2s decompression.compares signatures with minimizers on g.gallus. we can see that using our signatures diminishes the average number of super k-mers in a read by about 1015 percent. also the number of k-mers in the largest (disk) bin is significantly reduced, sometimes more than twice. these achievements directly translate to smaller ram and disk space consumption. how (k, x)-mers affect bin processing is shown infor two datasets. it is easy to see that the number of strings to sort is more than halved for x 3, yet the speedup is more moderate, due to the extra split phase [i.e. extracting (k, x)-mers from super kmers] and sorting over longer strings. still, (k,3)-mers versus plain kmers reduce the total time by more than 20 (and even 38 for h.sapiens 2 and k 55). the impact of k on processing time and disk space is presented in figures 4 and 5, respectively. longer k-mers result in even longer super k-mers, which minimizes i/o, but makes the sorting phase longer. for this reason, the disk space consumption shrinks smoothly with growing k , but the effect on processing time is not so clear. still, counting k-mers for k ! 32 is generally slower than for smaller values of k. from, we can see that using more memory accelerates kmc 2, but the effect is mediocre (only about 10 speedup when raising the memory consumption from 16 to 40 gb). the reasons behind the speedup are basically 2-fold: (i) the extra ram allows to use a larger number of sorter threads (which is more efficient than few sorters with more internal threads per sorter) and (ii) occasional large bins disallow to run other sorters at the same time if memory is limited. finally, we analyze the scalability and cpu load of our software . as expected, the highest speed is achieved when the number of threads matches the number of (virtual) cpu cores (12). still, the time reduction between 1 and 12 threads is only by factor 3 or less, when the input data are in non-compressed fastq. using theavg. in read is the average number of super k-mers per read. no. k-mers largest bin is the number (in millions) of k-mers in the largest bin. min. memory is the amount of memory (in gbytes) necessary to process the k-mers in the largest bin, i.e. the lower bound of the memory requirements. the size of temporary disk space is determined by the average number of minimizers/signatures in a read. for example, the disk space requirements for minimizer/signature length 7 are 25.4 gb (signatures, k 28) and 28.6 gb (minimizers, k 28).a 12-gb ram set, gzipped input. sorted fraction is the ratio of the number of (k, x)-mers to the number of k-mers. for h.sapiens 2, the largest bin was too large to fit the assumed amount of ram in two cases, and the ram consumption of kmc 2 was 25 gb for (55, 0)-mers, 18 gb for (55, 1)-mers, 15 gb for (55, 2)-mers and 13 gb for (55, 3)-mers. compressed input broadens the gap to factor 6.4 for k 28 and 4.9 for k 55. the corresponding gaps between 1 and 6 threads (i.e. equal to the number of physical cores) are 2.3 and 2.5 (k 28 and k 55) with non-compressed input and 4.9 and 3.9 (k 28 and k 55) with gzipped input. the latter experiment tells more about the scalability of our tool, since the performance boost from intel hyper-threading technology can be hard to predict, varying from less than 10 (,) to about 60 in real code.although the dominating trend in it solutions nowadays is the cloud, the progress in bioinformatic algorithms shows that even home computers, equipped with multi-core cpus, several gigabytes of ram and a few fast hard disks (or one ssd disk) get powerful enough to be applied for real omics tasks, if their resources are loaded appropriately. the presented kmc 2 algorithm is currently the fastest k-mer counter, with modest resource (memory and disk) requirements. although the used approach is similar to the one from mspkmercounter, we obtain an order of magnitude faster processing, due to the following kmc features: replacing the original minimizers with signatures (a carefully selected subset of all minimizers), using (k, x)-mers and a highly parallel overall architecture. as opposed to most competitors, kmc 2 worked stably across a large range of datasets and test settings. in real numbers, we show that it is possible to count the 28-mers of a human reads collection with 44-fold coverage (106 gb of compressed size) in about 20 min, on a 6-core intel core i7 pc with an ssd. with enough amounts of available ram, it is also possible to run kmc 2 in memory only. in our preliminary tests, it gave rather little compared with an ssd (about 510 speedup) but may be an option in datacenters, with plenty of ram but possibly using network hdds with relatively low transfer. in this scenario, a memory-only mode should be attractive. after our work was ready, we learned about an interesting possibility of using frequency-based minimizers . the idea is to select the (globally) least frequent m-mer in a given k-mer and it dramatically reduces the memory use in the application of enumerating the maximal simple paths of a de bruijn graph. in our preliminary experiments freq-based minimizers reduce the memorytimek = 28 k = 55. dependence of kmc 2 processing time on maximal available ram and type of disk for h.sapiens 2 dataset. there are 4 results for k 55 and 13 gb ram. these results are for set 6 gb, 8 gb, 10 gb, 12 gb as maximal ram usage. however, the largest bin enforced to spend at least 13 gb of ram  
