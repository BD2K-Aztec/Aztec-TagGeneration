sequence analysis scalablast 2.0: rapid and robust blast calculations on multiprocessor systems motivation: blast remains one of the most widely used tools in computational biology. the rate at which new sequence data is available continues to grow exponentially, driving the emergence of new fields of biological research. at the same time, multicore systems and conventional clusters are more accessible. scalablast has been designed to run on conventional multiprocessor systems with an eye to extreme parallelism, enabling parallel blast calculations using 416 000 processing cores with a portable, robust, fault-resilient design that introduces little to no overhead with respect to serial blast. availability: scalablast 2.0 source code can be freely downloaded fromgenome and protein sequence analysis using blast continues to be among the most used tools for computational bioinformatics. the continued exponential growth in throughput of sequencing platforms has continued to drive the need for ever-expanding capacity for blast calculations to support genome annotation, functional predictions and a host of other foundational analysis for sequence data. parallel blast accelerators have been implemented in the past including mpiblast and scalablast 1.0 . parallel blast drivers accelerate large lists of blast calculations using multiprocessor systems. scalablast 1.0 used a hybrid parallelization scheme in which the sequence list was statically partitioned among processor pairs (process groups). process groups performed independent blast calculations simultaneously, gaining a degree of speedup on the overall calculation in proportion to the number of process groups used in the calculation. the main limitation of scalablast 1.0 was the use of static data partitioning that did not have fault-resilience properties. by contrast, the main limitation of mpiblast is the need for pre-formatting datasets to achieve optimized run-time, sometimes requiring repeated attempts on the same dataset to find the right pre-formatting configuration. we have addressed these limitations in scalablast 2.0 by (i) re-implementing the task scheduling layer by introduction of a dynamic task management scheme that (ii) does not require preformatting. this technique allows processors to obtain work units independently and at run-time based on their availability. this is a highly tolerant and fault-resilient approach that ensures that all processors are doing as close as possible to the same amount of work throughout a calculation. in addition, this implementation allows for continued operation even in the presence of processor or other system failures. this is critical for all large-scale calculations and is independent of the code being run because the longer the run and the larger the system, the more likely one is to encounter a component failure during a calculation. as the expected run-time increases, the likelihood of successfully completing the calculation before the next failure tends to zero. we demonstrate near-ideal scaling using scalablast 2.0 calculations to machine capacity on a linux cluster having418 000 compute cores even during process failure events. scalablast 2.0 can be downloaded freely from http:// omics.pnl.gov/software/scalablast.php. output and input on globally mounted or local file systems or combinations of both. after file distribution is complete, the manager is responsible for tracking which tasks have been assigned and which tasks have been completed. the manager is also responsible for processing the fasta input files (both query and target database are in fasta format, eliminating the need for pre-formatting database files) and distributing these processed files. the task groups can be controlled by the user and can span multiple compute nodes. for instance, a system with eight-core nodes can have a task group size of 24 in which sets of three nodes work together as a single task group, having one sub-manager core and 23 worker cores. this dynamic scheduling layer ensures that when processes fail or get loaded down with tasks taking a long processing time, other processes continue to do meaningful work. this allows for highly skewed input sets to be processed as much as possible in an even run-time. dynamic scheduling is implemented by having the manager hand out tasks to submanagers. workers completing a task do not write their output until they verify from the manager (via the sub-manager) whether the task has already been checked back in. workers then request a new assignment from the manager. when all the tasks have been assigned, any workers reporting for new work are given a duplicate task that has not yet been completed. in this way, nodes that fail during a calculation are simply ignored. any tasks assigned to them will be re-assigned to other workers until one of them completes the calculation.scalablast 2.0 was run on a linux cluster at pacific northwest national laboratory that has 2310 compute nodes, each having eight cores for a total of 18 480 compute elements. for blastp scaling runs, our query dataset contained 203 200 proteins with widely varying size distribution. our query list had an average protein length of 175.1 ae 138.5 residues, with a minimum length of eight and a maximum length of 4299 residues. this list was compared against a version of the non-redundant database from ncbi dated may 2010 and containing 12 million reference proteins. each query sequence was compared to the reference database using blastp with default blosum62 scoring matrix and print option 9.  
