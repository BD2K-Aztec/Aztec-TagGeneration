data and text mining a statistical framework for biomarker discovery in metabolomic time course data motivation: metabolomics is the study of the complement of small molecule metabolites in cells, biofluids and tissues. many metabolomic experiments are designed to compare changes observed over time under two experimental conditions or groups (e.g. a control and drug-treated group) with the goal of identifying discriminatory metabolites or biomarkers that characterize each condition. a common study design consists of repeated measurements taken on each experimental unit thus producing time courses of all metabolites. we describe a statistical framework for estimating time-varying metabolic profiles and their within-group variability and for detecting between-group differences. specifically, we propose (i) a smoothing splines mixed effects (sme) model that treats each longitudinal measurement as a smooth function of time and (ii) an associated functional test statistic. statistical significance is assessed by a non-parametric bootstrap procedure. results: the methodology has been extensively evaluated using simulated data and has been applied to real nuclear magnetic resonance spectroscopy data collected in a preclinical toxicology study as part of a larger project lead by the comet (consortium for metabonomic toxicology). our findings are compatible with the previously published studies.metabolomics, or metabolic profiling, experiments monitor the levels of the myriad of small molecules in biological systems and provide a snapshot of the metabolic state of the organism under study . metabolomics is a key part of the systems biology approach to biological problems, reporting on a different layer of biomolecular organization to that assayed by proteomics and transcriptomics. to assay metabolite levels, metabolomics studies employ spectroscopic or spectrometric methods such as nuclear magnetic resonance (nmr) spectroscopy to whom correspondence should be addressed. or mass spectrometry (ms), resulting in high quantities of complex yet information rich data. this complexity necessitates sophisticated statistical and bioinformatic approaches to data analysis both at the level of the raw data processing (e.g. peak detection, alignment, etc.) and also when investigating clustering, classification and other biological features in the data . one of the most common goals in metabolomic experiments is to discover biomarkers: metabolites whose concentrations are associated with metabolic status. for example, one may search for metabolites that respond to a biological stimulus or which change in concentration between healthy and diseased individuals. this goal extends to time series experiments, where a common objective is to find metabolites whose time profiles show significant differences between conditions. such metabolites may reveal novel insights into the complex regulatory mechanisms underlying normal physiology and how they are altered in pathological conditions.shows an example of selected time series for one spectral bin (or variable) taken from a real toxicology study on rats, which provides some insight into the difficulties encountered (see section 2 for more details on this study). first, the time series are extremely short and it is rare to find metabolomics datasets with more than 310 time points. in contrast, classical approaches to time series analysis typically require several tens of data points in order to adequately model the time variation . second, even within the same experimental groups, there can be a lot of biological variability; for instance, the temporal profiles for the three biological replicates (distinct biological units, in this case rats) shown inare different, although they were all observed under the same experimental conditions. being able to accurately model this variability is therefore critical in order to detect meaningful temporal patterns. third, there are missing observations, either by design or because they were deemed to be outliers and hence removed or due to errors in the measurement process. many classical approaches would be unable to handle these missing observations without resorting to imputation procedures. finally, the observations are clearly temporally correlated and the response is non-linear. to date, most analyses of time series data in metabolomics do not explicitly include the time ordering in the model; that is, the same results would be obtained if the time order of the data was permuted. an obvious consequence of ignoring the time ordering is that information is lost, resulting in less power to discover time-related structure. a notable exception is the batch modeling approach in which the time-ordered multivariate data are regressed against the time variable using partial. short individual time series observed for one metabolite (creatine, 3.0395 ppm) for three example rats in the high-dose group of our example dataset. shown are the raw observations with fitted individual curves from our sme model (dashed lines). least squares . this approach, however, implicitly assumes linear trends with respect to time and does not make use of any temporal autocorrelation structure present in the data. other difficulties presented by the data include the high dimensionality, with thousands of spectral bins under study simultaneously, strong degree of collinearity between variables and the presence of noise.provides a comprehensive review of methods appropriate for the modelling of metabolomics time course datasets, ranging from differential equations to state-space models, while noting that the number of approaches is very limited and that they often fail to correctly take in to account the time ordering. many of the methods discussed are motivated by datasets with many more time points than our example case study (145 versus 10) or where the focus is on modelling pre-selected metabolites rather than identifying potentially novel biomarkers. it is this established gap in the metabolomics data analysis toolbox that has motivated the development of our proposed statistical framework. our framework is designed for modelling short, high-dimensional and heterogenous metabolomic profiles and detecting significant differences between two biological groups. the first part of the framework is our proposed smoothing spline mixed effects (sme) model which sets out to address the challenges described above. the sme model has its roots in functional data analysis (fda) , a rapidly developing area of statistics that has been successfully applied to longitudinal data arising in genomics experiments that exhibit very similar characteristics to metabolic profiling data and other authors have noted the importance of taking inspiration from transcriptomics in developing metabolomics data analysis approaches . in the fda paradigm, each observed time series is seen as the realization of an underlying stochastic process or smooth curve that needs to be estimated. these estimated curves are then treated as the basic observational unit in subsequent data analysis, such as the task of detecting variables that exhibit a significant difference between two groups that we focus on in this article. the second part of our framework, therefore, is our proposed moderated functional t-type statistic for quantifying the difference between two sets of curves that makes full use of the estimated within-group variability obtained from the sme model. several solutions to this problem have been proposed in the literature such as the use of the functional l 2 distance or the heuristic procedure of cox and lee (2008) which discretizes the curves on a fine grid and then carries out point-wise t-tests. our proposed test statistic builds on these approaches by borrowing strength across all variables in order to increase power. this article is structured as follows. the suggested methodology, based on functional mixed-effect models, is described in section 3. the experimental results are presented in section 4. we summarize with some conclusions in section 5.in this article, we have presented our complete framework for estimation and testing in metabolomics time course datasets with the goal of identifying significant biomarkers that discriminate between two treatment groups. we have focused on the development of a flexible functional model that can accurately describe longitudinal metabolomic data and a functional t-test that fully exploits the model parameters to increase power to detect significant differences between the two groups. we note that while we have demonstrated our approach with nmr, the framework can be applied to any kind of longitudinal data including widely used techniques such as mass spectrometry where its ability to handle inter-subject and inter-metabolite heterogeneity can be considered a useful advantage. the general functional mixed-effects model (1) has appeared in a number of different specific forms in proposed approaches for the analysis of longitudinal data arising from genomics experiments. these approaches vary in their ability to handle biological replicates, analysis goalstypically either clustering (; ma) or detecting differentially expressed genes model selection procedures and the representations used for the fixed-and random-effects functions. despite the wide range of resulting models, the message is clear: functional mixed-effects models are well suited to handling time series data that is very short, noisy and replicated. in this work, we have set out to improve upon the existing methodology and adapt it to better suit our case study in three key ways. first, we have modelled the individual-level effects as full curves, yielding a flexible model that can more accurately describe the often extremely heterogeneous responses we observe among different biological replicates than simpler models such as edge. second, we perform model selection on a per-variable basis, avoiding the possibility of overfitting that the more flexible model presents. this procedure is made possible by the smoothing spline representation that simplifies the optimization for each variable to a 2d search of real-valued smoothing parameters and avoids having to select knots. third, we have introduced a moderated functional t-statistic that makes full use of the fitted model parameters by incorporating the replicate-level effects and borrows strength across all spectral bins to increase power. the results from our simulation study demonstrate the tangible benefits that the combined use of the sme model and moderated functional t-statistic present compared with the most closely related existing approach. furthermore, the ability of the model and test statistic to give biologically meaningful results in the context of the comet study suggests these benefits are equally applicable to real datasets.  
