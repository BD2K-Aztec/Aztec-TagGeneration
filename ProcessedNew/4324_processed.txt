sofia: a data integration framework for annotating high-throughput datasets motivation: integrating heterogeneous datasets from several sources is a common bioinformatics task that often requires implementing a complex workflow intermixing database access, data filtering , format conversions, identifier mapping, among further diverse operations. data integration is especially important when annotating next generation sequencing data, where a multitude of diverse tools and heterogeneous databases can be used to provide a large variety of annotation for genomic locations, such a single nucleotide variants or genes. each tool and data source is potentially useful for a given project and often more than one are used in parallel for the same purpose. however, software that always produces all available data is difficult to maintain and quickly leads to an excess of data, creating an information overload rather than the desired goal-oriented and integrated result. results: we present sofia, a framework for workflow-driven data integration with a focus on gen-omic annotation. sofia conceptualizes workflow templates as comprehensive workflows that cover as many data integration operations as possible in a given domain. however, these templates are not intended to be executed as a whole; instead, when given an integration task consisting of a set of input data and a set of desired output data, sofia derives a minimal workflow that completes the task. these workflows are typically fast and create exactly the information a user wants without requiring them to do any implementation work. using a comprehensive genome annotation template, we highlight the flexibility, extensibility and power of the framework using real-life case studies.biological entity annotation is the process of retrieving the broader biological context around a biological entity from available data sources. it is a crucial step in essentially all current biological analyses based on high-throughput experiments . only by considering the biological context within which an experiment took place can the results be properly interpreted and a pertinent conclusion reached. as increasingly more knowledge about biological systems is discovered and stored in structured databases, the effort required to integrate all relevant information in projects producing comprehensive experimental datasets becomes more and more involved. providing investigators all available information can lead to information overload, as such complete annotation sets contain much more data than is relevant to the investigated hypothesis. there is a lack of fast and intuitive methods that (i) allow researchers to specify exactly the type of information that they think is best suited to their investigations and (ii) produce this information quickly and fully automatically. data integration for the life sciences is by no means a new topic current approaches can be broadly categorized into three classes. so-called data warehouses are relational databases that integrate a selected set of data into a common schema . accessing the data in a data warehouse requires either the ability to program complex queries (usually in sql), or the usage of specific point-and-click user interfaces encapsulating such queries. the latter solution is the only option when programming expertise is lacking, but is inflexible and involves costly interface development. a second class of data integration systems are based on linked open data and semantic web standards . these offer more flexibility in terms of data modelling, but require efforts comparable to data warehousing for building semantically integrated datasets . both approaches perform data integration prior to any concrete analysis, which implies that they usually try to be as comprehensive as possible to cover unforeseen applications. creating or updating this large integrated dataset is highly complex and time consuming, increasing the danger of using outdated data (j rg and). more recently, a third class of systems has emerged that are based on flexible integration workflows . in these approaches, data integration is performed by starting a pipeline of steps that are defined in advance by a workflow developer. results of these workflows are typically directly consumed by the user or by other tools and not meant to be materialized in a persistent, maintained manner. accordingly, every analysis uses the most recent data available. to be fast, these workflows are specialized; a drawback when no available workflow exactly meets the users requirements. either a new workflow has to be developed, or multiple workflows with potentially overlapping subtasks have to be executed, yielding inflexibility and unnecessary computation. what is lacking is a data integration method that, based on a formalized understanding of an application domain, is able to automatically determine the minimal complete sequence of steps required to fulfil a given user request starting from a given set of input data. sofia, software for flexible integration of annotation, aims at filling this gap. using sofia, workflow designers specify comprehensive workflow templates covering as much of a given application domain as possible, much like defining the process used to populate a data warehouse. however, these templates are not intended to be executed in their entirety. instead, they should be understood as a formalized knowledge base of processes transforming various types of input data into various types of annotations using background knowledge. when given a set of input data (e.g. results from wet lab experimentation) and a desired output (e.g. gene names related to the experimental data in a specific way), sofia uses the template to infer a minimal set of actions necessary to produce the output from the inputs. in case there is a unique way for doing so, the process runs fully automatically. we see that the sofia approach has the following important advantages: a. it is fast, as only the necessary steps are executed; b. it supports reproducibility, as users can choose which data versions will be integrated; c. from a users perspective, it is very simple to use, as only inputs and outputs have to be specified; d. it relieves from the need to build and update a large, comprehensive database for unforeseen applications; e. it clearly separates data provisioning from data transformation/ annotation steps, which yields a clear workflow design.we developed sofia specifically for performing multiple types of analysis in next generation sequencing (ngs), such as variant annotation, rna-seq analysis, chip-seq analysis, or epigenetic analysis. common aspects of these experiments are that they (i) map experimental read-outs to parts of a genomic sequence, (ii) require specific information on these genome parts to produce biologically meaningful results and (iii) multiple tools and databases exist for each step in such a pipeline. the design of sofia directly targets these requirements. in this paper, we present our framework for data integration, describe a template that is readily provided with the system, and demonstrate the flexibility and power of sofia by case studies in variant annotation for a breast cancer dataset, comparison of the impact of using different gene models, and an analysis of translation efficiencies using e.coli protein expression data.we developed sofia, a framework that models biological entity annotation as an integration workflow that associates provided input data from high-throughput experiments to requested annotations with little effort on the part of the user. sofia is freely available and comes with a comprehensive template for annotating genomic data which we demonstrate here with three real-life use cases: annotating sequence variants, comparing gene model consequences and calculating sequence features that purportedly estimate translational efficiency.we strongly believe that supporting individualized data annotation is an important yet often neglected matter. even when using the same type of data, two different groups may have different annotation needs depending on their overall research goal. for instance, while some groups may seek information from drug response databases such as drugbank or context specific genetic dependencies such as achilles , others may be more interested in the population genetics which requires comparing their variant set to those from projects like the 1000 genomes project (1000 genomes project consortium et al., 2015) or dbsnp . a third group might study the mechanistic effects of variants for particular types of diseases, which requires them to integrate data from diseases-specific sequencing panels like the cancer genome atlas or the international cancer genome consortium . supporting all such needs with a single, up-to-date data warehouse requires complex schemas, costly maintenance operations, and continuous systems re-engineering to properly integrate novel datasets and types into the schema. the same extensions in sofia would require only programming a series of data transformations, which are later only executed on-demand.  
