genome analysis bigdatascript: a scripting language for data pipelines motivation: the analysis of large biological datasets often requires complex processing pipelines that run for a long time on large computational infrastructures. we designed and implemented a simple script-like programming language with a clean and minimalist syntax to develop and manage pipeline execution and provide robustness to various types of software and hardware failures as well as portability. results: we introduce the bigdatascript (bds) programming language for data processing pipelines, which improves abstraction from hardware resources and assists with robustness. hardware abstraction allows bds pipelines to run without modification on a wide range of computer architectures, from a small laptop to multi-core servers, server farms, clusters and clouds. bds achieves robustness by incorporating the concepts of absolute serialization and lazy processing , thus allowing pipelines to recover from errors. by abstracting pipeline concepts at programming language level, bds simplifies implementation , execution and management of complex bioinformatics pipelines, resulting in reduced development and debugging cycles as well as cleaner code. availability and implementation: bigdatascript is available under open-source license at http://pcingola.github.io/bigdatascript.processing large amounts of data is becoming increasingly important and common in research environments as a consequence of technology improvements and reduced costs of highthroughput experiments. this is particularly the case for genomics research programs, where massive parallelization of microarray-and sequencing-based assays can support complex genome-wide experiments involving tens or hundreds of thousands of patient samples . with the democratization of high-throughput approaches and simplified access to processing resources (e.g. cloud computing), researchers must now routinely analyze large datasets. this paradigm shift with respect to the access and manipulation of information creates new challenges by requiring highly specialized skill, such as implementing data-processing pipelines, to be accessible to a much wider audience. a data-processing pipeline, referred as pipeline for short, is a set of partially ordered computing tasks coordinated to process large amounts of data. each of these tasks is designed to solve specific parts of a larger problem, and their coordinated outcomes are required to solve the problem as a whole. many of the software tools used in pipelines that solve big data genomics problems are cpu, memory or i/o intensive and commonly run for several hours or even days. creating and executing such pipelines require running and coordinating several of these tools to ensure proper data flow and error control from one analysis step to the next. for instance, a processing pipeline for a sequencingbased genome-wide association study may involve the following steps : (i) mapping dna sequence reads obtained from thousands of patients to a reference genome; (ii) identifying genetic changes present in each patient genome (known as calling variants); (iii) annotating these variants with respect to known gene transcripts or other genome landmarks; (iv) applying statistical analyses to identify genetic variants that are associated with differences in the patient phenotypes; and (v) quality control on each of the previous steps. even though efficient tools exist to perform each of these steps, coordinating these processes in a scalable, robust and flexible pipeline is challenging because creating pipelines using general-purpose computer languages (e.g. java, python or shell scripting) involves handling many low-level process synchronization and scheduling details. as a result, process coordination usually depends on specific features of the underlying systems architecture, making pipelines difficult to migrate. for example, a processing pipeline designed for a multi-core server cannot directly be used on a cluster because running tasks on a cluster requires queuing them using cluster-specific commands (e.g. qsub). therefore, if using such a language, programmers and researchers must spend significant efforts to deal with architecture-specific details that are not germane to the problem of interest, and pipelines have to be reprogrammed or adapted to run on other computer architectures. this is aggravated by the fact that the requirements change often and the software tools are constantly evolving. in the context of bioinformatics, there are several frameworks to help implement data-processing pipelines; although a full comparison is beyond the scope of this article, we mention a few that relate to our work: (i) snakemake (k oster and) written as a python domain-specific language (dsl), which has a strong influence from make command. just as in make, the workflow is specified by rules, and dependencies are implied between one rules input files and another rules output files. (ii) ruffus , a python library, uses a syntactic mechanism based on decorations. this approach tends to spread the pipeline structure throughout the code, making maintenance cumbersome , which is also written as a python library, expresses pipelines as graphs drawn using ascii characters. although visually rich, the authors acknowledge that this representation is harder to maintain than the traditional code. (iv) bpipe is implemented as a dsl on top of groovy, a java virtual machine (jvm)-based language. bpipe facilitates reordering, removing or adding pipeline stages, and thus, it is easy for running many variations of a pipeline. (v) nextflow (www.nextflow.io), another groovy-based dsl, is based on data flow programming paradigm. this paradigm simplifies parallelism and lets the programmer focus on the coordination and synchronization of the processes by simply specifying their inputs and outputs. each of these systems creates either a framework or a dsl on a pre-existing general-purpose programming language. this has the obvious benefit of leveraging the languages power, expressiveness and speed, but it also means that the programmer may have to learn the new general-purpose programming language, which can be taxing and take time to master. some of these pipeline tools use new syntactic structures or concepts (e.g. nextflows data-flow programming model or leafs pipeline drawings) that can be powerful, but require programming outside the traditional imperative model, and thus might create a steep learning curve. in this article, we introduce a new pipeline programming language called bigdatascript (bds), which is a scripting language designed for working with big data pipelines in system architectures of different sizes and capabilities. in contrast to existing frameworks, which extend general-purpose languages through libraries or dsls, our approach helps to solve the typical challenges in pipeline programming by creating a simple yet powerful and flexible programming language. bds tackles common problems in pipeline programming by transparently managing infrastructure and resources without requiring explicit code from the programmer, although allowing the programmer to remain in tight control of resources. it can be used to create robust pipelines by introducing mechanisms of lazy processing and absolute serialization, a concept similar to continuations that helps to recover from several types of failures, thus improving robustness. bds runs on any unix-like environment (we currently provide linux and os.x pre-compiled binaries) and can be ported to other operating systems where a java runtime and a go compiler are available. unlike other efforts, bds consists of a dedicated grammar with its own parser and interpreter, rather than being implemented on top of an existing language. our language is similar to commonly used syntax and avoids inventing new syntactic structures or concepts. this results in a quick-to-learn, clean and minimalistic language. furthermore, creating our own interpreter gives better control of pipeline execution and allows us to create features unavailable in general-purpose language (most notably, absolute serialization). this comes at the expense of expressiveness and speed. bds is not as powerful as java or python, and our simple interpreter cannot be compared with sophisticated just-in-time execution or jvm-optimized bytecode execution provided by other languages. nonetheless, in our experience, most bioinformatics pipelines rely on simple programmatic constructs. furthermore, in typical pipelines, the vast majority of the running time is spent executing external programs, making the executing time of the pipeline code itself a negligible factor. for these reasons, we argue that bds offers a good trade-off between simplicity and expressiveness or speed.to illustrate the use of bds in a real-life scenario, we present an implementation of a sequencing data analysis pipeline. this example illustrates three key bds properties: architecture independence, robustness and scalability. the data we analyzed in this example consist of high-quality short-read sequences (200 coverage) of a human genome corresponding to a person of european ancestry from utah (na12877), downloaded from illumina platinum genomes (http://www.illumina.com/platinumgenomes). the example pipeline we created follows current best practices in sequencing data analysis , which involves the following steps: (i) map reads to a reference genome using bwa , (ii) call variants using gatks haplotypecaller and (iii) annotate variants using snpeff and snpsift . the pipeline makes efficient use of computational resources by making sure tasks are parallelized whenever possible.shows a flowchart of our implementation, while the pipelines source code is available at include/bio/seq directory of our projects source code (https://github.com/pcingola/ bigdatascript). architecture independence. we ran the exact same bds pipeline on (i) a laptop computer; (ii) a multi-core server (24 cores, 256 gb shared ram); (iii) a server farm (5 servers, 2 cores each); (iv) a 1200-core cluster; and (v) the amazon aws cloud computing infrastructure . for the purpose of this example and to accommodate the fact that running the pipeline on a laptop using the entire dataset would be prohibitive, we limited our experiment to reads that map to chromosome 20. thethe script is executed from a terminal. the go executable invokes main bds, written in java, performs lexing, parsing, compilation to ast and runs ast. (c) when the task statement is run, appropriate checks are performed. (d) a shell script task1.sh is created, and a bds-exec process is fired. (e) bds-exec reports pid, executed the script task1.sh while capturing stdout and stderr as well as monitoring timeouts and os signals. when a process finishes execution, the exit status is logged architectures involved were based on different operating systems and spanned about three orders of magnitude in terms of the number of cpus (from 4 to 1200) and ram (from 8 gb to 12 tb). bds can also create a cluster from a server farm by coordinating raw ssh connections to a set of computers. this minimalistic setup only requires that the computers have access to a shared disk, typically using nfs, which is a common practice in companies and university networks. in all cases, the overhead required to run the bds script itself accounted for 52 ms per task, which is negligible compared with typical pipeline runtimes of several hours. robustness. to assess bdss robustness, we ran the pipeline on a cluster where $10 of the nodes have induced hardware failures. as opposed to software failures, which are usually detected by cluster management systems, hardware node failures are typically more difficult to detect and recover from. in addition, we elevated the cluster load to 495 to make sure the pipeline was running on less than ideal conditions. as shown in, the pipeline finished successfully without any human intervention and required only 30 more time than in the ideal case scenario because bds had to rerun several failed tasks. this shows how bds pipelines can be robust and recover from multiple failures by using lazy processing and absolute serialization mechanisms. scalability. to assess bdss scalability, we ran exactly the same pipeline on two datasets that vary in size by several orders of magnitude : (i) a relatively small dataset (chromosome 20 subset, $2 gb) that would typically be used for development, testing and debugging and (ii) a high-depth whole-genome sequencing dataset (over 200 coverage, roughly 1.5 tb).we introduced bds, a programming language that simplifies implementing, testing and debugging complex data analysis pipelines. bds is intended to be used by programmers in a similar way to shell scripts, by providing glue for several tools to ensure that they execute in a coordinated way. shell scripting was popularized when most personal computers had a single cpu and clusters or clouds did not exist. one can thus see bds as extending the hardware abstraction concept to data-center level while retaining the simplicity of shell scripting. bds tackles common problems in pipeline programming by abstracting task management details at the programming language level. task management is handled by two statements (task and wait) that hide system architecture details, leadingnotes: running the same bds-based pipeline, a sequence variant calling and analysis pipeline, on the same dataset (chr20) but different architectures, operating systems and cluster management systems.notes: the same sample pipeline run on dataset of 2 gb (reads mapping to human chromosome 20) and 1.5 tb (whole-genome data set). computational times vary according to systems resources, utilization factor and induced hardware failures. to cleaner and more compact code than general-purpose languages. bds also provides two complementary robustness mechanisms: lazy processing and absolute serialization. a key feature is that being architecture agnostic, bds allows users to code, test and debug big data analysis pipelines on different systems than the ones intended for full-scale data processing. one can thus develop a pipeline on a laptop and then run exactly the same code on a large cluster. bds also provides mechanisms that eliminate many boilerplate programming tasks, which in our experience significantly reduce pipeline development times. bds can also reduce cpu usage, by allowing the generation of code with fewer errors and by allowing more efficient recovery from both software and hardware failures. these benefits generally far outweigh the minimal overhead incurred in typical pipelines.  
