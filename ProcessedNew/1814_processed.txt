data and text mining figure summarizer browser extensions for pubmed central biomedical scientists need to access figures to validate research facts and to formulate or test novel research hypotheses. however, figures are difficult to comprehend without associated text (e.g. figure legend and other reference text). our evaluation has shown statistically significant differences in figure comprehension when varying levels of text were provided . when the full-text article is not available, presenting just the figure+legend left biomedical researchers lacking 3968 of the information about a figure as compared to having complete figure comprehension; adding the title and abstract improved the situation, but still left biomedical researchers missing 30 of the information. when the full-text article is available, figure comprehension increased to the highest 8697 . the results indicate that there is information in the abstract and in the full-text that biomedical scientists require to understand the figures that appear in biomedical articles . on the other hand, we also found that the associated text of a figure is typically distributed across the full-text body and is frequently redundant in content . for example, the following three redundant sentences in the abstract, introduction, and caption describe an image of the article (we hypothesize that a succinct and structured summary will allow biomedical researchers to comprehend the figure data efficiently and to access relevant figures in a timely manner.we first found that biomedical researchers prefer a structured summary that follows the imrad format (introduction, methods, results and discussion) . we developed a supervised machine learning classifier to automatically classify sentences appearing in a full-text biomedical articles into the imrad format with an f1-score of 91.55 . we then developed a simple information retrieval approach that selects one sentence from each of the imrad categories based on cosine similarity between the sentence and the image caption . we generated a manual gold-standard for 44 figures in 7 articles by asking annotators to select sentences from the article that best summarize the figure. we then evaluated our figure summarizer using the rouge score (recall-oriented understudy for gisting evaluation) and obtained an average rouge-1 score of 0.70. in comparison, the abstract of the article attained a rouge-1 score of 0.33. in the future, we plan to conduct a cognitive evaluation as we have done in (  
