in silico phenotyping via co-training for improved phenotype prediction from genotype motivation: predicting disease phenotypes from genotypes is a key challenge in medical applications in the postgenomic era. large training datasets of patients that have been both genotyped and phenotyped are the key requisite when aiming for high prediction accuracy. with current genotyping projects producing genetic data for hundreds of thousands of patients, large-scale phenotyping has become the bottleneck in disease phenotype prediction. results: here we present an approach for imputing missing disease phenotypes given the genotype of a patient. our approach is based on co-training, which predicts the phenotype of unlabeled patients based on a second class of information, e.g. clinical health record information. augmenting training datasets by this type of in silico phenotyping can lead to significant improvements in prediction accuracy. we demonstrate this on a dataset of patients with two diagnostic types of migraine, termed migraine with aura and migraine without aura, from the international headache genetics consortium. conclusions: imputing missing disease phenotypes for patients via co-training leads to larger training datasets and improved prediction accuracy in phenotype prediction. availability and implementation: the code can be obtained at: http://www.bsse.ethz.ch/mlcb/ research/bioinformatics-and-computational-biology/co-training.htmlpredicting disease phenotypes from genotypic information of a patient is a key question in medical research, with implications for disease diagnosis, prognosis and therapy. any prediction system, or classifier, relies critically on the existence of a training dataset which includes labeled examples, that is to say, patients for which both genotypic and phenotypic data are present. the limiting factor when creating such training datasets used to be the low number of patients for which genotypic or even full-genome data were available. but experimental advances in genotyping, using genotyping chips (wellcome trust) or nextgeneration sequencing , plus the advent of many large-scale sequencing studies (1000 genomes project) and biobanks that store genotypic information, are steadily changing this situation; gradually, the availability of disease phenotypes is turning into the bottleneck when collecting training datasets for phenotype prediction. automated approaches to phenotyping, such as image phenotyping which extracts features from images, are currently gaining popularity, e.g. in model organisms and plant genetics , but not for all kinds of phenotypes such images are available, e.g. for many human diseases. health record information on patients is collected in growing numbers, both manually and by electronic devices .in order to systematically evaluate the utility of our in silico phenotyping approach, we conducted the following experiments: firstly, we determined the lower and upper bounds for the prediction performance of the algorithm assuming perfect labeling of the data, we then examined how the actual prediction performance of in silico phenotyping compared to these bounds. secondly, the effect of varying the amount of data in the training and co-training sets on the prediction performance of in silico phenotyping was established. finally, using the relative sizes of data splits detailed at the end of section 2.1, we empirically examined the effect of varying the number of snps that are selected in the univariate feature selection (section 2.2.2).  
