genome analysis the human genome contracts again the number of human genomes that have been sequenced completely for different individuals has increased rapidly in recent years. storing and transferring complete genomes between computers for the purpose of applying various applications and analysis tools will soon become a major hurdle, hindering the analysis phase. therefore, there is a growing need to compress these data efficiently. here, we describe a technique to compress human genomes based on entropy coding, using a reference genome and known single nucleotide polymorphisms (snps). furthermore, we explore several intrinsic features of genomes and information in other genomic databases to further improve the compression attained. using these methods , we compress james watsons genome to 2.5 megabytes (mb), improving on recent work by 37. similar compression is obtained for most genomes available from the 1000 genomes project. our biologically inspired techniques promise even greater gains for genomes of lower organisms and for human genomes as more genomic data become available.with the constant advances in sequencing technologies, genome sequencing has become faster and more affordable. although the main effort thus far has been to sequence the genomes of different organisms, the focus is gradually shifting toward sequencing different instances of the same genome (i.e. different individuals) to study the variations underlying phenotypic differences between individuals and to identify the variations that are associated with diseases and disorders. consequently, the number of complete human genomes that have been sequenced is increasing rapidly. as the amount of and demand for genomic data grow, the cost of storage and transmission of these data is fast becoming a bottleneck for research and future medical applications. thus, there is a growing need for compression algorithms suited to genomic data. genome compression has been the subject of multiple studies in the past several years (see supplementary material for an overview of these related studies). the most successful methods are those that use reference genomes and code just the differences between the input genome and a reference genome (which, for humans, account for 50.2 of the genomes length). the best single-reference compression was reported in, who compressed james watsons genome to 4 mb by utilizing dbsnp to represent more efficiently known snps in the difference map. in this work, we report further improvements to this scheme, which result in a significant reduction of 37 in the size of the compressed genome, for only 2.5 mb. our work is motivated by two observations. first, entropy-coding techniques can be nearly optimal in exploiting known patterns in a dataset but have not been fully optimized on human genome data. second, in finding patterns to exploit, a wealth of biological insight remains untapped. our improved scheme is the result of incorporating multiple sources of information on the presence of haplotypes, tag snps, coding and non-coding regions and other biologically motivated modifications.we applied our compression tool to each of the 1092 human genomes available from the 1000 genomes project (1000 genomes), as of october 2012. the results are presented in, showing the uncompressed file sizes (converted to the same uncompressed format as watsons genome), and the compression ratios, labeled by population. these results confirm that the success of the algorithm in compressing genomes is not unique to watsons genome. however, we notice a significant variation by population in both the uncompressed file size and the compression ratio achieved by our approach. further analysis is required to understand the functional significance of the underlying differences. it should be noted that the 1000 genomes project provides allele-specific information on variations, while the publicly available difference map for watsons genome combines the variations into a single list, without specifying which allele(s) they occur at. therefore, we modified our algorithm to account for diploids, by introducing a vector indicating homo/heterozygosity (and which allele, in case of heterozygosity). this is done by using two bits per variation, which can take the value 11 (homozygosity, both alleles have the variation), 01 or 10. coding the allele indicator vector takes $500 kb per genome, using a huffman code for a markov chain fit to the indicator vector (described in greater detail in the supplementary material). another difference between the 1000 genomes and watsons genome is that the 1000 genomes project lists indels in addition to snps, deletions and insertions. the number of indels per genome is small (on average, there are only 179 indels out of $3.8m variations per genome in the 1000 genomes data), and we represent each as a pair of an insertion and a deletion.  
