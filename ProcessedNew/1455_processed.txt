data and text mining dnorm: disease name normalization with pairwise learning to rank motivation: despite the central role of diseases in biomedical research, there have been much fewer attempts to automatically determine which diseases are mentioned in a textthe task of disease name normalization (dnorm)compared with other normalization tasks in biomedical text mining research. methods: in this article we introduce the first machine learning approach for dnorm, using the ncbi disease corpus and the medic vocabulary, which combines mesh and omim. our method is a high-performing and mathematically principled framework for learning similarities between mentions and concept names directly from training data. the technique is based on pairwise learning to rank, which has not previously been applied to the normalization task but has proven successful in large optimization problems for information retrieval. results: we compare our method with several techniques based on lexical normalization and matching, metamap and lucene. our algorithm achieves 0.782 micro-averaged f-measure and 0.809 macro-averaged f-measure, an increase over the highest performing baseline method of 0.121 and 0.098, respectively. availability: the source code for dnorm is available at http://www. ncbi.nlm.nih.gov/cbbresearch/lu/demo/dnorm, along with a web-based demonstration and links to the ncbi disease corpus. results on pubmed abstracts are available in pubtator: http://www.ncbi.nlm. nih.gov/cbbresearch/lu/demo/pubtatordiseases are central to many lines of biomedical research, and enabling access to disease information is the goal of many information extraction and text mining efforts (islamaj dog an and). the task of disease normalization consists of finding disease mentions and assigning a unique identifier to each. this task is important in many lines of inquiry involving disease, including etiology (e.g. genedisease relationships) and clinical aspects (e.g. diagnosis, prevention and treatment). disease may be defined broadly as any impairment of normal biological function . given the wide range of concepts that may thus be categorized as diseasestheir respective etiologies, clinical presentations and their various histories of diagnosis and treatmentdisease names naturally exhibit considerable variation. this variation presents not only in synonymous terms for the same disease, but also in the diverse logic used to create the disease names themselves. disease names are often created by combining roots and affixes from greek or latin (e.g. hemochromatosis). a particularly flexible way to create disease names is to combine a disease category with a short descriptive modifier, which may take many forms, including anatomical locations (breast cancer), symptoms (cat-eye syndrome), treatment (dopa-responsive dystonia), causative agent (staph infection), biomolecular etiology (g6pd deficiency), heredity (x-linked agammaglobulinemia) or eponyms (schwartz-jampel syndrome). modifiers are also frequently used to provide description not part of the name (e.g. severe malaria). when diseases are mentioned in text, they are frequently also abbreviated, exhibit morphological or orthographical variations, use different word orderings or use synonyms. these variations may involve more than single word substitutions. for example, because affixes are often composed, a single word (oculocerebrorenal) may correspond to multiple words (eye, brain and kidney) in another form. the disease normalization task is further complicated by the overlap between disease concepts, forcing systems that locate and normalize diseases in natural language text to balance handling name variations with differentiating between concepts to achieve good performance. previous works addressing disease name normalization (dnorm) typically use a hybrid of lexical and linguistic approaches (islamaj dog an and lu, 2012b;). while string normalization techniques (e.g. case folding, stemming) do allow some generalization, the name variations in the lexicon always impose some limitation. machine learning may enable higher performance by modeling the language that authors use to describe diseases in text; however, there have been relatively few attempts to use machine learning in normalization, and none for disease names. in this work we use the ncbi disease corpus (islamaj dog an and), which has recently been updated to include concept annotations (islamaj dogan et al., unpublished data), to consider the task of disease normalization. we describe the task as follows: given an abstract, return the set of disease concepts mentioned. our current purpose is to support entityspecific semantic search of the biomedical literature and computer-assisted biocuration, especially document triage (in this article we introduce dnorm, the first machine learning method to normalize disease names in biomedical text. our technique learns the similarity between mentions and concept names directly from the training data, thereby focusing on the candidate generation phase of normalization. our technique can learn arbitrary mappings between mentions and names, including synonymy, polysemy and relationships that are not 1-to-1. moreover, our method specifically handles abbreviations and word order variations. our method is based on pairwise learning to rank (pltr), which has been successfully applied to large optimization problems in information retrieval , but to the best of our knowledge has not previously been used for concept normalization.during development, all techniques were evaluated using the development subset of the ncbi disease corpus. varying the learning rate demonstrated 10 4 to provide the highest performance on the development set, and this is the setting used for all experiments reported in this section. final evaluation was performed using the test subset of the ncbi disease corpus. our evaluation considers only the set of disease concepts found within each abstract, ignoring the exact location(s) where each concept was found. thus, the number of true positives in an abstract is the size of the intersection between the set of concepts annotated in the gold standard and the set of concepts returned by the system. the number of false negatives and false positives are defined analogously. our result measures are precision, recall and f-measure, which were calculated as follows:micro-averaged results were calculated by summing the number of true positives, false positives and false negatives over the entire evaluation set. macro-averaged results were determined from the number of true positives, false positives and false negatives for each abstract, and the mean result was calculated across all abstracts.reports the evaluation results for dnorm and all baseline methods, using micro-averaged performance.reports the results for the same experiments using macroaveraged performance.reports the recall for the banner lucene, banner cosine similarity and dnorm (banner pltr) experiments if we return more than the highest scoring result from the candidate generation. we created our own implementation of pltr using the colt matrix library (http://acs.lbl.gov/software/colt). the implementation enables high performance by taking advantage of the sparsity of the mention and name vectors, training on the ncbi disease corpus training subset in 51 h using a single 2.80 ghz intel xeon processor, limited to 10 gb memory. our implementation scores one mention against the nearly 80 000 names in the lexicon in $25 ms using the same equipment. we have applied dnorm to all pubmed abstracts and made the results publicly available in pubtator .though the nlm lexical normalization method has higher recall than any method besides dnorm, the precision remainsthe remaining methods use separate stages for ner and normalization; because all use banner for ner, the errors caused by the ner component are the same. the remaining methods also use abbreviation resolution, significantly reducing the number of false positives caused by ambiguous abbreviations. the inference method handles term variations by using string similarity and lucene search, though it tends to select highly specific concepts, such as mapping inherited disorders to blood coagulation disorders, inherited (mesh:d025861). analyzing the errors made by banner lucene but not by banner cosine similarity shows that most are due to the lucene scoring function insufficiently penalizing lexicon names containing tokens not present in the mention. the majority of the errors made by banner cosine similarity but not by dnorm are due to term variation. because banner lucene, banner cosine similarity and dnorm (banner pltr) use the same processing pipeline, the performance difference between these methods is solely due to the normalization methodology. in addition, because the scoring function for cosine similarity is equivalent to the one used by dnorm before training, the performance difference between these methods is solely due to the weights learned during training. to further isolate the effect of pltr training on performance, we performed a normalization experiment comparing lucene, cosine similarity and pltr using the gold-standard mentions from the ncbi disease corpus test subset as input instead of the mentions found by banner. we again used the pltr model trained using 10 4. in this comparison, we count a result as correct if the concept associated with the lexicon name scored highest by dnorm matched the annotated concept for the mention. out of the 960 mentions, lucene found 674 (70.2), cosine similarity found 687 (71.6) and pltr found 789 (82.2). this experiment confirms the effectiveness of the novel learning procedure used by dnorm. we performed an experiment to demonstrate the effect that varying the learning rate has on training time and performance. we varied exponentially between 10 2 and 10 8 , and report the results in. the best performance was achieved with 10 4 , which required a training time of 48.8 min and resulted in a micro-averaged f-measure of 0.782. while the final performance is similar over a wide range of values for , the training time varied widely, ranging from 511 min to 477 h, with smaller values requiring longer training times.we have shown that pltr successfully learns a mapping from disease name mentions to disease concept names, resulting in a significant improvement in normalization performance. we have also shown that the training time requirements are modest and that inference time is fast enough for use online. our approach models many kinds of term variations, learning the patterns directly from training data. our error analysis showed that ner is a continued concern, and the analysis of the learned weight matrix showed that morphological analysis is important for this problem. our technique primarily addresses the candidate generation step in normalization, and could be paired with more sophisticated techniques for disambiguation. we believe that pltr may prove to be sufficiently useful and flexible to be applicable to normalization problems in general.while general applicability should be verified in future work, the present article represents an attempt to move toward a unified framework for normalizing biomedical entity mentions with machine learning.  
