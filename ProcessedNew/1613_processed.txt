error correction of high-throughput sequencing datasets with non-uniform coverage motivation: the continuing improvements to high-throughput sequencing (hts) platforms have begun to unfold a myriad of new applications. as a result, error correction of sequencing reads remains an important problem. though several tools do an excellent job of correcting datasets where the reads are sampled close to uniformly, the problem of correcting reads coming from drastically non-uniform datasets, such as those from single-cell sequencing, remains open. results: in this article, we develop the method hammer for error correction without any uniformity assumptions. hammer is based on a combination of a hamming graph and a simple probabilistic model for sequencing errors. it is a simple and adaptable algorithm that improves on other tools on non-uniform single-cell data, while achieving comparable results on normal multi-cell data.the continuing improvements to high-throughput sequencing (hts) platforms have begun to unfold a myriad of new and exciting applications such as transcriptome analysis, metagenomics, singlecell assembly and variation detection. in addition, classical problems such as assembly are becoming feasible for large-scale endeavours, such as the genome 10k project which seeks to assemble the genomes of 10 000 novel species (genome 10k community of). all these projects face the same difficulty of handling the base errors that are inevitably prevalent in hts datasets. error correction of reads has thus come to the forefront as an essential problem, with a slew of publications in the last 2 years . a common approach of error correction of reads is to determine a threshold and correct k-mers whose multiplicities fall below the threshold . choosing the correct threshold is crucial since a low threshold will result in too many uncorrected errors, while a high threshold will result in the loss of correct k-mers. the histogram of the multiplicities of k-mers will show a mixture of two distributionsthat of the error-free k-mers, and that of the erroneous k-mers. when the coverage is high and uniform, these distributions are centered far apart and can be to whom correspondence should be addressed. separated without much loss using a cutoff threshold; such methods therefore achieve excellent results . though hts platforms provide relatively uniform coverage in many standard sequencing experiments, in some of the more challenging applications, such as transcriptome sequencing, the coverage remains drastically uneven . another prominent emerging example is that of singlecell sequencing, which has enabled the investigation of a diverse range of uncultivated bacteria, from the surface ocean environment to the human body . though these bacteria are not amenable to normal sequencing, recent advances in dna amplification technology have enabled genome sequencing directly from individual cells without requiring growth in culture . the read datasets thus obtained from single-cells suffer from amplification bias, resulting in orders of magnitude difference in coverage and the complete absence of coverage in some regions. previous, single-cell studies have used error correction tools that assume near-uniform coverage. however, such approaches are not as effective and result in decreased quality of assemblies when the two multiplicity distributions of erroneous and error-free k-mers are not separable using a simple threshold. thus, despite the initial technological difficulties, the challenges facing single-cell genomics are increasingly computational rather than experimental . for applications such as these, it becomes paramount to develop better error-correction algorithms that do not assume uniformity of coverage. recent papers have introduced a powerful idea that has the potential to remove uniformity assumptions . consider two k-mers x and y that are within a small hamming distance and present in the read dataset. if our genome does not contain many non-exact repeats, then it is likely that both x and y were generated by the k-mer among them that has higher multiplicity. in this way, we can correct the lower multiplicity k-mer to the higher multiplicity one, without relying on uniformity. this can be further generalized by considering the notion of the hamming graph, whose vertices are the distinct k-mers (annotated with their multiplicities) and edges connect k-mers with a hamming distance less than or equal to a parameter . in this article, we develop the method hammer for error correction without any uniformity assumptions. hammer is based on a combination of the hamming graph and a simple probabilistic model. it is a simple and adaptable algorithm that improves onpage: i138 i137i141to test the effectiveness of hammer on non-uniform datasets, we used reads generated from a single-cell of e.coli k-12 strain, using one lane of the illumina gaii pipeline (, submitted for publication). they total 600 coverage mostly by 100 bp long reads. we mapped the reads to the e.coli genome using bowtie and found extremely non-uniform coverage . there were 94k positions that are not covered by any read, with an additional 116k being covered by less than six reads. in contrast, in an alternate dataset that was generated on a normal multi-cell sample with the same coverage and read length, there were no uncovered regions. prior to working with this dataset, we trimmed the first three bases and any trailing bases with a quality value of 2; we then removed any remaining reads with ambiguous bases. we use a value of k = 55 to run hammer on this dataset, motivated by the fact that it was found to work best for assemblers on this data (2011, submitted for publication). the distance used in building the hamming graph was set to two, since we found larger values did not significantly decrease the number of singleton clusters. for the singletoncutoff, we used the default value of 1, which, roughly speaking, means that any singleton k-mer that appears at least twice with decent quality values is kept. however, to demonstrate that hammer can be made super-sensitive if needed, we also ran hammer with a singletoncutoff of 0.5. for the savecutoff, we used a value of 10. the results are shown inand. to measure the quality of a set of reads, we measure its sensitivity and positive predictive value (ppv) with respect to the source genome. let k be the set of distinct k-mers present in the reads, and let s be the kspectrum of e.coli (i.e. the set of all distinct k-mers). the sensitivity is measured as ks s and reflects the percentage of k-mers from the genome that are present in the reads. the positive predictive value is ks k and reflects the fraction of k-mers in the dataset that do not contain errors. note that we do not measure the quality of the corrected reads against the quality of uncorrected reads directly, but by measuring the quality of each separately against the e.coli genome.despite the success of many error-correction tools, emerging areas such as single-cell sequencing fuel the need to develop better algorithms for situations of non-uniform coverage. recent approaches have taken steps in the right direction by looking together i140  
