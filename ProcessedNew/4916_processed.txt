when less is more: 'slicing' sequencing data improves read decoding accuracy and de novo assembly quality motivation: as the invention of dna sequencing in the 70s, computational biologists have had to deal with the problem of de novo genome assembly with limited (or insufficient) depth of sequenc-ing. in this work, we investigate the opposite problem, that is, the challenge of dealing with excessive depth of sequencing. results: we explore the effect of ultra-deep sequencing data in two domains: (i) the problem of decoding reads to bacterial artificial chromosome (bac) clones (in the context of the combinatorial pooling design we have recently proposed), and (ii) the problem of de novo assembly of bac clones. using real ultra-deep sequencing data, we show that when the depth of sequencing increases over a certain threshold, sequencing errors make these two problems harder and harder (instead of easier, as one would expect with error-free data), and as a consequence the quality of the solution degrades with more and more data. for the first problem, we propose an effective solution based on divide and conquer: we slice a large dataset into smaller samples of optimal size, decode each slice independently, and then merge the results. experimental results on over 15 000 barley bacs and over 4000 cowpea bacs demonstrate a significant improvement in the quality of the decoding and the final assembly. for the second problem, we show for the first time that modern de novo assemblers cannot take advantage of ultra-deep sequencing data. availability and implementation: python scripts to process slices and resolve decoding conflicts are available fromwe have recently introduced in a novel protocol for clone-by-clone de novo genome sequencing that leverages recent advances in combinatorial pooling design (also known as group testing). in our sequencing protocol, subsets of non-redundant genome-tiling bacterial artificial chromosomes (bacs) are chosen to form intersecting pools, then groups of pools are sequenced on an illumina sequencing instrument via low-multiplex (dnabarcoding). sequenced reads can be assigned/decoded to specific bacs by relying on the combinatorial structure of the pooling design: since the identity of each bac is encoded within the pooling pattern, the identity of each read is similarly encoded within the pattern of pools in which it occurs. finally, bacs are assembled individually, simplifying the problem of resolving genome-wide repetitive sequences. in , we reported preliminary assembly statistics on the performance of our protocol in four barley (hordeum vulgare) bac sets (hv3hv6). further analysis on additional barley bac sets and two genome-wide bac sets for cowpea (vigna unguiculata) revealed that the raw sequence data for some datasets was of significantly lower quality (i.e. higher sequencing error rate) than others. we realized that our decoding strategy, solely based on the software hashfilter , was insufficient to deal with the amount of noise in poor quality datasets. we attempted to (i) trim/clean the reads more aggressively or with different methods, (ii) identify low quality tiles on the flow cell and remove the corresponding reads (e.g. tiles on the bottom middle swath), (iii) identify positions in the reads possibly affected by sequencing bubbles and (iv) post-process the reads using available error-correction software tools (e.g. quake, reptile). unfortunately, none of these steps accomplished a dramatic increase in the percentage of reads that could be assigned to bacs, indicating that the quality of the dataset did not improve very much. these attempts to improve the outcome led however, to a serendipitous discovery: we noticed that when hashfilter processed only a portion of the dataset, the proportion of assigned/decoded reads increased. this observation initially seemed counterintuitive: we expected that feeding less data into our algorithm meant that we had less information to work with, thus decrease the decoding performance. instead, the explanation is that when data is corrupted, more (noisy) data is not better, but worse. the study reported here directly addresses the observation that when dealing with large quantities of imperfect sequencing data, less can be more. more specifically, we report (i) an extensive analysis of the trade off between the size of the datasets and the ability of decoding reads to individual bacs; (ii) a method based on slicing datasets that significantly improves the number of decoded reads and the quality of the resulting bac assemblies; (iii) an analysis of bac assembly quality as a function of the depth of sequencing, for both real and synthetic data. our algorithmic solution relies on a divide-and-conquer approach, as illustrated in.once all the decoded reads are assigned to 13 bacs using the procedure above, velvet is executed to assemble each bac individually. as was done in , we generated multiple assembly for several choices of velvets l-mer (hash) size (2579, step of 6). the assembly reported is the one that maximizes the n50 (n50 indicates the length for which the set of all contigs of that length or longer contains at least half of the total size of all contigs). we employed several metrics to evaluate the improvement in read decoding and assembly enabled by the slicing algorithm. for one of the barley sets (hv10) we executed hashfilter using several choices of k (kon the full 748 m reads dataset (i.e. with no slicing) as well as with k 32 using the slicing algorithm described ealrier. the first five rows ofsummarize the decoding results. first, observe that as we increase k, the number of decoded reads increases monotonically. however, if one fixes k (in this case k 32, which is the maximum allowed by hashfilter), slicing hv10 in 4 slices of 4 m reads increases significantly the number of decoded reads (84.60 compared with 77.19) available for assembly. analysis of the number of assignments to ghost bacs also shows significant improvement in the decoding accuracy when using slicing: 0.000086 of the reads are assigned to unused bac signatures compared with 0.0003050.001351 when hashfilter is used on the full dataset. we carried out a similar analysis on slicing sequencing data improves read decoding and assemblyhv9: when the full dataset was processed with hashfilter (k 26), the number of reads assigned to ghost bacs was very high, 1.9 m reads out of 196 m (0.9653). when the optimal slicing is used (k 32), only 19 140 reads out of 516 m are assigned to ghost bacs (0.0037). also, observe inhow the improved decoding affects the quality of the assembly for hv10. when comparing no slicing to slicing-based decoding, the average n50 jumps from 12 260 to 42 819 bp (both for k 32) and the number of reads used by velvet in the assembly increases from 86.7 to 90.7. for hv10, we also measured the number of decoded reads that map (with 0, 1, 2 and 3 mismatches) to the assembly of a subset of 26 bacs that are available from .reports the average percentage of decoded reads (either from the full dataset or from the optimal slicing) that bowtie can map to the 454-based assemblies. observe how the slicing step improves by 67 the number of reads mapped to the corresponding bac assembly, suggesting a similar improvement in decoding accuracy. similar improvements in decoding accuracy were observed on the other datasets (data not shown). on hv8, we investigated the effect of the slice size on the decoding and assembly statistics: earlier we claimed that the optimal size corresponds to the peak of the graphs in. for instance, notice that the peak for hv8 is 2 m reads. we decoded and assembled reads using slicing sizes of 2 m reads as well as (non-optimal) slice size of 3 m reads. the experimental results are shown in. observe that the decoding with 3 m does not achieve the same decoding accuracy or assembly quality of the slicing with 2 m, but again both are significantly better than without slicing. again, notice inhow improving the read decoding affects the quality of the assembly. the average n50 increases from 4126 bp (k 26, no slicing) to 34 262 bp (k 32, optimal slicing) and the number of reads used by velvet in the assembly increases from 55.6 to 91.2, respectively. for hv8, 207 genes were known to belong to a specific bac clone : the assembly using slicing-based coding recovered at least 50 of the sequence of 187 190 of them, compared with 178 using no slicing. finally, we compared the performance of our slicing method against the experimental results in , which were obtained by running hashfilter with no data slicing (k 26). the basic decoding and assembly statistics when no slicing is used are reported in. first, observe the large variability of results among the 10 sets. although the average number of decoded reads for k 26 is 460 m, there are sets which have less than half that amount (hv6 and hv9) and sets have more than twice the average (e.g. hv3). as a consequence, the average fold-coverage ranges from 72 (hv6) to 528 (hv10). in general, the assembly statistics (without slicing-based decoding) are not very satisfactory: the n50 ranges from 2630 (hv9) to 8190 bp (hv3); the percentage of reads used by velvet ranges from 66.0 (hv9) to 85.9 (hv3 and hv4); the percentage of known genes covered at least 50 of their length by the assemblies ranged from 66 (hv4) to 97 (hv3). when we decoded the same 10 datasets using the optimal slice size (using this time k 32) the assemblies improved drastically. the decoding and assembly statistics are summarized in: note that each set has its optimal size and the corresponding number of slices. first observe how the number of decoded reads increased significantly for most datasets (e.g. 330785 m for hv7, 289669 m for hv8, 209516 m for hv9, 369907 m for vu1 and 448695 m for vu2). only for two datasets the number of decoded reads decreased slightly (by 12 m reads in hv5, and by 44 m in hv10). for all the datasets, the average n50 increased significantlyfrom an average of about 5.7 to 30 kbp (see supplementary dataset 2 for detailed assembly statistics on each dataset). even for datasets for. decoding and assembly statistics for hv8: comparing no slicing and slicing with two different slice sizes (2 m reads is optimal according to the peak in)which slicing decreased the number reads (hv5 and hv10), the n50 increased significantly. the number of reads used by velvet increased from an average of 7792; the fraction of known genes that were recovered by the assemblies increased from 81 to 85. we recognize that the improvement from tables 4 to 5 is not just due to the slicing, but also to the increased k (from 26 to 32). we have already addressed this point in3, where we showed that increasing k from 26 to 32 helps the decoding/assembly but the main boost in accuracy and quality is due to slicing. recall that the assemblies in tables 4 and 5 were carried out using velvet with l 25; 31;. .. ; 79 and choosing the assembly with the largest n50. on the hv3 dataset, we have also tested velvet with fixed l 49, spades with l 31; 33;. .. ; 79, and idbaud with l 31; 33;. .. ; 79 (see supplementary). velvet (best n50) and spades performance were comparable, while idba-ud achieved lower n50. we also tested velvet with l 49, and spades with l 31; 33;. .. ; 79 on all the other datasets (supplementary). setting l 49 for velvet led to less bloated assemblies, somewhat comparable to spades output.as a final step, we investigated how the depth of sequencing affects bac assembly quality. to this end, we multiplexed 16 barley bacs on one lane of the illumina hiseq2000, using custom multiplexing adapters. the size of these bacs ranged 70185 kbp (see supplementary). after demultiplexing the sequenced reads, we obtained 34.4 m 92-bases paired-end reads (insert size of 275 bases). we quality-trimmed the reads, then cleaned them of spurious sequencing adaptors; finally reads affected by e.coli contamination or bac vector were discarded. the final number of cleaned reads was 23.1 m, with an average length of 88 bases. the depth of sequencing for the 16 bacs ranged from 6600 to 27 700 (see supplementary). another set of 52 barley bacs was sequenced by the department of energy joint genome institute using sanger long reads. all bacs were sequenced and finished using phred/phrap/ consed to a targeted depth of 10. the primary dna sequences for each of these 52 bacs were assembled in one contig, although two of them were considered partial sequence. the intersection between the set of 16 bacs sequenced using the illumina instrument and the set of 52 bacs sequenced using sanger is a set of seven bacs (highlighted in bold in supplementary), but one of these seven bacs is not full-length (052l22). we used the six full-length sanger-based bac assemblies as the ground truth to assess the quality of the assemblies from illumina read at increasing depth of sequencing. to this end, we generated datasets corresponding to 100,, 7000 and 8000 depth of sequencing (for each of the six bacs), by sampling uniformly short reads from the high-depth datasets. for each choice of the depth of sequencing, we generated 20 different datasets, for a total of 1200 datasets. we assembled the reads on each dataset with velvet v1.2.09 (with hash value k 79 to minimize the probability of false overlaps) and collected statistics for the resulting assemblies.shows the value of n50 (a), the size ofthe largest contig (b), the percentage of the target bac not available in the assembly (c) and number of assembly errors (d) for increasing depth of sequencing. each point in the graph is the average over the 20 datasets, and error bars indicate the sd. in order to compute the number of assembly errors we used the tool developed for the gage competition . according to gage, the number of assembly errors is defined as the number of locations with insertion/deletions of at least six nucleotides, plus the number of translocations and inversions. a few observations onare in order. first, note that both the n50 and the size of the longest contig reach a maximum in the 5002000 range, depending on the bac. also observe that in order to minimize the percentage of bac missed by the assembly one needs to keep the depth of sequencing below 2500 (too much depth decreases the coverage of the target). finally, it is very clear from(d) that as the depth of sequencing increases so do the number of assembly errors (with the exception of one bac). we have also investigated whether similar observations could be drawn for other assemblers. in, we report the same assembly statistics, namely (a) the value of n50, (b) the size of the largest contig, (c) the percentage of the target bac not available in the assembly and (d) number of assembly errors for increasing depth of sequencing for one of the bacs. this time we used three assemblers, namely velvet, spades v3.1.1 and idba-ud (statistics for all bacs are available in supplementary figs s36). although there are performance differences among the three assemblers, the common trend is that as the coverage increases, the n50 and the size of the largest contig decreases, while the percentage of the bac missing and the number of assembly errors increases. among the three assemblers, spades appears to be less affected by high coverage. spades was run with hash values k 25, 45, 65 and option careful (other parameters were default). idba-ud was run with hash values k 25, 45, 65 (other parameters were default). the reported assembly is the one chosen by idba-ud.independently from us, the authors of made similar observations on assembly degratadation. in their study, the authors assembled e.coli (4.6 mb), saccharomyces kudriavzevii (11.18 mb) and caenorhabditis. elegans (100 mb) using soapdenovo, velvet, abyss, meraculous and idba-ud at increasing sequencing depths up to 200. their analysis showed that the optimum-sequencing depth for assembling these genomes is about 100, depending on the specific genome and assembler. finally, we analyzed the performance of idba-ud, spades and velvet on simulated reads. we generated 100 bp 2 paired-end reads from the sanger assembly of bac 574b01 using the read simulator wgsim (github.com/lh3/wgsim) at 100, 250, 500, 1000, 2000, 3500, 5000, 6000, 7000 and 8000 depth of sequencing. insert length was 250 bp, with a standard deviation of 10 bp. for each depth of sequencing, we generated simulated reads at 0, 0.5, 1 and 2 sequencing error rate (substitutions). insertions and deletions were not allowed. idba-ud was executed with hash values k 25, 45, 65 (other parameters were default). velvet was run with k 49. we repeated the simulations 20 times for idba-ud and 10 times for velvet and spades. in supplementary9, we report the usual assembly statistics, namely n50, largest contig, percentage missing, and number of assembly errors for velvet, idba-ud and spades on these datasets. observe that with perfect reads (0 error rate), ultra-deep coverage does not affect the performance of idba-ud and velvet. with higher and higher sequencing errors, however, similar behaviors to the assembly of real data can be observed for idba-ud and velvet: n50 and longest contig rapidly decrease, and missing portions of the bac and number of mis-assemblies increase. surprisingly, spades seems to be immune to higher sequencing error rates.because the introduction of dna sequencing in the 70s, scientists had to come up with clever solutions to deal with the problem ofde novo genome assembly with limited depth of sequencing. as the cost of sequencing keeps decreasing, one can expect that computational biologists will have to deal with the opposite problem: excessive amount of sequencing data. the lander-waterman-roach theory has been the theoretical foundation to estimate gap and contig lengths as a function of the depth of sequencing. we do not have a theory that would explain why the quality of the assembly starts degrading when the depth is too high. possible factors include the presence (in real data) of chimeric reads, sequencing errors, and read duplications, or their combination thereof. in this study, we report on the de novo assembly of bac clones, which are relatively short dna fragments (100150 kbp). with current sequencing technology it is very easy to reach depth of sequencing in the range of 100010 000 and study how the assembly quality changes as the amount of sequencing data increases. our experiments show that when the depth of sequencing exceeds a threshold the overall quality of the assembly starts degrading . this appears to be a common problem for several de novo assemblers . the same behavior is observed for the problem of decoding reads to their source bac , which is the main focus of this article. the important question is how to deal with the problem of excessive sequencing depth. for the decoding problem we have presented an effective divide and conquer solution: we slice the data in subsamples, decode each slice independently, then merge the results. in order to handle conflicts in the bac assignments (i.e. reads that appear in multiple slices that are decoded to different sets of bacs), we devised a simple set of voting rules. the question that is still open is what to do for the assembly problem: one could assemble slices of the data independently, but it is not clear how to merge the resulting assemblies. in general, we believe that the problem of de novo sequence assembly must be revisited from the ground up under the assumption of ultra-deep coverage.  
