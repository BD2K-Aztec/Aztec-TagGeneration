bayesian variable selection for binary outcomes in high-dimensional genomic studies using non-local priors motivation: the advent of new genomic technologies has resulted in the production of massive data sets. analyses of these data require new statistical and computational methods. in this article, we propose one such method that is useful in selecting explanatory variables for prediction of a binary response. although this problem has recently been addressed using penalized likelihood methods, we adopt a bayesian approach that utilizes a mixture of non-local prior densities and point masses on the binary regression coefficient vectors. results: the resulting method, which we call imomlogit, provides improved performance in identifying true models and reducing estimation and prediction error in a number of simulation studies. more importantly, its application to several genomic datasets produces predictions that have high accuracy using far fewer explanatory variables than competing methods. we also describe a novel approach for setting prior hyperparameters by examining the total variation distance between the prior distributions on the regression parameters and the distribution of the maximum likelihood es-timator under the null distribution. finally, we describe a computational algorithm that can be used to implement imomlogit in ultrahigh-dimensional settings (p n) and provide diagnostics to assess the probability that this algorithm has identified the highest posterior probability model. availability and implementation: software to implement this method can be downloaded at:recent developments in bioinformatics and cancer genomics have made it possible to measure thousands of genomic variables that might be associated with the manifestation of cancer. the availability of such data has resulted in a pressing need for the development of statistical methods to use these data to identify variables that are associated with binary outcomes (e.g. cancer or control, survival or death). the topic of this article is a statistical model for identifying, from a large number p of potential feature vectors, a sparse subset that are useful in predicting a binary outcome vector. throughout this article, we assume that the binary vector of interest is denoted by y, and that the matrix of potential explanatory variables is denoted by x. letting x k denote the submatrix of x containing the true predictors, we assume thatwhere f denotes a known binary link function (assumed to be the logistic distribution in what follows), and p is the n vector of success probabilities for y. the regression coefficient b k represents the nonzero regression effect for each column of x k in predicting p. the primary statistical challenge addressed in this article is the selection of the submatrix x k to be used for the prediction of p. a number of related methods have been proposed to address this problem. these include the lasso , which is a penalized likelihood method that maximizes a product of the binary likelihood function implied by (1) and a constraint on the sum of the absolute value of components of the regression coefficient b k. a closely related method called smoothly clipped absolute deviation (scad) uses a non-convex penalty function and has been demonstrated to have certain oracle properties in idealized asymptotic settings. other penalized likelihood functions include the adaptive lasso and the dantzig selector ; these methods share asymptotic properties similar to scad. in ultrahigh-dimensions (p n), an effective computational technique for implementing the techniques described above is the iterative sure independence screening (isis) procedure , which iteratively performs a correlation screening step to reduce the number of explanatory variables so that penalized likelihood methods can be applied. isis has been used in conjunction with several penalized likelihood methodsincluding adaptive lasso , the dantzig selector , and scad to perform model selection. a number of bayesian methods have also been proposed for variable selection. notable among these are the approaches proposed by, which used a mixture-of-normals approximation to spike-and-slab priors on the regression coefficients.proposed a hierarchical probit model along with mcmc based stochastic search to perform gene selection in high-dimensional settings using a latent response variable and gaussian priors on model coefficients.provided a bayesian approach to this problem employing singular value regression and classes of informative prior distributions to estimate coefficients in high-dimensional settings.studied mixtures of g priors for bayesian variable selection as an alternative to default g priors to overcome several consistency issues associated with the default g prior densities. along more similar lines,studied the utilization of non-local priors in bayesian classifiers where they also address the problem of identifying variables with high predictive power. except for, each of the bayesian methods described above impose local prior densities on regression coefficients in the true model. that is, the prior density on the regression coefficients has a positive prior density function at 0 (and in most cases has its mode at 0), which from a bayesian perspective makes it more difficult to distinguish between models that include regression coefficients that are close to 0 and those that do not. johnson and rossell (2012) proposed two new classes of non-local prior densities to ameliorate this problem. in the model selection context, non-local prior densities are 0 when a regression coefficient in the model is 0. this makes it easier to distinguish between coefficients that do not have an impact on the prediction of y from those that do.used a markov chain monte carlo (mcmc) algorithm to sample from the posterior distribution on the model space; the convergence properties of this algorithm were studied in johnson (2013). the primary goal of this article is to extend the methodology proposed infor application to binary outcomes and to compare the performance of this algorithm to leading penalized likelihood methods. in addition, we describe a default procedure for setting the hyperparameters (i.e. tuning parameters) in the non-local priors, and we examine a numerical strategy for identifying the highest posterior probability model (hppm).to investigate the performance of the proposed model selection procedure, we applied our procedure to both simulated data sets and real data. we compared the performance of our algorithm to isisscad in both real and simulated data because isis-scad has proven to be among the most successful model selection procedures used in practice. for the real data analyses, we also compared our method to another bayesian procedure based on the product moment prior .bayesian variable selection for binary outcomesin this article, we introduced a bayesian method, imomlogit, for variable selection in binary response regression problems in high and ultrahigh-dimensional settings. there are many applications associated with these type of data. such data are of great interest to bioinformaticians and biologists, who routinely collect gene expression data to find prognostic features to classify cancer types. for two real datasets, imomlogit identified sparse models with low prediction error rates. in both cases, biological considerations suggest that the genes reported by imomlogit appear to be valid predictors of biological outcomes. the primary disadvantage of the imomlogit procedure is that it is computationally much more intensive than isis-scad and related penalized likelihood methods. we are currently investigating methods for reducing the computational burden of our algorithm by implementing various screening procedures that are similar to those used in isis-scad.  
