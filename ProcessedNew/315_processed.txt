data and text mining an efficient method to estimate the optimum regularization parameter in rlda motivation: the biomarker discovery process in high-throughput genomic profiles has presented the statistical learning community with a challenging problem, namely learning when the number of variables is comparable or exceeding the sample size. in these settings, many classical techniques including linear discriminant analysis (lda) falter. poor performance of lda is attributed to the ill-conditioned nature of sample covariance matrix when the dimension and sample size are comparable. to alleviate this problem, regularized lda (rlda) has been classically proposed in which the sample covariance matrix is replaced by its ridge estimate. however, the performance of rlda depends heavily on the regularization parameter used in the ridge estimate of sample covari-ance matrix. results: we propose a range-search technique for efficient estimation of the optimum regulariza-tion parameter. using an extensive set of simulations based on synthetic and gene expression microarray data, we demonstrate the robustness of the proposed technique to gaussianity, an assumption used in developing the core estimator. we compare the performance of the technique in terms of accuracy and efficiency with classical techniques for estimating the regularization parameter. in terms of accuracy, the results indicate that the proposed method vastly improves on similar techniques that use classical plug-in estimator. in that respect, it is better or comparable to cross-validation-based search strategies while, depending on the sample size and dimensionality, being tens to hundreds of times faster to compute. availability and implementation: the source code is available at https://github.com/ridge estimation is a type of shrinkage and traces back to the pioneering work of,b) on estimating regression parameters. they considered the standard linear model y xb e ;where y is the n-dimensional observation vector, x is a known n p matrix, b b 1 ; b 2 ;. .. ; b p t is a p-dimensional parameter vector to be estimated, and e is the n-dimensional error vector with mean 0 and covariance matrix r 2 i p. if we assume x is a full (column) rank matrix (p n), the ordinary least-square solution to this familiar linear model is given by b b x t x 1 x t y:however, when p n, the solution (2) does not exist because x t x becomes degenerate. even the solution obtained by generalized inverse form of matrix x t x is not working well.,b) then formulated a problem in which the residual sum of squares is replaced by its 2-penalized form given by l 2 bjjy xbjj 2 kjjbjj 2 ;where k 0 denotes a penalty factor controlling the length of b. minimizing l 2 b results in the so-called ridge regression given by b b x t x ki p 1 x t y:in this way, the inverse of possibly ill-conditioned x t x is stabilized by adding the scalar matrix ki p. this idea was then used by di pillo (1976) to replace the estimate of the sample covariance matrix used in linear discriminant analysis (lda) by its ridge estimate resulting in the so-called regularized lda (rlda). the goal is to improve the performance of lda in situations where dimensionality of observations, p, is larger or comparable to the number of measurements, n. di pillo (1979) attempts to determine the optimum value of the optimum regularization parameter in rlda. on this di pillos study, peck and ness (1982) comment that he found the analytical solution to this problem intractable, and so used a simulation study to choose an optimum value for k [the regularization parameter]. he concluded that if an algorithm can be found which leads to a value of k near the optimum value, then considerable improvement in the pcc [probability of correct classification] should occur.suggested the use of cross-validation in finding the optimum value of regularization parameter. in this procedure, cross-validation is used to estimate the true error of rlda for each value of the regularization parameter selected from a pre-specified set of size 2550. the estimate of the optimum regularization parameter is then the one that results in the minimum cross-validation estimate of true error. despite the computational complexity of cross-validation in such a search algorithm [e.g. see comments inand tasjudin and, this approach has remained the most popular method in estimating the optimum value of regularization parameter in rldafor instance, seeand ye and xiong (2006) to cite just a few articles. recently, we constructed a generalized consistent estimator of true error of rlda. in this regard, we proposed an estimator that converges to true error in a double asymptotic sense. in this setting, the estimator converges to the actual parameter in an asymptotic scenario in which dimension and sample size increase in a proportional manner (n ! 1; p ! 1 and p=n ! j 0) . in developing this estimator, we assumed that the true distributions governing the data follow multivariate gaussian model. however, the underlying mechanism to develop the estimator was based on double asymptotics and random matrix theory, both of which suggest applicability of the estimator in non-gaussian settings as well [see p. xii in, p. 335 in bai and silverstein (2010) and. in this work, we employ this estimator of true error in a one-dimensional search to estimate the optimum regularization parameter of rlda. as such, we employ data taken from seven gene expression microarray studies as well as synthetically generated gaussian and non-gaussian data. we compare the performance (in terms of accuracy and efficiency) of the search technique that uses this estimator with similar search schemes that use cross-validation or plug-in estimators. using an extensive set of simulations, we observe that the proposed technique is an efficient method that can outperform cross-validation and plugin estimate-based schemes in estimating the optimum regularization parameter of rlda. throughout this work, we use boldface lower case letters to denote a column vector. a boldface upper case letter denotes a matrix and tr: is the trace operator. the identity matrix of p dimension is denoted by i p .  
