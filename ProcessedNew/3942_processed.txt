gene expression reducing the algorithmic variability in transcriptome-based inference motivation: high-throughput measurements of mrna abundances from microarrays involve several stages of preprocessing. at each stage, a user has access to a large number of algorithms with no universally agreed guidance on which of these to use. we show that binary representations of gene expressions, retaining only information on whether a gene is expressed or not, reduces the variability in results caused by algorithmic choice, while also improving the quality of inference drawn from microarray studies. results: binary representation of transcriptome data has the desirable property of reducing the variability introduced at the preprocessing stages due to algorithmic choice. we compare the effect of the choice of algorithms on different problems and suggest that using binary representation of microarray data with tanimoto kernel for support vector machine reduces the effect of the choice of algorithm and simultaneously improves the performance of classification of phenotypes.a plethora of computational methods for the statistical analysis of high-throughput gene expression measurements is available to users interested in making inferences from transcriptomes. the preprocessing stages involved in the analysis include background correction, within and between-array normalizations, probe-specific correction and summarization. after the raw data is processed it is subject to sophisticated machine learning approaches such as classification, cluster analysis and the modelling of time-course data by means of dynamical systems. the preprocessing stages lead to quantifications of relative mrna abundances, taking scanned images as input. the choices made here have been shown to have an important effect on the results of statistical inference approaches . we review the effect of the choice of algorithms on different problems and suggest that using binary representation of microarray data with tanimoto kernel for support vector machine (svm) reduces the variability due to the to whom correspondence should be addressed. choice of algorithm and simultaneously improves the performance of classification on transcriptome data. the most extensive study so far that shows significant algorithmic variability is due to p.c. boutros 1 who analysed an impressive 19 446 different combinations of preprocessing algorithms, and quantified the sensitivity and stability of expression levels. while this and similar studies seek the best combination of algorithms on a small number of datasets, they do not offer a generic solution for practitioners to select a combination that leads to reliable results in downstream inference. our approach, starting from the premise of seeking sensible numerical precisions to represent microarray data, is similar to the bar code method advanced by zilliox and irizarry (2007). in our previous work , we established that gene expressions quantized to binary precision lead to minimal average loss in the quality of inference drawn from them, in a range of applications such as classification, clustering and the analysis of time-course data. any loss of information due to binarization was shown to be easily recovered using metrics of similarity between gene expression profiles that are best suited for high-dimensional binary spaces . our results showed that the tanimoto metric, successfully used in matching chemical fingerprints in the chemoinformatics literature , when cast in kernel svm and spectral clustering frameworks, is able to achieve performances often better than, and never worse than, using data to the high numerical precision with which it is often reported and archived.as reviewed above, gene expression values obtained by different preprocessing algorithms yield different results. we use a gene selection problem to illustrate the impact of this variation. we took the dataset of a breast cancer study and computed the 50 most discriminant genes that could be used as hypothetical markers for discrimination. these genes were selected using the fishers ratio as criterion, as is commonly done . we notice substantial differences in the genes that were identified as carrying the most discriminant information in three different algorithmic combinations, as shown in the confusion matrix of. the three algorithmic combinations chosen here correspond to those achieving the minimum, maximum and mean class prediction performance of an svm classifier, measured in terms of the area under the receiver operating characteristics curve (auroc). the names of the genes selected are given in supplementary table s4. the selected genes between the best and the worst performing classifiers overlap by only about 50. this observation motivates a systematic study to explore the variability in inference quality caused by the choice of algorithms in the preprocessing stages, and possible approaches to reducing this variability. as we report in the remainder of this article, a binary representation with kernel classification in high-dimensional spacesis able to achieve a drastic reduction in the variability caused by algorithmic choice.in this article, we address the variability in the results of inferences from transcriptome studies that arises from the choice of preprocessing algorithms. this variability has previously been shown to have a significant effect on the gene expressions measured by microarrays. our work reported here shows that the binary representation helps significantly in reducing this algorithmic choice induced variability in classification problems, when used in combination with a high-dimensional kernel method. while previous studies have largely focused on pointing out that such a variability exists by reference to how well-measured expressions of a gene correlates with spiked-in concentration or with page: 1190 11851191sds of the aurocs, shown as box plots inare shown. statistical significances using f-test show levels of confidence at which our proposed method of binary tanimoto differs from the alternate approach.an alternate measurement such as qpcr (quantitative polymerase chain reaction), we have focused on the quality of inference drawn, in the context of classification problems. to the best of our knowledge, comparisons of inference algorithms on microarray data (e.g. support vector machines versus nearest neighbour as predictors of phenotype), of which there is a very large body of literature in the statistical and machine learning communities, are based on the application of one set of preprocessing algorithms, often published by the original authors of a particular study. given the variability we observe, we believe there is room for scepticism of conclusions drawn from such studies.  
