genome analysis assembling the 20 gb white spruce (picea glauca) genome from whole-genome shotgun sequencing data white spruce (picea glauca) is a dominant conifer of the boreal forests of north america, and providing genomics resources for this commercially valuable tree will help improve forest management and conservation efforts. sequencing and assembling the large and highly repetitive spruce genome though pushes the boundaries of the current technology. here, we describe a whole-genome shotgun sequencing strategy using two illumina sequencing platforms and an assembly approach using the abyss software. we report a 20.8 giga base pairs draft genome in 4.9 million scaffolds, with a scaffold n50 of 20 356 bp. we demonstrate how recent improvements in the sequen-cing technology, especially increasing read lengths and paired end reads from longer fragments have a major impact on the assembly contiguity. we also note that scalable bioinformatics tools are instrumental in providing rapid draft assemblies. availability: the picea glauca genome sequencing and assembly data are available through ncbi (accession: alwz0100000000 pid: prjna83435). http://www.ncbi.nlm.nih.gov/bioproject/83435.the assembly of short reads to develop genomic resources for non-model species remains an active area of development . the feasibility of the approach and its scalability to large genomes was demonstrated by the abyss publication using human genome sequencing data and was later used to assemble the panda genome with the soapdenovo tool . the technology provides high quality results, as demonstrated for bacteria , and has been successfully applied numerous times on more complex genomes . estimated at 20 giga base pairs (gb) , sequencing and assembly of the genome of this gymnosperm species of the pine (pinaceae) family present unique challenges. on the data generation end, those challenges include representation biases in whole-genome shotgun sequencing data, and difficulties in building reduced representation resources to scale down the magnitude of the problem. on the bioinformatics end, assembling massive sequencing datasets is extremely demanding on computing cycles, memory usage, storage requirements, and for parallel programming implementations on communication traffic. we addressed the data representation challenges by preparing and sequencing multiple whole-genome shotgun libraries on the hiseq 2000 and miseq sequencers from illumina (san diego, ca, usa). compared with localized sequencing protocols, such as building and sequencing fosmid libraries, or the recent approach of isolating $10 kb dna strands to generate indexed sequencing fragments in high throughput (moleculo, san diego, ca, usa), a shotgun only sequencing approach rapidly provides sequence data effectively covering the target genome at a cost that can be an order of magnitude less. the difference in cost is especially substantial when sequencing a large genome. in this work, we demonstrate that shotgun sequence assembly at this scale remains viable and produces valuable results. to to whom correspondence should be addressed. the author 2013. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. for commercial re-use, please contact journals.permissions@oup.com assemble the spruce genome, we used the abyss algorithm , which captures a representation of read-to-read overlaps by a distributed de bruijn graph and uses parallel computations to build the target genome. the modular nature of the tool allowed us to execute a large number of tests to tune the message passing interface for a successful execution, train the assembly parameters for an optimal assembly and quantify the utility of long reads for large genome assemblies. to the best of our knowledge, the abyss algorithm is unique in its ability to enable genome assemblies of this scale using whole-genome shotgun sequencing data.prior experience indicates that sampling a large genome with multiple libraries and fragment lengths can mitigate potential sampling biases and capture a more even representation of the underlying genome . one novel feature in our sequencing approach was to complement the high coverage data from the hiseq 2000 sequencers with longer reads from the miseq, at low coverage, to support the assembly process. using an early access 2 150 bp kit on the hiseq 2000, we generated pet reads from two libraries with 250 bp nominal fragment lengths and 18 libraries with 500 bp nominal fragment lengths to a total of 64-fold raw coverage. using the miseq, we generated 2 300 bp pet reads from four libraries with 500 bp nominal fragment lengths and 2 500 bp pet reads from one library with 500 bp nominal fragment length, contributing a further 4-fold raw coverage . we also generated large fragment libraries of 6, 8 and 12 kb nominal fragment lengths, to provide linkage information across repeat structures. the first two of these libraries were prepared using the mpet protocol of illumina, and the third was a pool of seven libraries prepared using a modified 454 paired end sequencing protocol (454 life sciences/roche, branford, ct). all long fragment libraries were sequenced at 2 100 bp, and the resulting sequences were used only for their linkage information during the scaffold stage of the assembly. the first release of the sequencing chemistry on the miseq allowed for 150 read cycles, which increased to 250 cycles in a subsequent release. to obtain longer read lengths, we modified the sequencing instrument as detailed in the supplementary material. longer reads obtained from this platform were instrumental in improving the contiguity of the genome assembly as described later in the text. to build the white spruce genome, we used the abyss de novo assembly tool , which has been rigorously evaluated . the process broadly has three stages: unitig, contig and scaffold building. performed on a computer cluster of dual intel xeon 6-core processors, each addressing 48 gb of memory, the three stages of the spruce genome assembly took approximately two, four and four days using 1560, 288 and 36 cpu cores, respectively. the first assembly stage constructs a distributed and scalable de bruijn graph to represent read-to-read overlaps for unitig assembly. the second stage involves read-to-unitig alignments and path traversals on the unitig adjacency graph from the previous stage for contig construction, resolving short repeats along those paths, when possible. similarly, the third stage involves read-to-contig alignments and path traversal on the contig-adjacency graph for scaffold construction, denoting unresolved sequence content with ns. the first assembly stage can further be conceptualized in two parts, with roughly equal run times. the first part provides preliminary unitigs and a graph representing their adjacency, which is defined as (k-1) bp overlaps, where k is the primary assembly parameter denoting the minimum read-to-read overlap length that is considered specific enough for assembly. the second part eliminates short redundant unitigs, when their neighbors on both sides share more than a certain length of sequence (default: 10 bp), and enables further graph simplification. we based the preliminary assessment of assembly quality on the first part of the first stage, and optimized our assembly parameters, using the pre-unitig contiguity statistics . the relatively short run time of this part of the assembly process allowed us to execute a large number of tests. here, we also take this opportunity to demonstrate the utility of longer reads, as they pertain to the optimal k-mer length. as a larger k would resolve longer sequence ambiguities, it is desirable to choose it as 1494 large as possible, yet not too large; otherwise, we lose the sensitivity to detect valid read-to-read overlaps. the choice of this parameter is determined by several factors, including read lengths; fold coverage; genome complexity; genome size; and experimental noise. among these, genome complexity and size are determined by the choice of the species to study, and the experimental noise is determined by the sample collection methods and the choice of the sequencing platform. longer reads and higher fold coverage would enable one to use a longer k-mer length.shows results of our parameter search for an optimum k-mer length using the short hiseq 2000 reads only and combined short hiseq 2000 and long miseq reads. the contiguity statistics nx (describing x reconstruction in assembled sequence lengths nx or longer) are typically locally concavedown functions of the k-mer length in a neighborhood where the total sequence reconstruction is close to the genome length. controlled for the misassemblies, contig or scaffold n50 lengths are widely used quality metrics for genome assemblies. we demonstrate that the optimum pre-unitig n50 occurred at k 109 bp when using both short and long reads, and at k 101 bp when using just the short reads. thus, we observe that incorporating the modest low coverage long read data from the miseq allowed us to use a more stringent overlap parameter for the assembly process, and resulted in improved assembly statistics. optimum k using short and long read data yielded n50 1335 bp, whereas the optimum k using short read data only yielded n50 1236 bp. the difference between assembly contiguity numbers (7.4) became more pronounced when the assembly process proceeded to use 158 million and 186 million pre-unitigs in these two cases, respectively, to construct unitigs (9.7), contigs (9.8) and scaffolds (17.8). we also note that the optimum k values in both datasets were longer than the sequencing length of the mpet libraries. we propagated the optimum k 109 bp assembly with short and long reads through the assembly pipeline. the full assembly of our data yielded 191 347 (171 971) scaffolds over 20 356 bp (22 967 bp) in length, representing 450 of the 20.8 gb reconstructed (20 gb estimated) spruce genome. the assembly statistics of the white spruce genome are presented inin comparison with the whole-genome shotgun sequence assemblies of three barley cultivars . the recent barley genome assemblies represent results from a whole-genome shotgun sequencing and assembly project using similar data and offer a context for plant genomics. as a means to assess the quality of the assembled spruce genome, the sequences of several bacs from the same genotype were aligned against the assembly using blast . six previously sequenced targeted bacs containing known terpene synthase and cytochrome p450 genes (see supplementary material) were then compared with mummer dot plots . twenty-six scaffolds longer than 1000 bp aligned with 495 similarity to reconstruct 62 of the sequence of these six bacs.note: short fragment libraries prepared by the illumina pet protocol were used for their sequence content. long fragment libraries prepared by the illumina mpet, and a modified 454 protocols were used for linkage information during scaffolding. as long fragment libraries do not contribute to the sequence content of the assembly, their contribution to genome coverage is marked as n/a (not applicable).. assembly optimization. the de bruijn graph stage (pre-unitig) of the assembly was used to optimize the overlap parameter, k-mer length, and the effect of inclusion of longer reads was assessed. the contiguity metrics n50 (solid curves, left y-axis) and n20 (dotted curves, right y-axis) are shown for assemblies that use the short reads only (blue) and short and long reads (black). the contiguity of the two datasets peaked for different k-mer lengths, with dataset of short and long reads having a maximum n50 and a maximum n20 for the same k 109 bp. for short reads only, optimization with respect to n20 resulted in a slightly lower k-mer length (98 bp) compared with optimization with respect to n50 (101 bp), both of which are lower than the optimum k-mer length for the full dataset. longer k-mers were desirable, as they help disambiguate longer repeat motifs 1495the choice between a whole-genome shotgun sequencing approach and sequencing reduced representation libraries was extensively discussed during the human genome project , and the former became the dominant technology as the sequencing throughput rapidly increased, rendering library techniques to prepare data for the latter approach relatively expensive. a decade later, researchers studying conifer genomes are trying to answer the same question. in our study, we demonstrate that modern whole-genome shotgun sequencing and assembly methods can provide competitive draft genome assemblies at the multi-gb scale for downstream biological studies in a cost-effective way, even if it is far from producing chromosome level contiguous sequence. we note that a rigorous assessment of the reported assembly is not a trivial undertaking and will need to be performed, as the assembly evolves from its draft stage toward a more established reference. for example, de novo assembly evaluation tools such as cgal , frcbam and ale would either not scale to the size of the problem or require substantial time and computational resources. still, compared with previous targeted gene and genome subsampling studies, the assembly introduced in this article already gives the community considerably greater power to identify and study gymnosperm genes, to assist forest management strategies and to understand the environmental biological interactions that involve spruce trees at a basic level.  
