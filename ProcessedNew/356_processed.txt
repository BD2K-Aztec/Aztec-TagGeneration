systems biology an integrated strategy for prediction uncertainty analysis motivation: to further our understanding of the mechanisms underlying biochemical pathways mathematical modelling is used. since many parameter values are unknown they need to be estimated using experimental observations. the complexity of models necessary to describe biological pathways in combination with the limited amount of quantitative data results in large parameter uncertainty which propagates into model predictions. therefore prediction uncertainty analysis is an important topic that needs to be addressed in systems biology modelling. results: we propose a strategy for model prediction uncertainty analysis by integrating profile likelihood analysis with bayesian estimation. our method is illustrated with an application to a model of the jak-stat signalling pathway. the analysis identified predictions on unobserved variables that could be made with a high level of confidence, despite that some parameters were non-identifiable. availability and implementation: source code is available at:mathematical modelling is used to integrate hypotheses about a biochemical network in such a manner that such networks can be simulated. in addition to the formulation and testing of biochemical properties, computational models are used to predict unmeasured behaviour. despite great advances in measurement techniques, the amount of data is still relatively scarce and therefore parameter uncertainty is an important research topic. we focus on biochemical networks modelled using ordinary differential equations (odes). such models consist of equations which contain parameters p, inputs u(t) and state variables x(t). in many cases, these systems are only partially observed, which means that measurements y(t) are performed on a subset or a combination of the total number of states n in the model. this results in a mapping from an internal state to an output. additionally, these measurements are hampered by noise . moreover, many techniques used in biology (e.g. western blotting) necessitate the use of scaling and offset parameters q . for ease of notation, we define , which lists all the parameters that should be defined in order to simulate the model. to whom correspondence should be addressed.considering m time series of length n i with additive independent gaussian noise, we can obtain (2) for the probability density function of the output data.in this equation y t represents the true system with true parameters t , whereas i,j indicates the sd of a specific datapoint and k serves as a normalization constant. in maximum likelihood estimation (mle), the goal is to find model parameters for which the probability density function most likely produced the data. in mle one attempts to maximize the likelihood function l(y d ) whose formula is identical to (2). a second formalism commonly applied to inferential problems is known as bayesian inference. in contrast to mle, bayesian inference does attach a notion of probability to the parameter values. applying bayes theorem to the parameter estimation problem, we obtain (4). since the probability of the data does not depend on the parameters, it merely acts as a normalizing constant. the posterior probability distribution is given by normalizing the likelihood multiplied with the prior to a unit area.whereas mle tends to focus on estimating best fit parameters, the bayesian methodology attempts to elucidate posterior parameter probability distributions. in this article, we provide a strategy for uncertainty analysis consisting of multiple steps. by performing these steps sequentially we show how to avoid problems associated with the different techniques.we illustrate our approach using a model of the jak-stat signaling pathway . the model is based on a number of hypothesized steps (see the supplementary material for model equations). first erythropoietin (epor) activates the epor receptor which phosphorylates cytoplasmic stat (x 1 ). this phosphorylated stat (x 2 ) dimerizes (x 3 ) and is subsequently imported into the nucleus (x 4 ). here dissociation and dephosphorylation occurs, which is associated with a delay. similar to the implementation given in the original article, the driving input function was approximated by a spline interpolant, while the delay was approximated using a linear chain approximation. we used data from the article by) for parameterization and inference . observables were the total concentration of stat and the total concentration of phosphorylated stat in the cytoplasm, both reported in arbitrary units thereby requiring two additional scaling parameters s 1 and s 2. the initial cytoplasmic concentration of stat is unknown while all other forms of stat are assumed zero at the start of the simulation. the vector of unknown parameter values consists of the elementsin order to investigate the existence of multiple modes, we first performed a large scale search using mcmm with initial parameters, and their associated wrsss (dots). note that all of the optimized parameter sets shown are acceptable with respect to the lr ratio. right: model predictions from parameter sets taken from location a, b and c for two measured outputs as well as one unmeasured internal state.based on a log uniform random sampling between the ranges 10 3 and 10 2 (n=10 000). after optimization, samples are either accepted or rejected based on the lr bound based on the best fit value. the resulting distribution and associated wrss are shown in. it is clear that there are at least three local minima in the likelihood. although all three modes describe the data adequately, they show different prediction results for the unobserved internal states of the model. subsequently, a pl analysis was performed. in order to increase confidence that no acceptable regions of parameter space were missed, we started pls from each mode detected using the mcmm method (step 1). subsequently we merged these profiles and verified whether they covered the full span of acceptable parameter sets obtained in step 1. based on the pl, shown in the top panel of, it can be concluded that the model based on first principles is structurally non-identifiable . from scatter plots of the pls (shown in the supplement), it was determined that the parameters x 0 1 , s 1 and s 2 were structurally related and therefore unidentifiable. analogously to we specify a gaussian prior ( = 200nm, = 20nm) for the initial condition (which is comparable to assuming that the initial concentration was measured with this accuracy). in order to check whether the prior affects the profiles in the desired manner one can compute new profiles using map estimation (by incorporating the prior in the procedure). in our case, the gaussian prior constrains both the initial condition as well as both scaling factors (see). we can also observe that parameter p 2 is practically non-identifiable at = 0.05. in the case of jak-stat, at least three priors are required to render the model identifiable for all levels of significance. as the name suggests, priors based on prior belief are preferred. however, in many cases, little is known beforehand regarding the parameters of a system. for the initial condition we specify a gaussian priorin this article, we proposed a new strategy for prediction uncertainty analysis. by performing pl analysis, we were able to specify sufficient priors to ensure that the posterior distribution was proper and could be sampled from. using mcmc a sample of parameter sets proportional to the probability density of that parameter set was obtained. the strategy enables a comprehensive analysis on the effect of parameter uncertainty on model predictions and enables the modeller to relate these effects to the model parameters. given a sufficient amount of data, such an analysis should be relatively insensitive to the assumed priors. as observed in the case of jak-stat, however, it can be seen that even for a small model, identifiability can be problematic. it is important to realize that in such cases the choice of priors will affect the outcome of the analysis. furthermore, most priors are not re-parameterization invariant and therefore uniform priors do not reflect complete ignorance. although seemingly uninformative, a uniform prior in untransformed parameter space implies that extremely large rates have an equal a-priori probability of occurring than slow rates. in our case, for the completely unknown kinetic parameters we assumed a uniform prior in logarithmic space. for positively defined parameters, a uniform distribution in logarithmic space corresponds to an uninformative prior . such a prior gives equal probability to different orders of magnitude (scales). an approximate scale invariance of kinetic parameters has indeed been observed in biological models . note that in a bayesian analysis there is no such thing as not specifying a prior.our strategy can be used to gain insight into prediction uncertainty. note, however, that aside from the computational model and the prior distributions, the noise model also affects the resulting posterior distribution. it should be stressed that investigating what kind of noise model to use when (and subsequently determining the appropriate likelihood function for this noise model) is important. practical solutions to non-additive noise can usually be found. one example would be a multiplicative noise model (which is often associated with non-negative data), where data preprocessing such as taking the logarithm of both the model and data can help alleviate problems . if the likelihood function truly becomes intractable, then one can resort to approximate bayesian methods, where rather than computing the likelihood function, one computes a distance metric between simulations (with simulated noise) and data . when the goal of prediction uncertainty analysis is model falsification then one could opt for an approach based on interval analysis (, b). in these works, uncertainty analysis is reformulated into a feasibility problem. using this approach, regions of parameter space that cannot describe the data can systematically be determined. an attractive aspect of these methods is that these methods provide guarantees on finite parameter searches, but have up to this point only been performed on small scale models. different approaches for prediction uncertainty analysis based on optimization are proposed in . such methods are useful for probing consistent behaviour (termed core predictions) among multiple parameter sets, even in the non-identifiable case. however, they do not result in a probabilistic assessment of the prediction uncertainty. probing consistent behaviour is also the main focus of a workflow proposed by for classifying consistent model behaviours and hypotheses. several steps in the proposed approach are computationally challenging and require many model evaluations. because of this, model simulation time is a primary concern. many packages (including ours) have been able to attain significant simulation speed-ups by compiling simulation code, reducing model evaluation time by up to two orders of magnitude [potters wheel ; copasi ; sloppy cell (. additionally, new computational platforms such as general purpose programming on the graphical processing unit are being explored . in conclusion, our strategy enables the modeller to account for parameter uncertainty when making model predictions. in the case of a fully identifiable model, we can work with uninformative priors and overconfident conclusions that could result from a model described by a single parameter set can be avoided. regarding nonidentifiable models, a practical approach can be adopted where the dependence with respect to the assumed prior distributions can be determined a posteriori. note that though this makes computing the posterior distribution feasible, such an approach underestimates the parameter uncertainty. performing the analysis and obtaining a sample from the posterior takes considerably more computational effort than determining a single parameter set. however, once such a sample is obtained, the results can be used for a wide array of model analysis techniques which more than warrants the additional computational time invested. relations within this posterior distribution and also its relation to the posterior predictive  
