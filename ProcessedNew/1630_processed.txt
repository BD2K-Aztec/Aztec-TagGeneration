estimating optimal window size for analysis of low-coverage next-generation sequence data motivation: current high-throughput sequencing has greatly transformed genome sequence analysis. in the context of very low-coverage sequencing (50.1), performing binning or windowing on mapped short sequences (reads) is critical to extract genomic information of interest for further evaluation, such as copy-number alteration analysis. if the window size is too small, many windows will exhibit zero counts and almost no pattern can be observed. in contrast , if the window size is too wide, the patterns or genomic features will be smoothed out. our objective is to identify an optimal window size in between the two extremes. results: we assume the reads density to be a step function. given this model, we propose a data-based estimation of optimal window size based on akaikes information criterion (aic) and cross-validation (cv) log-likelihood. by plotting the aic and cv log-likelihood curve as a function of window size, we are able to estimate the optimal window size that minimizes aic or maximizes cv log-likelihood. the proposed methods are of general purpose and we illustrate their application using low-coverage next-generation sequence datasets from real tumour samples and simulated datasets. availability and implementation: an r package to estimate optimal window size is available at http://www1.maths.leeds.ac.uk/arief/r/ win/.the recent advent of high-throughput sequencing (or ngs, next-generation sequencing) has revolutionized the quantity and quality of data produced. the ability to sequence a large number of dna or cdna fragments at reasonable cost is proving to be flexible and powerful. one of the applications is to use ngs to assess copy-number alterations (cna) in tumour cells. although information about copy number is often obtained by analysing high-coverage data (420) , we have previously shown that it can also be reliably obtained by more affordable lowcoverage data (50:05) from small amounts of fragmented dna obtained from formalin-fixed paraffin-embedded samples. we expect low-coverage data will still be a valuable choice because, regardless of falling sequencing costs, data storage, analysis time and infrastructure costs associated with large datasets will not decrease as quickly. wider use of ngs means it will be used more and more in diagnostic settings, where low costs and rapid analysis time are critical. finally, the recently launched sequencing machines (illumina miseq, life technologies pgm) have allowed costs to be within reach of individual laboratory budgets, although this means that they produce fewer reads. for these reasons, ngs low-coverage data could become common and partially replace hybridization-based technologies such as acgh and snp arrays. owing to the sparse nature of the data, however, it is important to extract the maximum information. in particular, the size of the genomic window used for binning reads is a critical tuning parameter: if it is too wide, the analysis will miss some genomic regions that exhibit important features, and if it is too narrow, the noise level will be dominant and many windows will contain zero reads. we expect that in high-coverage cases (420), the choice of window size is less crucial because by the time reads windows are so small, they are of the same order of magnitude as the reads themselves, and information about chromosomal junctions can be precisely obtained by reads spanning across chromosomal regions that are disjointed in the reference genome. the number of reads per window, should, in theory, follow a poisson distribution. it was, however, evident from the early experiments that there is overdispersion . this is due to a number of reasons including gc content bias , mappability , underlying changes in copy number (both somatic and germ line) and possibly other biological and technical factors still to be described. in this study, our focus is on estimating an optimal window size for analysis of ngs data to best track the depth of coverage signal in a very low-coverage setting, motivated by our study of cna. the principle that we use to answer this problem is by considering that the binning or windowing is basically the to whom correspondence should be addressed. same process as a histogram construction. in the construction, we calculate either the count of the data falling in each window or, equivalently, the density in each window. by assuming a statistical model for the underlying density in each window, we are able to quantify the statistical distance of the model to the truth across different window sizes. from here, we can identify the optimal window size as the one that minimizes the distance. once the optimal window size is estimated, further analysis, e.g. gc correction, comparison with matched normal, cna analysis, can be performed based on the optimal window size. before we propose our method, we consider in the next section some studies that have discussed the notion of an optimal window size in the context of next-generation sequence data.for the ls041 data, the results on estimation of optimal window size are presented in, based on the aic and cv loglikelihood. the minimum aic is achieved at the window size 100 kb, equivalent to an average of 60 reads per window in both tumour and normal samples. similarly, the maximum cv log-likelihood is located at the window size 100 kb in both tumour and normal samples. because the horizontal axis of the figures is in a log scale, the range of window sizes around the optimal one that can be considered near-optimal is actually wider than it seems to be. for example, although the window size 100 kb is optimal, window sizes between 50 and 250 kb can be considered near-optimal in(supplementarywhere the horizontal axis is in a linear scale). this can be regarded as an advantage as we will revisit in the discussion section.the estimation of optimal window size for analysis of low-coverage next-generation sequence data can be considered to be a histogram construction where the breaks for histogram are the window limits. as such, each experiment will result in one particular optimal window size, depending on the underlying features and biases, both technical and biological. previous approaches to this problem are problematic. some methods tend to underestimate the optimal window size in ngs data because of a poor approximation to the underlying reads density in the genome. for example, the method proposed byresulted in an estimated optimal window size of 40 kb (24 reads per window) to 10 kb (6 reads per window) in our datasets, for detecting a copy-number ratio of 1.2 to 1.4, respectively, with p-value of 0.0001 (supplementary material), although cnv-seq is only meaningful for window size410 reads per window. the latter requirement is to prevent the random variability to dominate the analysis. some other approaches, which mainly rely on a subjective consideration, also tend to overestimate the optimal window size. this is because having a wide window size gives a smooth pattern of genomic features that can easily be interpreted. however, this mainly subjective approach ignores the potential discoveries of genomic features that are present in short regions, as we discussed previously. in this study, we proposed to use aic and cv log-likelihood, given in the model in section 2.3, because the methods do not depend on an approximation of the reads density, have a simple interpretation and are relatively easy to implement. results from our analysis with the ls041 and ls010 data suggest that some window sizes around the optimal one can be considered as near-optimal (and 2 with a linear-scale horizontal-axis in the supplementary material). this is an advantage when we want to analyse ngs data across different samples or patients. as each sample may exhibit a different optimal window size, there may be a window size that is in the overlapping regions of near-optimal window sizes across the different samples. our tool will help the experimenter to make an informed decision when estimating an optimal window size to analyse the data. our previous experience suggested that we lose little information when we use slightly suboptimal window size compared with the optimal one in the analysis across different samples. one important message that we can take here is that the calculation of aic and cv log-likelihood usually fails or becomes uninterpretable when the window size we evaluate has 55 reads per window on average. although the optimal window size differs from one dataset to the next or from one experiment to the next, we found in our ls041 and ls010 datasets that the optimal window size is $60 reads per window. we also found in our study that window size of 30180 reads per window can be considered near-optimal. in the ls041 data, this corresponds to a. the aic (left) and cv log-likelihood (right) as a function of window size or number of reads per window, from the low-coverage samples. the horizontal axis is in log scale. sensitivity analysis of the simulated data at different window sizes in detecting gains or losses, when the cna are segmented using cbs and smooth segmentation. the solid grey diagonal line is the identity line range of 50300 kb window size, and in the ls010 data, 40 250 kb window size. our proposed methods assume a simple model for the underlying density of reads. therefore, our estimation of the optimal window size depends on all technical and biological factors that contribute to the observed signal. the window size needs to be optimized so that it is capable of tracking all the concurring factors that contribute to the final signal. as an example, to be able to correct for gc content, the signal needs to follow the gc content bias, and a window size unnecessarily too large or too small might prevent the best normalization. however, we have observed that reads mapped with low confidence sometimes cluster in particular regions of the genome and introduce extreme peaks in the signal that might negatively affect the optimal size of the window, as they dominate the calculation of aic and cv log-likelihood. for this reason, we suggest that an estimation of optimal window size is made on the data with good mapping quality. our simulation study (supplementary material) indicates that the proposed methods are able to estimate an optimal window size that minimizes the distance between the observed reads density in the low-coverage data and the true underlying density. this holds in many typical cases where, for example, structural rearrangements are reasonable. in a rare case where we have extreme structural chromosome rearrangements, our proposed methods can still estimate a near-optimal window size, which is close to the optimal one. in this regard, the proposed methods can still be useful to identify a good estimate of window size that can be used in the analysis. last but not the least, our proposed methods can also be used in a higher coverage context as well, or, possibly, in transcriptome or chip-seq experiments or whenever data need to be binned in windows of predefined size.in the context of the analysis of very low-coverage next-generation sequence data, the estimation of optimal window size is critical. if the window is too narrow or too wide, we will potentially miss genomic features of interest. to estimate the optimal window size, we first assume the reads density to be a step function. given this, the optimal window size is estimated as the one that minimizes aic or maximizes cv log-likelihood across different window sizes that we evaluate. our analysis on ls041 and ls010 data indicates that the optimal window size is approximately equivalent to 60 reads per window. our simulation study confirms that the optimal window size we estimated produces the closest distance between the reads density in the low-coverage data and the true underlying density.  
