data and text mining cross-validation under separate sampling: strong bias and how to correct it motivation: it is commonly assumed in pattern recognition that cross-validation error estimation is almost unbiased as long as the number of folds is not too small. while this is true for random sampling, it is not true with separate sampling, where the populations are independently sampled, which is a common situation in bioinformatics. results: we demonstrate, via analytical and numerical methods, that classical cross-validation can have strong bias under separate sampling , depending on the difference between the sampling ratios and the true population probabilities. we propose a new separate-sampling cross-validation error estimator, and prove that it satisfies an almost unbiased theorem similar to that of random-sampling cross-validation. we present two case studies with previously published data, which show that the results can change drastically if the correct form of cross-validation is used. availability and implementation: the source code in c+ +, alongthe most important property of a classifier is its error rate (probability of misclassification) because the error rate quantifies the predictive capacity of the classifier. if the feature-label distribution is known, then the true error can be found exactly; however, in practice, the feature-label distribution is unknown and the error must be estimated. if the sample is small, then the estimation must be computed using the same data as that used for training the classifier. perhaps the most commonly used training data-based classification error estimator is cross-validation. it has a long history going back to 1968 . in its most basic form, the k-fold cross-validation error estimate, cvk n , for a sample of size n (it is assumed that k divides n) is computed by selecting randomly a partition of the sample into k data folds (subsets), for each fold applying the classification rule on the data not in the fold, computing the error rate of the designed classifier on the left-out fold, and then averaging the resulting k error rates. when k = n, one gets the leave-one-out estimator, l n. cross-validations salient good property is that, under random sampling, it can be proved (see) that it is almost unbiased, in the sense thatwhere n is the true error (probability of misclassification) of a classifier designed on a sample of size n. hence, the bias is not too great as long as n/k is small. for leave-one-out, e l n =e n1 , and the estimator is essentially unbiased. the salient point motivating the present article is that (1) depends on the sampling being random, and that when sampling is not random, there can be severe bias. the importance of bias for an arbitrary error estimator n can also be gleaned from its role in the estimator root-mean-square error:where bias n =e n n and var dev n =var n n (braga). as mentioned previously, for classical cross-validation under random sampling, it follows from(1) that, if n/k is small, then bias n 0, in which case rms n var 1=2 dev n . while the variance of cv is known to be large in small-sample cases (braga), it will typically reduce to zero as n ! 1 . however, the bias introduced by application of the classical cv estimator under nonrandom sampling will generally not approach zero as n ! 1. the result is an inconsistent estimator, which is imprecise under arbitrarily large sample sizes. under random sampling, an independent and identically distributed (i.i.d.) sample s is drawn from the mixture of the populations 0 and 1. this means that if a sample of size n is drawn for binary classification, then the numbers of sample points n 0 and n 1 drawn from the populations 0 and 1 , respectively, are random variables n 0 $binomialn; c and n 1 $ binomialn; 1 c, where c=py=0 is the a priori probability that the label y is zero, i.e. the sample point comes from population 0. this random-sampling assumption is so pervasive that it is usually assumed without mention and in books is often stated at the outset and then forgotten. for instance,state, in typical supervised pattern classification problems, the estimation of the prior probabilities presents no serious difficulties. they are referring to the fact that the prior probability c=pr y=0 can be consistently estimated by the sampling ratio, numbers: n0 n ! c in probability. however, suppose the sampling is not random, in the sense that the ratios r= n0 n and 1 r= n1 n are chosen before the sampling procedure. in this separate-sampling case, s=s 0 [ s 1 , where the sample points in s 0 and s 1 are selected randomly from 0 and 1 , but given n, the individual class counts n 0 and n 1 are not determined by the sampling procedure. with separate sampling, we have no sensible estimate of c. recognition of this particular problem of estimating the prior probability when sampling is separate and its effect on linear discriminant analysis (lda) goes back to 1951 . often, one says that for separate sampling the ratios r= n0 n and 1 r= n1 n are chosen prior to the sampling procedure. but there is in fact no temporal meaning to this. for instance, one could simply separately randomly sample 0 and 1 with n 0 and n 1 being randomly selected by a process independent of the sampling procedure, and the sampling would still be separate. the point is that r cannot be reasonably used as an estimate of c.(taken from esfahani and dougherty, 2014) illustrates the effects of separate sampling on the expected true classifier error for two classification rules and multivariate gaussian distributions of equal and unequal covariance structures and dimensionality d = 3. for a given sample size n, sampling ratio r, and classification rule, the expected true error rate e n jr is plotted for different class prior probabilities c, for lda and a non-linear radial basis function support vector machine (rbf-svm). for each r and n, n 0 is determined as n 0 =dnre. we observe that the expected error is close to minimal when r = c and that it can greatly increase when r 6 c. this kind of poor performance for separate sampling ratios not close to c is commonplace . in this article, we investigate the effect of separate sampling on cross-validation error estimation. we will see that for a separatesampling ratio r not close to c there can be large bias, optimistic or pessimistic. a serious consequence of this behavior can be ascertained by looking at. whereas the expected true error of the designed classifier grows large when r greatly deviates from c, a large optimistic cross-validation bias when r is far from c can obscure the large error and leave one with the illusion of good performanceand this illusion is not mitigated by large samples! to overcome the bias problem for classical crossvalidation with separate sampling, we introduce a new crossvalidation estimator designed for separate sampling and prove that it satisfies a bias property analogous to (1).  
