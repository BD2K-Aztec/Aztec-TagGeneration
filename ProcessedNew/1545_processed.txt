genetics and population analysis efficient haplotype matching and storage using the positional burrowsâ€“wheeler transform (pbwt) motivation: over the last few years, methods based on suffix arrays using the burrowswheeler transform have been widely used for dna sequence read matching and assembly. these provide very fast search algorithms, linear in the search pattern size, on a highly com-pressible representation of the dataset being searched. meanwhile, algorithmic development for genotype data has concentrated on statistical methods for phasing and imputation, based on probabilistic matching to hidden markov model representations of the reference data, which while powerful are much less computationally efficient. here a theory of haplotype matching using suffix array ideas is developed, which should scale too much larger datasets than those currently handled by genotype algorithms. results: given m sequences with n bi-allelic variable sites, an o(nm) algorithm to derive a representation of the data based on positional prefix arrays is given, which is termed the positional burrowswheeler transform (pbwt). on large datasets this compresses with run-length encoding by more than a factor of a hundred smaller than using gzip on the raw data. using this representation a method is given to find all maximal haplotype matches within the set in o(nm) time rather than o(nm 2) as expected from naive pairwise comparison, and also a fast algorithm, empirically independent of m given sufficient memory for indexes, to find maximal matches between a new sequence and the set. the discussion includes some proposals about how these approaches could be used for imputation and phasing.given a large collection of aligned genetic sequences, or haplotypes, it is often of interest to find long matches between sequences within the collection, or between a new test sequence and sequences from the collection. for example, sufficiently long identical substrings are candidates to be regions that are identical by descent (ibd) from a common ancestor. (i will use the word substring to denote contiguous subsequences, as is standard in the computer science text matching literature.) when using imputation approaches to infer missing values one wants to identify sequences that are as close as possible to the test sequence around the location being imputed, such as those that are ibd, or at least share long matches with the test sequence. maximizing the number of such long matches could also form the basis of genotype phasing. naive substring match testing would take on 2 m time for each test sequence, where there are n variable sites and m sequences, and hence on 2 m 2 time for complete all-pairs comparison within a set of sequences. by keeping a running match score to find maximal matches as in blast, it is straightforwardly possible to reduce this to o(nm) per single test, and so onm 2 across the whole collection, but this is still large for large m. recently suffix-array-based methods have proved powerful in standard sequence matching, as exemplified by bowtie , bwa and soap2 . here an approach based on suffix arrays is described that can find best matches within a set of sequences in o(nm) time, following preprocessing of the dataset also in o(nm) time, and empirically best single haplotype matches in o(n) time. the differences between the algorithms described here and standard suffix array based sequence matching are derived from the fact that there are many sequences that are all of the same length and already aligned. so on the one hand there is no need to consider offsets of the test sequence with respect to the sequences in the collection, but on the other hand the test sequence is long and we are looking for maximal matches of an arbitrary substring of the test sequence, not of the whole test sequence.here i present initial results on simulated data. a dataset of 100 000 haplotype sequences covering a 20mb section of genome sequence was simulated using the sequentially markovian coalescent simulator macs chen (2009) using essentially the command macs 100000 2e7-t 0.001-r 0.001 (in fact a larger simulation was undertaken, which crashed a little beyond 20mb, and the remaining material was trimmed down to this set). there are 370 264 segregating sites in this dataset. the raw macs output contains essentially the haplotype sequences written in 0s and 1s, and so is approximately 37gb in size. this compresses with gzip to 1.02gb. an initial implementation pbwt of the key algorithms was produced. this uses single byte run length encoding for the pbwt, with the top bit encoding the value, the next two bits selecting whether the length is in units of 1, 64 or 2048, and the remaining 5 bits giving the number of units. for runs 464 but 52048 this typically requires 2 bytes, and for runs 42048 but 564k this typically requires 3 bytes. all experiments were carried out on an apple mac air laptop with a 2.13ghz intel core 2 duo processor using a single core. encoding the dataset of 100 000 sequences described above took 1070 s (user plus system), generating a pbwt representation that is 7.7mb in size, over 130 times smaller than the gzip compression of the raw data. further results including application to subsets of the data are given in, showing that the relative gain increases with the number of sequences, indicating clearly the non-linear benefits of the algorithm. this can be clearly seen by the observation that for each increase of a factor of ten in the number of sequences, the average number of bytes used by the pbwt to store the haplotype values at a site only approximately doubles. as a test on real data, similar measures were applied to the chromosome 1 data from the 1000 genomes project phase 1 data release 1000 genomes project (2012), consisting of 2184 haplotypes at 3 007 196 sites. the gzip file of this data took 303mb, whereas the pbwt used 51.1mb, nearly a factor of six smaller, not far from the factor expected based on the simulated data. next algorithm 4 was implemented to find all set-maximal matches within the simulated datasets. as expected the timei have presented here a series of algorithms to generate positional prefix array data structures from haplotype sequences and to use them for very strong compression of haplotype data, and time and space efficient haplotype matching. in particular, the matching algorithms remove a factor of m, the size of the set of haplotype sequences being matched to, from the search time taken by direct pair-wise comparison methods. this makes it possible to find all best matches within tens of thousands of sequences in minutes, and generates the potential for practical software that scales to millions of sequences. although the algorithms are presented for binary data, they can be extended to multi-allelic data with a little care. these algorithms share aspects of their design with analogous algorithms based on suffix arrays for general string matching, but are structured by position along the string resulting in substantial differences. one consequence is that, unlike with suffix array methods where linear time sorting algorithms are non-trivial, building the sorted positional prefix arrays in linear time using algorithm 1 is straightforward. the approach used here is reminiscent of that used byto generate a string bwt from very large sets of short strings. with respect to efficient representation, it is interesting to note that the original bwt was introduced byfor string data compression, not search, and it in fact forms the basis of the bzip compression algorithm.have previously explored the use of bwt compressed self-index methods for efficient compression and search of genetic sequence data from many individuals, but this does not require a fixed alignment of variable sites as in the work presented here, and is substantially different. all the algorithms described here require exact matching without errors or missing data. as for sequence matching, if a more sensitive search is required that permits errors, it is still possible to use the exact match algorithms to find seed matches, and then join or extend these by direct testing. this would typically be the approach taken by production software, but having powerful methods to identify seeds is key to performance. an alternative to using suffix/prefix array methods in sequence matching is to build a hash table to identify exact seed matches. analogous to the creation of position prefix arrays described here, it would be possible to build a set of positional hash tables for each position in the haplotype sequences. hash based methods when well tuned can be faster than suffix array based methods, because the basic operations are simpler, but they typically require greater memory, particularly in cases where the suffix representation can be compressed as it can be here. a problem with genotype data not present in standard sequence matching is that the information content of positions varies widely, with a preponderance of rare sites with very little information, which would mean that the length of hash word would need to change depending on position in the sequence. an alternative would be to build hashes based on a subset of sites with allele frequency greater than some value such as 10, or in some frequency range, but this would lose information leading to false seed matches. most research into algorithms for analyzing large sets of haplotype or genotype data has focused on statistical methods that are powerful for inference, but only scale up to a few thousand sites and sequences; e.g. see marchini and howie (2010). recently accelerated methods have been developed that can handle data up to tens of thousands of sequences, e.g.or delaneau et al. (2012). however, these methods provide approximations to the statistical matching approaches, and are still much heavier than the algorithms presented here. over a million people have been genotyped, and although there are logistical issues in bringing together datasets on that scale, genotype data on sets of 4100 000 people are becoming available . one approach to more efficient phasing and imputation may be to use computationally efficient approaches such as the positional prefix array methods to seed matches for statistical genotype algorithms, or at other computational bottlenecks. for example, in their beagle software browning and browning (2007) build a probabilistic hidden markov model from a variable length markov model of the local haplotype sequences which is essentially derived from a dynamic truncation of the positional prefix array. although their algorithms using this probabilistic model take them in a different direction, i suggest that the methods described here could be used to significantly speed up the model building phase of beagle. alternatively, a more direct approach may also be possible. most phasing and imputation algorithms build a model from the entire dataset, then thread each sequence in turn against it to provide a new phasing based effectively on a series of matches. instead, the positional prefix algorithms progress jointly along all sequences. if we start at both ends of the data, then at some position k we have information about matches in both directions based on the current phasing, and can propose an assignment of alleles for all sequences at k in a single step, before incrementing k. approaches based on this idea may be fast and complementary to current methods.  
