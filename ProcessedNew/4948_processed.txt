yoabs: yet other aligner of biological sequencesâ€”an efficient linearly scaling nucleotide aligner motivation: explosive growth of short-read sequencing technologies in the recent years resulted in rapid development of many new alignment algorithms and programs. but most of them are not efficient or not applicable for reads 200 bp because these algorithms specifically designed to process short queries with relatively low sequencing error rates. however, the current trend to increase reliability of detection of structural variations in assembled genomes as well as to facilitate de novo sequencing demand complimenting high-throughput short-read platforms with long-read mapping. thus, algorithms and programs for efficient mapping of longer reads are becoming crucial. however, the choice of long-read aligners effective in terms of both performance and memory are limited and includes only handful of hash table (blat, ssaha2) or trie (burrows-wheeler transform-smith-waterman (bwt-sw), burrows-wheeler alignerr-smith-waterman (bwa-sw)) based algorithms. results: new o(n) algorithm that combines the advantages of both hash and trie-based methods has been designed to effectively align long biological sequences (200 bp) against a large sequence database with small memory footprint (e.g. 2 gb for the human genome). the algorithm is accurate and significantly more fast than blat or bwt-sw, but similar to bwt-sw it can find all local alignments. it is as accurate as ssaha2 or bwa-sw, but uses 3+ times less memory and 10+ times faster than ssaha2, several times faster than bwa-sw with low error rates and almost two times less memory. availability and implementation: the prototype implementation of the algorithm will be available upon request for non-commercial use in academia (local hit table binary and indices are atdevelopment of sensitive local alignment algorithms was started in late 1980 with several pioneering tools such as fasta and blast . for alignments of highly similar sequences to genomes, they were followed later by a new generation of faster methods, e.g. megablast , ssaha2 , blat and patternhunter . next-generation sequencing technologies pushed development of new algorithms even further for efficient processing of millions of short (100 bp) reads. these ultra-fast tools were orders of magnitude faster and included soap , maq , bowtie , bwa , stampy , etc. however, emerging single molecule sequencing technologies are constantly pushing read lengths into longer and longer realm (1k base pairs and more). most of these ultra-fast short read tools do not perform well on these not so short reads as the tools were exclusively designed for reads 100 bp. but efficiently aligning long reads (200 bp) against a long reference sequence (1 gb, like e.g. the human genome) has different overall objectives and hence represents a different challenge to the development of alignment tools. in contrast to short-read alignment when the best match is deduced by the end-to-end mapping of the query to the reference that minimizes a number of mismatches, long-read alignment is often based on several local matches and thus is being able to detect both structural variations in the query and erroneous assemblies in the reference. additionally, short-read aligners are optimized for ungapped alignment and introduction of even limited number of short (several base pairs) gaps impose heavy performance penalties on these short-read algorithms. long-read aligners on the contrary should be able (and optimized) to deal with arbitrary number of gaps of arbitrary size each. the majority of currently available long-read alignment algorithms may be classified as either using hash table indexing, like in blat or in ssaha2 , or using some sort of compressed trie indexing based on burrows wheeler transform (bwt) , for example, in bwt-sw or in bwa-sw . but in spite of using different indexing strategies all the above long alignment algorithms follow the seed-andextend paradigm, i.e. they first search for one or more of the so called seeds (either short exact matches, as in ssaha2 and blat, or longer gapped matches in unique regions, as in bwasw). the found seeds are then extended to cover the whole query sequence using the smithwaterman algorithm (bwa-sw uses this algorithm for identifying long gapped seeds as well). this extension algorithm is computationally expensive and although long-read aligners introduce various heuristic accelerations to limit use of this computationally penalizing phase by reducing unnecessary seed extensions especially in highly repetitive regions, the resulting tollremains heavy. even in currently fastest heuristically accelerated bwt-based algorithms (e.g. bwa-sw) this toll is indirectly present through a large constant associated with each bwt operation. this article outlines a new long alignment algorithm, yet other aligner of biological sequences (yoabs), that does not use the seedand-extend paradigm and, hence, does not bear those computational expenses imposed by the smithwaterman algorithm (as a matter of fact, it does not use the dynamic programming at all). the algorithm is designed to combine the advantages of both hash-and trie-based algorithms. similar to hash-based algorithms it uses a series of look ups to build a correspondence between a query and a reference and, hence, has low-associated computational overhead. similar to compressed trie algorithms it compresses highly repetitive regions and suppresses short repetitive matches that poison the performance of hash-based algorithms. the high accuracy, the low computational complexity as well as low memory requirements make the algorithm a candidate for specialty implementations with graphics processing units (gpus) or field-programmable gate arrays (fpgas). the article also presents some preliminary evaluation of algorithms possible practical performance using work-in-progress test implementation along with bwa-sw and ssaha2 on both simulated and real data.yoabs is an efficient algorithm (both memory-and performancewise) for aligning a several hundred or more base pairs query sequence to a long reference genome. it has high sensitivity and specificity (especially given a long query or a query with low error rate). the accuracy of yoabs is comparable with the most accurate long sequence aligners so far (e.g. bwa-sw or ssaha2). by design yoabs is well suited to detect arbitrary gaps and chimeras, therefore it can be used to facilitate detection of structural variations or reference misassemblies. in contrast to the majority of long sequence alignment algorithms (e.g. bwa-sw, ssaha2 or blat) yoabs does not use the seed-and-extend paradigm. instead it records all local hits between all l(prefix)+m(suffix) base pairs index entries for the reference sequence (organized as a forward and a backward tries) and all l +m base pairs subsequences (including l and 2l gapped) of the query sequence. the local hits are stored as a table of l-scaled modulo 2 k query subtracted location in the reference versus modulo l location in the query. as a result, the algorithm avoids using the expensive dynamic programming stage (the smithwaterman algorithm) altogether replacing it with linearly scaling simulated annealing type of procedure. the overall complexity of the algorithms is bounded by the complexity of the local hit table building step, that is o(2 k q) (where q is the query length) or o(256q) for k = 8 used in prototype implementation, hence it does not depend on the size r of the reference. it would be interesting to estimate the value of a constant in the complexity expression but i would expect it to be much lower than r 0.628 /256, taking into account relatively low cost of lookup operations that comprise most of the local hit table construction efforts when comparing with o(r 0.628 q) complexity expressions for bwa-sw or bwt-sw , or with o(rq) of the dynamic programming step in general. this low complexity also makes the algorithm compelling for gpu and/or fpga implementation. an important difference of the algorithm from various band accelerated modifications of the smithwaterman algorithm (that is from approaches maintaining a small fraction of the dynamic programming matrix and, hence, allowing better than o(rq) scaling at the expense of missing some of the possible matches, for example for gaps larger than chosen band size) is that it records all the local hits and therefore will not miss any of the true matches. other interesting projects to pursue consist in detailed comparison of performance and accuracy of the simulated annealing stage of the algorithm with the dynamic programming approach for ranges and errors typical for resequencing projects, as well as introducing a query indexing to make it sublinear complexity suitable for de novo sequencing.  
