data and text mining bayesian rule learning for biomedical data mining motivation: disease state prediction from biomarker profiling studies is an important problem because more accurate classification models will potentially lead to the discovery of better, more discriminative markers. data mining methods are routinely applied to such analyses of biomedical datasets generated from high-throughput omic technologies applied to clinical samples from tissues or bodily fluids. past work has demonstrated that rule models can be successfully applied to this problem, since they can produce understandable models that facilitate review of discriminative biomarkers by biomedical scientists. while many rule-based methods produce rules that make predictions under uncertainty, they typically do not quantify the uncertainty in the validity of the rule itself. this article describes an approach that uses a bayesian score to evaluate rule models. results: we have combined the expressiveness of rules with the mathematical rigor of bayesian networks (bns) to develop and evaluate a bayesian rule learning (brl) system. this system utilizes a novel variant of the k2 algorithm for building bns from the training data to provide probabilistic scores for if-antecedent-then-consequent rules using heuristic best-first search. we then apply rule-based inference to evaluate the learned models during 10-fold cross-validation performed two times. the brl system is evaluated on 24 published omic datasets, and on average it performs on par or better than other readily available rule learning methods. moreover, brl produces models that contain on average 70 fewer variables, which means that the biomarker panels for disease prediction contain fewer markers for further verification and validation by bench scientists.high-throughput omic data that measure biomarkers in bodily fluids or tissues are accumulating at a rapid pace, and such data have the potential for the discovery of biomarkers for early diagnosis, monitoring and treatment of diseases such as cancer. data mining methods that learn models from high-dimensional data are being increasingly used for the multivariate analyses of such biomedical datasets. together with statistical univariate analyses, some insights into predictive biomarkers of disease states can be gleaned, though to whom correspondence should be addressed. the results may not generalize due to the small sizes of available training data, typically less than 200 samples. due to the large imbalance between variable dimensionality (several thousand) and the sample size (a few hundred), there is a need for data mining methods that can discover significant and robust biomarkers from high-dimensional data. rule learning is a useful data mining technique for the discovery of biomarkers from highdimensional biomedical data. we have previously developed and applied rule learning methods to analyze omic data successfully . rules have several advantages, including that they are easy for humans to interpret, represent knowledge modularly and can be applied using tractable inference procedures. in this article, we develop and evaluate a novel probabilistic method for learning rules called the bayesian rule learning (brl) algorithm. this algorithm learns a particular form of a bayesian network (bn) from data that optimizes a bayesian score, and then translates the bn into a set of probabilistic rules. the use of the bayesian approach allows prior knowledge (as probabilities) to be incorporated into the learning process in a mathematically coherent fashion. the possibility of over-fitting is attenuated by the incorporation of prior probabilities into the rule-discovery process. brl outputs the predictive rule model with the best bayesian score, which represents the probability that the model is valid given the data. the remainder of the article is organized as follows. section 2 presents the brl algorithm and briefly reviews other popular rule learning methods. section 3 describes the datasets and the experimental setup to evaluate brl. section 4 presents the results of applying brl to 24 published omic datasets, and compares its performance with multiple rule-learning algorithms. section 5 presents our conclusions.the average baccs obtained from 10-fold cross-validation performed two times for each of the 24 datasets are shown inaverages over the genomic datasets 121 (ga) and their sds, as well as averages over the proteomic datasets 2224 (pa) and their sds. bold numbers indicate highest performance on a dataset.as can be seen from the average baccs for the 24 datasets, both brl 1 and brl 1000 clearly perform better than the other rule learning methods. this holds for both the genomic datasets (121) and the proteomic datasets (2224). we see that brl 1000 has the highest bacc on 15 datasets, while brl 1 has the highest bacc on 4 datasets. on the remaining five datasets, c4.5 has the highest bacc on three, ties with brl on one and conjunctive rule learner has the highest on one. only the first dataset is very easy to classify by all rule learners. as seen in, the performance of both brl 1 and brl 1000 are statistically significantly better than c4.5, its nearest competitor in terms of bacc. when compared with each other, brl 1000 outperforms brl 1. the average rcis obtained by the various rule learning methods are shown in. brl 1000 has the highest rci on 19 datasets, whereas brl 1 has the highest rci on 3 datasets. there was one tie among the two brl methods and c4.5. in addition, c4.5 has the highest rci on one dataset. in, we compare the difference in performance using the rci measure between c4.5 with brl 1 and brl 1000 and both brl methods are statistically significantly better than c4.5; the difference in performance using the rci measuredepicts a comparison of the average number of variables (markers) appearing in the rule models for c4.5, brl 1 and brl 1000 , when run with default parameter settings. the average was calculated over the models generated from 20 folds (obtained from stratified 10-fold cross-validation repeated two times) on the 24 datasets. as shown, the brl models have 70 less variables on average in their models than c4.5. if each predictor variable has only two discretized ranges of values, then brl with default parameters would generate between 2 3 and 2 5 rules on average. however, discretization could yield a larger number of value ranges for a variable, thereby increasing the number of rules generated by brl. to reduce the number of rules, we can prune rules with zero coverage, that is, those rules whose left-hand side does not match any of the samples in the training data. we notice that pruning does not harm brls performance. however, rule pruning could cause problems during testing, since rules that do not match training data could still match test data. we include an example of pruned rules and also c4.5 rules in the supplementary material. the variables chosen in brls predictive models are often different from those chosen by c4.5.there are several advantages that accrue from brl that are not available in current rule learning algorithms. brl allows for the evaluation of the entire rule set using a bayesian score. using such a score results in a whole model evaluation instead of a per rule (or local) evaluation, which often occurs with ripper and c4.5. the bayesian score allows us to capture the uncertainty about the validity of a rule set. brl currently uses this score only for model selection. however, the score could be utilized in extensions to brl for performing inference when rule sets can be weighted by this score, which would be a form of bayesian model averaging. a bayesian approach allows incorporation of both structure and parameter priors. when training data are scarce, such as in omic data analysis, it is useful to incorporate prior knowledge to improve the accuracy of learned models. for example, a scientist could define all of the variable relationships using either a knowledge base or restrict the possible variables with which to build the model . in a bayesian approach, a scientist might provide prior knowledge specifying conditional independencies among variables, constraining or even fully specifying the network structure of the bn. in addition to providing such structure priors, the scientist might also specify knowledge in the form of prior distributions over the parameter values of the model. structure priors are arguably the most useful, however, because in our experience scientists are often more confident about structural relationships than about parameter values. we have not explored informative priors in this article. we used uniform parameter and uniform structure priors. exploring informative structure priors in this domain is a direction for future research. there are different ways of representing non-informativeness of parameters using the dirichlet priors. we have explored one approach, it would be useful to explore other approaches as well. an interesting open problem is to investigate methods for brl rule ordering and pruning within a set of rules. for example, pruning a set of brl rules based on using local structure and scores would be worth investigating. a major advantage of brl is that it can find models with fewer variables (markers) that have equivalent or greater classification performance than those obtained from several other rule learning methods. fewer variables mean fewer markers for biological verification and subsequent validation. this is important in biomarker discovery and validation studies that have to be designed carefully and under tight resource constraints.  
