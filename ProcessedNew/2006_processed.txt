sequence analysis genome compression: a novel approach for large collections motivation: genomic repositories are rapidly growing, as witnessed by the 1000 genomes or the uk10k projects. hence, compression of multiple genomes of the same species has become an active research area in the past years. the well-known large redundancy in human sequences is not easy to exploit because of huge memory requirements from traditional compression algorithms. results: we show how to obtain several times higher compression ratio than of the best reported results, on two large genome collections (1092 human and 775 plant genomes). our inputs are variant call format files restricted to their essential fields. more precisely, our novel ziv-lempel-style compression algorithm squeezes a single human genome to $400 kb. the key to high compression is to look for similarities across the whole collection, not just against one reference sequence, what is typical for existing solutions.the dna sequencing technology has become so affordable that there are several large-scale projects in which at least hundreds of individuals of some species are sequenced. from many perspectives, including the advent of personalized medicine, the homo sapiens data belong to the most interesting, and this is the reason why large projects like the 1000 genomes project (1000gp) and the uk10k project (http://www.uk10k.org), with thousands of human genomes sequenced so far, were initiated. among such projects, the most ambitious perhaps is the personal genome project (pgp) , with genomes of 100 000 individuals as the anticipated outcome. large repositories are built not only for human genomes, to mention 1001 genomes project (1001gp), with arabidopsis thaliana genetic variation (http://www. 1001genomes.org/about.html). it is a well-known fact that two organisms of the same species are highly similar; it was estimated that the genomes of two persons are identical in 99.5 . the huge amount of data obtained in the large-scale projects demands efficient ways of storing them. taking into account the high similarity of organisms, it becomes obvious that some compression method may be effectively applied. compression of single genomic sequences is hardly efficient, as the best obtained compression ratios achieve a factor of 4 or 5 only. when realized that instead of the complete genomic sequence, storing only differences between it and some referential sequence is enough, the task became easier. in their seminal paper,showed how to store the description of variations between james watsons (jw) genome and a referential genome in only 4.1 mb. however, the authors prior knowledge was not only the reference sequence but also the single nucleotide polymorphism (snp) map. as the input, they took the information about the snps and insertion or deletion (indels) variations between jw genome and referential genome, and compressed these data using some clever, but simple, techniques. comparing with $3.1 gbases of human genome, this means $750-fold compression. in the following years, a number of articles on relative compression of genomes were published . in all the articles, the input sequences were complete genomes, not differences between genomes and a reference genome. this complicates the compression problem, as it is necessary to find the differences between genomes without any prior knowledge and without a database of variants (i.e. snp and indel database). the most successful of the algorithms seems to be gdc , which differentially compressed a collection of 69 human genomes from complete genomics inc. to 215 mb (3.1 mb per individual). it is based on the zivlempel paradigm and finds approximate matches between the genome sequences. recently,showed how to improve the technique from, compressing the jw genome to 2.5 mb, with very similar average results on multiple 1000gp genomes. the introduced novelties are partly biologically inspired, e.g. making use of tag snps characterizing haplotypes. another line of research concerns indexing genomic collections (or more generally, repetitive sequences), i.e. building data structures enabling fast pattern search in the genomes . such indexes are efficient when the index resides in the main computer memory, which is challenging considering the sheer volume of indexed data. some of the listed works are rather theoretical and their to whom correspondence should be addressed. implementations are not yet available, whereas the works that have been implemented are tested on relatively small collections, not exceeding $1 gb. in this article, we try to answer the question how well a collection of genomes of the same species can be compressed, when knowledge of the possible variants is given. the cited works ofare so far the only attempts to compress a (single) genome sequence with a variant database. in this work, we take two large collections of genomes (h.sapiens and a.thaliana) and try to exploit cross-sequence correlations in the variant loci. our solution is a specialized ziv-lempel-style compressor, where the input sequences are basically formed by binary flags denoting if successive variants from the database were found in the individuals. this approach appears highly successful, allowing to store the human collection in 432 mb (395 kb per individual) and the plant collection in 110 mb (142 kb per individual). we point out that the general idea of exploiting common features for improved compression is known for some other ngs tasks, including compression of (both mapped and unmapped) reads . in the next section, we present the input data and the general idea of our approach. then, we show some details of the proposed compression algorithm. finally, we evaluate the compressor. the last section concludes the article.as mentioned earlier, for the evaluation of the proposed compression algorithm, we use two datasets of 1092 h.sapiens and 775 a.thaliana genomes. in all experiments, the data are processed chromosome by chromosome. this approach, typical in the genomic compression literature (see, e.g. deorowicz and), reduces the memory footprint, speeds up computations and improves the compression ratio for the generic algorithms. there are several tools for compressing genomic sequences in fasta format. unfortunately, the amount of our test data, $6.8 tb of raw sequences if converted to fasta, is so huge that the running times of some of those compressors would be counted in months. thus, we started from a preliminary test in which we evaluated the most powerful as well as the most recent tools for only two human chromosomes (14 and 21) and also two plant chromosomes (1 and 4). the results are presented in. this and all further experiments were performed on a computer equipped with four 4-core 2.4 ghz amd opteron cpus with 128-gb ram running red hat 4.1.2-46 linux. two of the presented compressors may be considered fast with regard to the compression speed: abrc with $100 mb/s (run with 8 threads) and gdc-normal with $40 mb/s speed (only a serial implementation exists), while the others are by about one [gdc-ultra and rlz (or about two (7z) orders of magnitude slower. interestingly, the only generic compressor in the tests, 7z, is the second best in the compression ratio (after gdc-ultra), but its compression speed is low (0.4 mb/s). the memory available for the compression has a major impact on the compression ratio. for example, 7z was run with its maximum setting, 1 gb for its lz-buffer (translating to 410 gb of total memory use), yet it fit only a small part of the input for h.sapiens data: 510 individuals (each of size $108 mb) for chromosome 14 dataset and 21 individuals (each of size $48 mb) for chromosome 21 dataset. this, supposedly, was the main reason for which its compression ratio in the latter case is significantly higher. the hypothesis is indirectly confirmed by the results for much shorter a.thaliana chromosomes, for which more individuals fit the 1-gb lz-buffer and the compression ratios are close to gdc-ultra. in the next experiment, we compared a few well-known generic compressors (gzip, bzip2, 7z) on vcfmin input files . compressed sizes (in megabytes) and compression times are reported for selected chromosomes and the collections in total. surprisingly perhaps, the best compression was obtained by bzip2 compressor.is similar, but now the inputs are in our temporary dense representation, vdbv. here, the generic compressors are compared against our proposal, tgc. two simple observations can be made: (i) the more compact of these two input representations, vdbv, is clearly more appropriate for the best generic compressor, 7z, both from the point of compression ratio and from speed; (ii) tgc is significantly better than vdbv7z in both measured aspects, which demonstrates that designing a specialized compression algorithm was a worthy goal in this case. the results are summarized in. for comparison purposes, we also show the sizes of the raw sequences as well as the compression results of the best compressor working on such representation, gdc-ultra. we resigned, however, from presenting the compression and decompression times of gdc-ultra, as it works on a completely different representation than vcfmin,which we use in the article. the compression ratios of gdc can be treated as a reference point. the most important numbers from this summary are the average sizes of genomes in the most compact, tgc, representation. the obtained 395 kb (for the human data) is more than six times smaller than offered by the best so far genome sequence, gdc-ultra, compressor. also, the very recent paper by, working on a representation similar to our vcfmin, reports more than six times larger files. when expressing the compression ratios in relation to the raw genome sequence sizes, it means that our algorithm squeezes h.sapiens genomes $15 500 times and a.thaliana $850 times.in the last experiment, we compared tgc against speedgene , an algorithm for efficient storage of snp datasets. for an honest comparison, we restricted the 1000gp set of variants to snps only. speedgene requires the input data to be in linkage format, in which there is no distinction between chromosomes in each pair, i.e. for each snp it describes only whether no snp is found in a genome, one snp (on any chromosome) is found, two snps are found (on both chromosomes) or the status of snp is unknown. thus, we changed our algorithm slightly and instead of processing each single chromosome, we joined chromosomes of each pair, and obtained vectors of dibits, i.e. bit pairs. these vectors are then transformed intonote: there are two columns containing ratios: ratio to raw tells how many times the compressed file is smaller than the genome sequences in fasta format. ratio to vcfmin is the compression ratio according to the size of vcfmin files. for gdc-ultra, compression and decompression times are not given, as this compressor uses a different input form than others and such a comparison would be irrelevant. the values marked in bold indicate best compression.note: all sizes are in megabytes and times are in seconds. the vdbv c-time column contains the conversion times from vcfmin format to vdbv format. in the remaining columns titled c-time, total compression time is given (i.e. vdbv 7z c-time denotes the sum vcfmin-to-vdbv conversion time and 7z compression time; tgc c-time is the total tgc processing time, comprising the conversion to vdbv and the actual compression). note that the variant database (part of the vdbv representation) is of size 933 mb for h.sapiens and 320 mb for a.thaliana. after compressing by tgc, their sizes (included in tgc size column) are $51.0 mb and 12.5 mb, respectively. the values marked in bold indicate best compression. the extended version of this table (with results for all chromosomes) can be found in supplementary table s2. byte vectors (each byte contains four consecutive dibits). in this way, our tool can compress these byte vectors without any change.presents the compressed sizes obtained by speedgene and tgc. we show the results for three chromosomes, as well as for the complete genome. as one can see, tgc reduces the dataset size more than four times better than speedgene.we examined the possibility of obtaining much better compression ratios of genomic collections than from existing tools, when additional knowledge is given. the knowledge was the information about the possible variants in genomes and the occurrence of these variants in specific genomes. this helps a lot in compression of genomic sequences, as all input sequences are perfectly aligned and the task of finding repetitions in data (usually the most important and time-consuming task handled by data compression algorithms) becomes rather simple. we should mention that in theory such perfect alignments can be found by compression algorithms, but the computational burden would be enormous. thus, compression tools usually make some heuristic decisions when comparing the sequences in the hope that they do not lose too much. the success of our algorithm was possible not only because of the variant database, but also because we searched for crosscorrelations between individuals. in other words, for each individual, similarities to any other previously processed individual (i.e. runs of repeating variants) can be found. in principle, the available memory may be a limiting factor but processing the collection on the chromosome level resulted in 52.5 gb memory use for the larger (human) of the tested datasets. in the future, when much more genomes are available, we may need to re-address the memory issue though, possibly via working on blocks smaller than whole chromosomes, or trying to re-order the sequences in a way to maximize local similarities. in the compression method design, we sometimes traded compression ratio for reduced memory requirements, e.g. some (rather minor) improvements in compression would be possible owing to higher-order contextual modeling. probably, a more practical approach is to make use of more biological knowledge; the very recent work ofgives new insight, which might be possible to use in our scheme, but we leave it for future work. why such experiments can be interesting? although accurate and efficient analyses of such huge (several terabytes in raw format) genomic collections remain a major challenge, we believe that the mere compressibility of human genomes (e.g. as a lower bound for memory requirements of future algorithms and tools) is a question worth investigating. for example, our compressed collection takes $430 mb, so including also a compressed reference genome (at most 700 mb) requires $1.1 gb of space, which seems quite modest. naturally, running efficient queries over such data is another matter (clearly with some overhead in space use), but our results suggest this is not impossible. the information kept in vcf or genome variation format (gvf) files is often more detailed (e.g. may include quality scores) than what our tool preserves. although clearly efficient compression methods for such data are also needed, we do not anticipate a possibility to obtain similar compression ratios to tgc, unless a (strongly) lossy mode is used. unfortunately, we cannot see a way to easily adapt our compression techniques for such data. tgc allows extracting an arbitrary chromosome (or a whole genome) from the compressed collection, yet this solution is simple and rather slow. making this extraction faster, or (even better) allowing for quick access to position-restricted arbitrary snippets of the genomes in the collection, is an important task left for future work. clearly, there must be some space-time tradeoffs for such functionalities. a somewhat related functionality will be to add or remove an individual genome to/from the collection. currently, changing the archive content requires recompressing the collection from scratch. the performed experiments showed that even the best genomic sequence compressor, gdc-ultra, is significantly (up to seven times) poorer in compression ratio than what can be obtained with extra knowledge. the main conclusions from our work are:note: vcfmin means a simplified vcf with snp calls only that spends only 4 bytes for each genotype. all sizes are in megabytes. the values marked in bold indicate best compression.  
