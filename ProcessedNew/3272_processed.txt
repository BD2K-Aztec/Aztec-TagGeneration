gene expression non-linear classification for on-the-fly fractional mass filtering and targeted precursor fragmentation in mass spectrometry experiments motivation: mass spectrometry (ms) has become the method of choice for protein/peptide sequence and modification analysis. the technology employs a two-step approach: ionized peptide precursor masses are detected, selected for fragmentation, and the fragment mass spectra are collected for computational analysis. current precursor selection schemes are based on data-or information-dependent acquisition (dda/ida), where fragmentation mass candidates are selected by intensity and are subsequently included in a dynamic exclusion list to avoid constant refragmentation of highly abundant species. dda/ida methods do not exploit valuable information that is contained in the fractional mass of high-accuracy precursor mass measurements delivered by current instrumentation. results: we extend previous contributions that suggest that fractional mass information allows targeted fragmentation of analytes of interest. we introduce a non-linear random forest classification and a discrete mapping approach, which can be trained to discriminate among arbitrary fractional mass patterns for an arbitrary number of classes of analytes. these methods can be used to increase fragmentation efficiency for specific subsets of analytes or to select suitable fragmentation technologies on-the-fly. we show that theoretical generalization error estimates transfer into practical application, and that their quality depends on the accuracy of prior distribution estimate of the analyte classes. the methods are applied to two real-world proteomics datasets.current mass spectrometry (ms) technologies provide unpreceded mass accuracy and resolution, yielding detailed qualitative and to whom correspondence should be addressed. quantitative insight into a sample under investigation. in fields including, but not limited to, proteomics, glycomics, lipidomics and metabolomics, ms is now an established workhorse methodology and high-resolution ms data acquisition have become the norm. the accurate observed mass of a chemical compound carries more information than the mere molecular weight. it can be used to distinguish between peptides and non-peptides , cysteine-rich and/or highly acidic peptides, glycopeptides and non-polar peptides ; between peptide oligonucleotide cross-links and dna fragments ; between unmodified and posttranslationally modified peptides ; and between target precursors and interference peptides , and it can pinpoint mass-defect calibrants in quantitative ms . accurate mass measurements enable on-the-fly analysis decisions that formerly required time-intensive off-line analytical procedures . mass defect, mass excess and fractional mass: the driving principle behind accurate mass-based decisions is the concept of nuclear mass defect, defined as the difference between the sum of the masses of the constituent nucleons and the measured exact mass of an atom . this difference is an instance of einsteins special theory of relativity stating that mass and energy are interchangeable : the nuclear mass defect accounts for the nuclear binding energy and is always non-negative. the concept of mass excess is a direct consequence of the nuclear mass defect: it is defined as the difference d = m obs m nom between the observed mass m obs and the nominal mass m nom of an element or compound. in particular, the mass excess of 12 c is defined as zero and mass excesses of other elements can either be positive (e.g. 1 h: 1.00783 and 14 n: 14.00307) or negative (e.g. 16 o: 15.99491, 32 s: 31.97207, 31 p: 30.97376 and 127 i: 126.90447). however, in practical experiments there is generally no prior knowledge about the chemical composition of an observed mass, and as a consequence, the nominal mass cannot directly be observed. instead, high-resolution ms instruments provide the fractional mass (m) = m m, i.e. the fraction of mass after the decimal point of a mass measurement m. the fractional mass is confined to the interval (m) [0,1) and, for mass excess larger than one, it wraps around to zero .theoretical fractional mass filtering performance measures hold in practical application: the results from experiment 1 (cf.and c) show that theoretical and practical performance estimates are close if the training and real-world modification state distributions are equal. in this case, the theoretical error estimates obtained for fractional mass classifiers trained on in silico datasets can be transferred to practical application and are valid estimates for the unknown and unobservable experimental errors. the required degree of modification state distribution similarity depends on the difficulty page: 796 791797and c as well as inall real-world aucs are larger than 0.68 and 0.90 for the phosphorylation and iodination problem, respectively. a random assignment to the associated classes according to the class priors would correspond to an auc of 0.5, illustrating that fractional mass-based classification yields effective precursor selection. consequently, there is a benefit in the combination of: (i) learning discrimination functions based on theoretical ground truth; and (ii) applying the resulting classifiers to characterize observations in practical ms experiments. more detailed efficiency analyses depend on the experiment at hand. for the phosphorylation data, we also calculated the positive and negative predictive values (ppv and npv, see supplementary). in the context of targeted fragmentation, the ppv can be understood as a measure of efficiency (the ratio of true positives among those reported as fragmentation candidates) and 1npv describes the miss rate (the ratio of false negatives among those reported as non-candidates). for the modification distributionadjusted phosphorylation experiments, optimal values of ppv and npv are between 60 and 70, providing further evidence that the fractional mass classification is beneficial although the underlying classification problem is non-trivial. on-the-fly classification is feasible and equally accurate: the numbers inindicate that the impact of discretizing the neutral mass domain is marginal. for the two datasets used in this study, theoretical auc values for the random forest and the discrete mapping classifier are within a single sd of each other. this observation also holds for all real-world classification experiments with the exception of the modification state distribution-adjusted sus scrofa thyroglobin classification. here, the random forest outperforms the discrete mapping approach quite significantly. evidence from other thyroglobulin iodination datasets (data not shown) suggests that this is related to the comparatively small number of iodinated peptides present in the real-world dataset, which causes significant changes in the auc with only very few contradicting classifications between the methods. classification performance depends on modification state prior distribution accuracy: real-world classification performance is influenced by two factors: (i) the complexity/difficulty of the underlying classification problem; and (ii) the discrepancy between the modification state prior distributions used for training and present in the real-world dataset. the former can easily be illustrated by comparing the theoretical rocs and aucs for the phosphorylation and iodination problems shown in. because the fractional mass shift for phosphorylation is smaller than for iodination, the classes representing the different phosphorylation states exhibit a larger overlap (cf.). consequently, their discrimination is more difficult and the area under the theoretical roc curve for the phosphorylation problem is smaller than for the iodination problem . if the correct modification state distribution is used in the training step, the theoretical and realworld rocs and aucs are very similar (and c), if not, the cross-validated performance estimate reported for the theoretical data may overestimate the true error rates attained in real-world application . it is important to realize that this is an observation that focuses on the theoretical error estimate with respect to its validity in real-world application: although more detailed sampling of masses of less prevalent modification states may yield improved performance , there is no guarantee that the error estimates are sufficiently conservative under such circumstances. in the phosphorylation dataset, the influence of imperfect prior distributions of the modification states had a huge influence on the accuracy of the theoretical auc estimates but only little impact on the classification performance on real-world data (supplementary). in cases where the classification problem is comparatively simple due to clear-cut, large mass defects, the performance predictions are relatively stable with respect to the class prior distributions, i.e. roc curves and aucs change only marginally for different prevalence settings. this is illustrated by the iodination rocs in panels c and d in. the reason for this behavior is that false positives and false negatives are costly in terms of ms 2 sampling efficiency and it is necessary to jointly optimize true positives and true negatives. for the optimization of the joint criterion, the (unknown) ratio between true positives and true negatives needs to be considered, causing a dependency on the modification state prior distribution . however, in many proteomics experiments the peptide modification state prior distribution is unknown or in itself a subject of ongoing investigation. in such cases, available ms 2 datasets can be used to estimate a lower bound of ptm rates, but the applicability of such ground truth is heavily influenced by the bias introduced through sample preparation, the specific chemical enrichment protocols used, and dda. manually curated ptm databases suffer from the same drawbacks and are not available for all organisms of interest. a potential approach to circumventing this problem is to include a data-dependent retraining scheme, where modifications identified by mascot search as well as protein sequence databases serve as ground truth and the classifier undergoes constant, iterative re-training. consequently, with a growing knowledge base, the classifier performance can iteratively adapt to the true modification prevalence and converge towards optimality. such a procedure is computationally expensive and should thus only be applied to standardized, stable protocols.  
