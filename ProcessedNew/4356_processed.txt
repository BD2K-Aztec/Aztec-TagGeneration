sequence analysis specificity control for read alignments using an artificial reference genome-guided false discovery rate motivation: accurate estimation, comparison and evaluation of read mapping error rates is a crucial step in the processing of next-generation sequencing data, as further analysis steps and interpretation assume the correctness of the mapping results. current approaches are either focused on sensitivity estimation and thereby disregard specificity or are based on read simulations. although continuously improving, read simulations are still prone to introduce a bias into the mapping error quantitation and cannot capture all characteristics of an individual dataset. results: we introduce arden (artificial reference driven estimation of false positives in next-generation sequencing data), a novel benchmark method that estimates error rates of read mappers based on real experimental reads, using an additionally generated artificial reference genome. it allows a dataset-specific computation of error rates and the construction of a receiver operating characteristic curve. thereby, it can be used for optimization of parameters for read mappers, selection of read mappers for a specific problem or for filtering alignments based on quality estimation. the use of arden is demonstrated in a general read mapper comparison, a parameter optimization for one read mapper and an application example in single-nucleotide polymorphism discovery with a significant reduction in the number of false positive identifications. availability: the arden source code is freely available at http:// sourceforge.net/projects/arden/.throughout the past years, the rapid development of next-generation sequencing (ngs) technologies has shaped computational biology. the analysis of large amounts of data from sequencing runs, which regularly reaches millions of reads, is a key challenge in retrieving biological information. hence, a common part of most ngs applications is to perform a read mapping, the search of given read sequences in a much larger reference sequence. various methods have been published to efficiently solve the read mapping problem. popular mappers include bowtie2 , mrsfast , bwa and razers . for a comprehensive overview on current read mappers, we refer the reader to . however, it is difficult to judge whether a mapping result is appropriate for a given dataset and how to efficiently compare read mappers and how to choose their increasingly large number of tuning parameters. these challenges in read mapping become particularly apparent in the search for genomic variations, such as single-nucleotide polymorphisms (snps). by definition, the sequence of reads indicating snps differs from the reference genome. hence, the difficulty is to distinguish true snps from sequencing errors or computational mapping errors. the distinction between error and variant is not obvious in case a read does not match perfectly to the reference. here, the parameterization of a read mapper plays a crucial role. using only default settings may result in imperfect mappings, as they might be optimized for certain organisms or sequencing platforms. allowing mismatches may result in a high number of mappings, but these may be error prone and have a low quality. in contrast, requiring a high similarity might hinder the detection of snps. thus, a method for evaluation and quality control is required to find an optimal setting for a read mapper. to the best of our knowledge, quality control of read mappers is primarily based on sensitivity measurements or relies on read simulation as in, e.g., oliver (2012) and, as, in general, no ground truth is available for ngs experiments. however, we observed that adequate read simulations are difficult to achieve and prone to introduce a bias. it is infeasible to model all influence factors on data acquisition and the continuously improving sequencing chemistry poses a challenge to keep simulations up to date. further, the correlation between simulation result and a specific real dataset is inexplicit, as it is challenging to set parameters such as the error rate of the instrument a priori. to avoid this bias, we developed arden (artificial reference driven estimation of false positives in ngs data), which takes the opposite approach: rather than replacing reads by a simulation with a known ground truth, arden uses real reads and a simulated decoy reference genome for generating confidence measurements. thereby, arden is able to estimate and to control the number of incorrect alignments. similarly to the widely used decoy strategy in proteomics , our decoy approach is used to estimate the number of false-positive read mappings. this is motivated by the assumption that the number of hits on the decoy genome provides an estimate of the expected number of false positives in the original genome. the expectancy is that the occurrence of one hit on the decoy genome (considered as a random hit) has to whom correspondence should be addressed. the author 2013. published by oxford university press. all rights reserved. for permissions, please email: journals.permissions@oup.com approximately the same probability as an incorrectly mapped read on the original reference genome. this leads to an approximation of a false discovery rate . as no read simulation is necessary, arden is applicable on every dataset and adjusts to the specific sequencing runs. thus, it can be applied as a concurrent quality control and allows adjusting specificity settings separately for single experiments and the exclusion of potentially incorrect mappings from subsequent analyses. further, it provides a novel approach to benchmark read mappers or different read mapping settings. we demonstrate the applicability of arden in several evaluations: first, we use arden for a basic read mapper comparison. further, we determine the best parameter setting for a specific read mapper, and, finally, we show a specific application example for snp discovery.arden is a method for the identification and control of false positives in mappings of ngs data, for which we demonstrate a broad range of applications. arden allows the comparison of mapping algorithms on any dataset of interest rather than relying on a simulated dataset with potentially differing properties. the here presented comparison study also gives insight into characteristic algorithmic properties of different classes of read mappers. for example, fewer reads are expected to map distinctively to different positions on reference and artificial reference for hamming-based methods, such as mrsfast. for these approaches, a single point mutation does not change the start or end position of an alignment that falls into the same region on the reference and the artificial reference. moreover, hamming-based methods have harder constraints for finding an alignment, as they only consider substitutions. because of the seed and extend step, index-based methods suffer from a higher probability for mapping a read to the same region but on a different shifted position. thus, two classes of errors may contribute to false-positive alignments, shifted alignments and alignments that map to diverse regions. in general, this leads to a higher error rate for mappers, such as bowtie2 or bwa, than for hamming distance-based methods. also, in general, edit distance mappers align a higher percentage of mapped reads (in comparison with hamming distance mappers) at the cost of an increased probability of false mappings. the reason is that edit distance mapping has relaxed constraints for finding an alignment than hamming distance mapping, as it allows substitutions and indels. accordingly, razers2 (configured in edit distance mode) and bowtie2 mapped a highernote: the ground truth contained 150 simulated snps. arden decreases the number of fp while retaining all tps. the effect of filtering depends on the particular mapper and the respective results of arden. for bowtie2, bwa and razers2, the percentage of all alignments that have been removed by the filter are 6:8, 2:5 and 3:4, respectively. the relative difference between the all and filt. category is denoted as .note: tps were compared with a simulated ground truth containing 1000 simulated snps and to public available snp data (a more detailed distinction is available in the supplementary material). for razers3 and bwa, the filtering with arden considerably reduced the numbers of fps along with a comparably small loss of tps. for bowtie2, the number of fps is decreased along with a gain in tps. the relative difference between the all and filt. category is denoted as .the error measurement allows the dataset specific optimization of mapping parameters and makes read mappers comparable by an auc metric. moreover, arden provides the possibility to determine a user-specified cut-off to improve the accuracy of alignments for a specific read mapper based on sensitivity, specificity and the corresponding auc. more accurate mappings improve the quality of follow-up applications, as we demonstrated in an snp discovery experiment where using arden decreased the number of false positives by up to 72 while maintaining the majority of true positives.  
