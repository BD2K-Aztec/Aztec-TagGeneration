data and text mining permutation importance: a corrected feature importance measure motivation: in life sciences, interpretability of machine learning models is as important as their prediction accuracy. linear models are probably the most frequently used methods for assessing feature relevance, despite their relative inflexibility. however, in the past years effective estimators of feature relevance have been derived for highly complex or non-parametric models such as support vector machines and randomforest (rf) models. recently, it has been observed that rf models are biased in such a way that categorical variables with a large number of categories are preferred. results: in this work, we introduce a heuristic for normalizing feature importance measures that can correct the feature importance bias. the method is based on repeated permutations of the outcome vector for estimating the distribution of measured importance for each variable in a non-informative setting. the p-value of the observed importance provides a corrected measure of feature importance. we apply our method to simulated data and demonstrate that (i) non-informative predictors do not receive significant p-values, (ii) informative variables can successfully be recovered among non-informative variables and (iii) p-values computed with permutation importance (pimp) are very helpful for deciding the significance of variables, and therefore improve model interpretability. furthermore, pimp was used to correct rf-based importance measures for two real-world case studies. we propose an improved rf model that uses the significant variables with respect to the pimp measure and show that its prediction accuracy is superior to that of other existing models. availability: r code for the method presented in this article is available atin recent years, statistical learning has gained increased attention in a large number of research fields. there exist two main goals for the application of statistical learning: either the generation of a (possibly black box) model that predicts a variable of interest given a number of putatively predictive features, or the generation of insight into how the predictive features impact on the variable of interest (given that the prediction model performs reasonably well). this latter task of feature discovery or feature ranking is the essence of biomarker discovery in bioinformatics and life sciences, for instance. unfortunately, not all statistical learning methods can be used for identifying interesting features because their underlying methods are too complex to analyze contributions of single covariates to the overall results. this problem applies, for instance, to artificial neural networks and support vector machines (svms) with nontrivial kernels. however, in the case of svms recently approaches to interpreting models that apply sequence kernels were presented . in life sciences, the most frequently applied methods for quantifying feature importance are linear models and decision trees. linear svm and linear logistic regression are well-studied theoretical models that can provide interpretable classification rules via model parameters. moreover, in difficult situations when the number of predictors exceeds greatly the number of available samples, regularizers such as the lasso penalty can be used for obtaining sparse models. however, linear classifiers fail to discover complex dependencies in the training data. this is clearly a drawback when biological data are analyzed, since biological processes usually involve intricate interactions. decision trees are suitable for finding non-linear prediction rules that are also interpretable, although their instability and lack of smoothness have been a cause of concern . the randomforest (rf;) classifier was designed to overcome these problems and recently became very popular because it combines the interpretability of decision trees with the performance of modern learning algorithms such as artificial neural networks and svms. the author of rf proposes two measures for feature ranking, the variable importance (vi) and gini importance (gi). a recent study showed that, if predictors are categorical, both measures are biased in favor of variables taking more categories . the authors of the article ascribe the bias to the use of bootstrap sampling and gini split criterion for training classification and regression trees (cart;). in the literature, the bias induced by the gini coefficient has been reported for years , and it affects not only categorical variables but also grouped variables (i.e. values of the variable cluster into well-separated groupse.g. multimodal gaussian distributions), in general. in biology, predictors often have categorical or grouped values (e.g. microarrays and sequence mutations).propose a new algorithm (cforest) for building rf modelspage: 1341 13401347in this work, we proposed an algorithm for correcting for two biased measures of feature importance. the method permutes the response vector for estimating the random importance of a feature. under the assumption that the random importance of a feature follows some distribution (gaussian, lognormal or gamma), the likelihood of the measured importance on the unpermuted outcome vector can be assessed. the resulting p-value can serve as a corrected measure of variable relevance. we showed how this method can successfully adjust the feature importance computed with the classical rf algorithm, or with the mi measure. we also introduced an improved rf model that is computed based on the most significant features determined with the pimp algorithm. simulation a demonstrated that the gi of the rf and mi favor features with large number of categories and showed how our algorithm alleviates the bias. simulation b demonstrated the usefulness of the algorithm for generating a correct feature ranking. for all methods, the feature ranking based on the unprocessed importance measures could be improved. when feature importances of rf are distributed among correlated features, our method assigns significant scores to all the covariates in the correlated group, even for very large group size. this improves model interpretability in applications such as microarray data classification, where groups of functionally related genes are highly correlated. pimp was used to correct for rf-based gi measures for two realworld datasets. both case studies use features based on nucleotide or amino acid sequences. as already discussed bycategorical features (e.g. nucleotide sequences) are often used together with derived continuous features (e.g. free-fold energy) for improving the prediction model. in this case, it may happen that the continuous variables are preferred by tree-based classifiers as they provide more meaningful cut points for decisions. pimp on the c-to-u dataset demonstrated successful post-processing page: 1347 13401347  
