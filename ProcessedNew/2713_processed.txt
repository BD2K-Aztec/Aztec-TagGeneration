lightassembler: fast and memory-efficient assembly algorithm for high-throughput sequencing reads motivation: the deluge of current sequenced data has exceeded moores law, more than doubling every 2 years since the next-generation sequencing (ngs) technologies were invented. accordingly, we will able to generate more and more data with high speed at fixed cost, but lack the computational resources to store, process and analyze it. with error prone high throughput ngs reads and genomic repeats, the assembly graph contains massive amount of redundant nodes and branching edges. most assembly pipelines require this large graph to reside in memory to start their workflows, which is intractable for mammalian genomes. resource-efficient genome assemblers combine both the power of advanced computing techniques and innovative data structures to encode the assembly graph efficiently in a computer memory. results: lightassembler is a lightweight assembly algorithm designed to be executed on a desktop machine. it uses a pair of cache oblivious bloom filters, one holding a uniform sample of g-spaced sequenced k-mers and the other holding k-mers classified as likely correct, using a simple statistical test. lightassembler contains a light implementation of the graph traversal and simplification modules that achieves comparable assembly accuracy and contiguity to other competing tools. our method reduces the memory usage by 50 compared to the resource-efficient assemblers using benchmark datasets from gage and assemblathon projects. while lightassembler can be considered as a gap-based sequence assembler, different gap sizes result in an almost constant assembly size and genome coverage.the advent of next-generation sequencing (ngs) technologies has revolutionized the genomic research, but has not been able to provide a complete picture of a sequenced organism, since the relative positions of the billions of fragmented pieces are unknown without a genome assembly, which is a highly ambiguous overlapping puzzle . de novo sequence assembly is an initial step towards downstream data analysis such as understanding evolutionary diversity across different species, evidenced by the multitude of data collection projects, including genome 10k . with the increasing efforts to sequence and assemble the genomes of more organisms, the assembly problem becomes more complicated and computationally intensive, especially with short inaccurate sequenced reads and genomic repeats . next-generation assembly algorithms play around two basic frameworks for efficiently completing their task: namely, de bruijn and string graphs. in a de bruijn graph, nodes are the set of distinct k-mers (substrings of length k) extracted from reads and the edges are the k 1 -overlap among them. the string graph is a simplified version of a classical overlap graph, where nodes are the sequenced reads and the non-transitive edges encode their suffix-to-prefix overlaps . many efforts have been made to fit the assembly graph into computer memory by the creation of resource-efficient genome assemblers. the term resource efficiency touches on both memory space and speed . one compressed representation for a string graph is introduced in sga using fm-index and burrowswheeler transformation of the sequenced reads . recently, an incremental hashing technique combined with a probabilistic data structure (bloom filter) revisited the string graph representation (ben). the early condensed representation of de bruijn graph is a sparse bit vector , later implemented in a gossamer sequence assembler . this representation is changed in minia by introducing the exact representation of de bruijn graph using the combination of a bloom filter and a hash table that holds an approximate set of false positive nodes. the hash table is replaced in subsequent versions of minia by a set of cascading bloom filters for further space optimization . the burrowswheeler transformation plays another role in the succinct representation of de bruijn graph by combining fm-index with frequency-based minimizers to reduce its complexity . sparseassembler stores a subsample of k-mers in a hash table with their overlap links, recorded to maintain de bruijn graph representation. abyss distributes the assembly graph nodes among different machines to reduce the representation complexity in a computer memory. resource-efficient sequence assemblers vary in their assembly results in terms of both accuracy and contiguity measures. each tool has a set of advantages and disadvantages according to the compromises made to achieve efficiency. also, different evaluation studies generally reported that the assembly algorithms differ in their outputs according to their working scenarios such as the quality of sequenced data and the complexity of the corresponding genome. there is a common conclusion that there is no one tool is best for all scenarios, and that there is still room for improvement in current assembly pipelines. in this paper, we revisit de bruijn graph representation and introduce an optimized cache oblivious bloom filter to the sequence assembly. our method is inspired by lighters idea to correct the sequenced errors using a pair of bloom filters and a simple statistical test. lighter stores a random sample of k-mers in a bloom filter and uses them with a simple statistical test as seeds to classify the read positions as trusted or untrusted. while lighters goal is to use the trust-classified k-mers to correct erroneous ones, our ultimate goal is assembling these k-mers without error correction since they are already classified as trusted nodes (k-mers made by k consecutive trust-classified positions in the sequenced reads are considered to be trusted). lightassembler obtains a uniform sample of k-mers by skipping g bases between the k-mers, where g is the gap length and stores them in a bloom filter. the erroneous bases in a read will produce rare k-mers and are unlikely to survive in the sample compared to the abundant k-mers generated by the correct bases. the trustiness of a read position will be determined by comparing the number of k-mers that cover the position and appear in the sample to a statistically computed threshold. lightassembler uses the k-mers made by k consecutive trust-classified reads positions as the set of assembly graph traversal nodes, while several assemblers rely on error correction modules to identify and correct the erroneous k-mers before starting the assembly process. the majority of error correction algorithms count the k-mers to determine their confidence and exclude ones with a multiplicity less than a specified threshold, which might result in missing a subset of true k-mers with low abundance. other assemblers such as velvet rely on intensive graph simplification modules to resolve the erroneous structures introduced by erroneous bases such as tips and bubbles. complex assembly pipelines combine both approaches and perform postprocessing graph filtering using mate pairs during scaffolding stage. lightassembler uses only two passes over the sequenced reads to identify the approximate set of trusted nodes without error correction or intensive graph simplification modules. also, one of the efficient representations of de bruijn graph based on a bloom filter is implemented in minia and uses k-mer counting module to identify the set of trusted k-mers. minias counting algorithm follows a divide and conquer paradigm and utilizes the disk space as secondary memory storage. our method is able to identify the set of trusted k-mers without utilizing either a counting module or disk-space overhead. we will present our comparable results to the current state-of-the-art sequence assemblers as well as resource-efficient ones using the simulated and benchmarked datasets.we evaluated the performance of lighassembler against minia v2.0.3 , sparseassembler and abyss v1.5.2 using simulated datasets from the escherichia coli reference genomewith different attributes listed in supplementary 1. we also compared our results with the same assemblers, including velvet v1.2.10 using real benchmark datasets from gage and assemblathon 2 evaluation studies, the characteristics of benchmark datasets are presented in supplementary 1. lightassembler, like the other chosen assembly tools, is a. lightassembler graph traversal module. the first step in the graph traversal module is computing the set of branching k-mers (k-mers have multiple extensions). (a) the successors for each trusted k-mer are computed by appending a nucleotide nt 2 a; c ; g; t f g , for example, the successors of a k-mer cata, where k 4 are ataa; atac ; atag; atat f g : (b) bloom filter b is queried for each successor to check its presence in the sequenced reads. if the number of existing successors for each trusted k-mer is larger than one, the trusted k-mer is considered as a branching node. otherwise, it is a simple node. (c) each assembled contig starts from a branching node in the assembly graph, where each node is extended one nucleotide at a time and bloom filter b is continuously queried for checking the presence of extended k-mers. the assembly graph is simplified by removing the dead end paths and resolving the simple bubbles de bruijn graph-based assembler. lightassembler, minia and sparseassembler are also considered as resource-efficient contigbased assembly tools. they do not utilize the paired-end information encoded in the sequenced libraries to perform scaffolding, while abyss and velvet have their own scaffolding modules. in order to make a fair comparison, we evaluated all methods based on their resulted contigs without using paired-end information such as the insert size. then, we performed scaffold analysis based on our resulted contigs compared to those from other methods using sspace v3.0. 0 as one of stand-alone scaffolding tools. one of the major assembly steps is evaluating assembly results to assess their accuracy and contiguity. when a reference genome is available, the assembly results are evaluated by mapping the assembled contigs or scaffolds back to the reference with the possibility of setting a minimum length threshold for the mapping contigs/scaffolds. the assembly evaluation tools have different reported metrics and even different approaches when a reference genome is absent. gage, assemblathon and quast are the most popular tools. the metrics used to evaluate the assembly results from the competing tools are described in supplementary 1according to their definition in gage and assemblathon studies. while quast has gage option to run the assembly evaluation using gage standards, we found that for the same minimum contigs threshold length, quast ng50 equals to gage n50 before performing the contigs correction step (breaking contigs at every misjoin and at every indel longer than 5 bases.). quast has a slightly higher n50 contig length compared to those from gage and assemblathon 2. the default minimum threshold for contigs analysis in quast is 500 bp, which can be adjusted by the end user. gage has a fixed threshold contig length equals to 200 bp, while it is not specified in the assemblathons paper their threshold-based analysis. the scaffold-based contiguity analysis is used in assemblathon 2 to report the contigs statistics by breaking the scaffolds into their corresponding contigs, which increases the n50 contig length compared to the length reported by the contigs-based analysis from other evaluation tools. also, ng50 reported by the assemblaton script equals to the n50 length reported by gage before doing the contigs correction step. we will use gage script to evaluate the assembly results from all competing programs. all conducted computer experiments and the exact command lines used for each tool are described in detail in supplementary 1.  
