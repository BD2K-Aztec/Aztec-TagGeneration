genecodeq: quality score compression and improved genotyping using a bayesian framework motivation: the exponential reduction in cost of genome sequencing has resulted in a rapid growth of genomic data. most of the entropy of short read data lies not in the sequence of read bases themselves but in their quality scoresthe confidence measurement that each base has been sequenced correctly. lossless compression methods are now close to their theoretical limits and hence there is a need for lossy methods that further reduce the complexity of these data without impacting downstream analyses. results: we here propose genecodeq, a bayesian method inspired by coding theory for adjusting quality scores to improve the compressibility of quality scores without adversely impacting geno-typing accuracy. our model leverages a corpus of k-mers to reduce the entropy of the quality scores and thereby the compressibility of these data (in fastq or sam/bam/cram files), resulting in compression ratios that significantly exceeds those of other methods. our approach can also be combined with existing lossy compression schemes to further reduce entropy and allows the user to specify a reference panel of expected sequence variations to improve the model accuracy. in addition to extensive empirical evaluation, we also derive novel theoretical insights that explain the empirical performance and pitfalls of corpus-based quality score compression schemes in general. finally, we show that as a positive side effect of compression, the model can lead to improved genotyping accuracy. availability and implementation: genecodeq is available at:over the past decade, unprecedented advances in next generation sequencing (ngs) technologies have led to a dramatic reduction in sequencing cost and much faster data production rates . these technological advances are fostering increasingly wide-ranging applications in biotechnology, public healthcare and personalized medicine . furthermore, as genomic sequencing data has grown exponentially, they have outpaced advances in some information technologies that they indirectly rely on: computing power and storage . in particular, genome sequencing results in a large storage footprint for each genome. thus, storing and transferring raw sequencing information is becoming prohibitively expensive and effective compression schemes of raw sequencing data are indispensable.the majority of ngs data output consists of read sequence information whereby each nucleotide base is associated with a sequence confidence level (also referred to as quality score), produced by the base caller . the phred scale q 10log 10 p encodes with integer quality score q an estimate of the probability p that the base has been called incorrectly. this information is used both for the quality control of raw read data and for downstream processing, including genome assembly, read mapping and genotyping . in compressed genomic datasets, quality score values take up the dominating share. for example, when compared to the read sequence data, quality scores of illumina reads take at least 2.3 more storage, although this can be an even higher ratio when using more aggressive sequence compression . quality scores are more difficult to compress due to a larger alphabet (6394 in original form) and intrinsically have a higher entropy . with lossless compression algorithms and entropy encoders reaching their theoretical limits and delivering only moderate compression ratios , there is a growing interest to develop lossy compression schemes to improve compressibility further. quantizing quality scores (i.e. reducing the alphabet size) is the most basic approach to improve compressibility in a lossy manner. one such approach of reducing all quality values to eight levels (bins) has become a widely used standard for the illumina platform and is enabled by default on the most recent machines . another approach called p) involves local quantization so that a representative quality score replaces a contiguous set of quality scores that are within a fixed distance of the representative score. similarly the r) scheme replaces contiguous quality scores that are within a fixed relative distance of a representative score. other lossy approaches improve compressibility and preserve higher fidelity by minimizing a distortion metric such as meansquared-error or l1-based errors (qualcomp and qvz) . however, adoption of lossy compression schemes for quality scores has been slow due to concerns about adverse effects on downstream analyses, in particular genotyping accuracy . however, there are also reports that compression schemes such as p-block, r-block, qvz and qualcomp can, under some circumstances, lead to a slight improvement in genotyping accuracy. a number of more recent approaches utilize the sequence data itself to guide the quality score compression. quartz achieves this using a reference corpus built from frequent 32-mers across reads from individuals sequenced in the 1000 genomes project . read base pairs that match any one 32-mer in the corpus (up to one allowed mismatch per 32-mer) have their quality score set to a fixed high value. this sparsification of quality scores reduces entropy, thus improving quality score compressibility. a different approach, leon, utilizes the dataset itself for building its set of k-mers, and to generate a reference probabilistic de brujin graph. in this case, bases in a read that have enough highly frequent k-mers covering it within the dataset are set to a fixed high-value quality score. both methods were reported to improve genotyping accuracy. in this article we present genecodeq, a lossy compression scheme that is inspired by coding theory and bayesian inference. uniquely, genecodeq uses a statistical approach to objectively reason about the compressibility of quality scores. briefly, our model estimates the posterior probability of a sequencing error given the evidence of the full read, including quality scores, together with information from a reference corpus. as a result, the posterior estimates of most quality scores are boosted above a saturation point of the phred scheme, corresponding to very high confidence. this approach results both in a significant reduction in entropy and better genotyping accuracy when compared to existing methods.weve used sequences from na12878 for evaluation purposes due to availability of high quality trio-validated snp calls to validate genotyping accuracy. unmapped raw reads were obtained for two datasets: @bullet srr622461, a 1000 genomes project (1000 genomes) dataset with 18.3 gigabases at 5 coverage. @bullet na12878j, a public dataset from the garvan institute with 122.6 gigabases at 30 coverage (see supplementary materials for details). resulting variant calls from genotyping were compared to the illumina platinum set (http://www.illumina.com/platinumgenomes), which served as a gold standard. in the supplementary material we also provide results for additional datasets. the raw fastq files were processed with genecodeq, quartz , r), p-block (c), qvz , qualcomp , leon and illumina 8-bin quantization . we used the gatk best practices workflow and the samtools recommended workflow, with and without basequality-score-recalibration (bqsr). in addition to running genecodeq in its default mode, we also explored its behaviour with a reference-only corpus (see section 3.2 for more details), as well as in combination with other approachesillumina 8-bin, p-block and r-block (see section 3.4 for more details). for comparison, quartz was also run with a reference-only corpus, and in combination with illumina 8-bin quantization. we found that the bqsr stage produces a greater variable change to genotyping accuracy than most of the lossy compression algorithms do alone (pre-bqsr). this makes it difficult to separate and properly assess the impact of lossy compression alone on postbqsr results. on the lossless dataset, applying bqsr resulted in a significant drop in genotyping accuracy. combinations of bqsr with lossy compression also resulted in lower genotyping accuracy versus the original across almost all methods. however, since bqsr had a large variable impact, sometimes the lossy compressed postbqsr results appeared better than the lossless post-bqsr results, skewing relative comparisons. there are strong indications that this is due to flaws with bqsr itself rather than due to the lossy compression. a detailed discussion, included additional results, can be found in the supplementary material. the improvements with genecodeq are seen across each combination of workflows, for brevity the main results shown here are with the gatk best practices workflow excluding bqsr. similar results are seen with the samtools recommended workflow, which can be found in the supplementary material, along with bqsr results. variant calls that resulted from the datasets and produced using these workflows, were ranked by their confidence levels (quality of the variant call) to generate receiver operating characteristic (roc) curves. this approach avoids choosing a specific quality threshold and hence is well suited to compare alternative methods (see alsowhere a similar approach is used). however, if two roc curves have different domains (i.e. a different set of variants for the x-axis) then they are not directly comparable to one another. for the results here and in the supplementary material, the roc curves and area under the curve (auc) were calculated with a common domain. as additional quality metrics, we also report precision, recall and f-score metrics. see supplementary material for more details on the evaluation approach for all these metrics. to estimate the impact in compressibility, both the raw and modified quality scores were compressed with bzip2 (http://www. bzip.org), unless a custom entropy coder was already integrated into the lossy compression (i.e. qvz, qualcomp and leon). we chose bzip2 because under its default options it consistently yielded superior compression numbers compared to 7zip (http://www.7-zip.org) and gzip (http://www.gzip.org). we note, however, that 7zip in its ppmd mode can perform even better, and these results are included in the supplementary material. this includes significantly better 7zip re-compressed results for leon which ordinarily uses the poorer performing zlib for its internal compression.reports quality score compression rates and genotyping accuracy metrics for these approaches.shows a scatter plot summarizing the best results in, depicting the genotyping accuracy and compressibility.  
