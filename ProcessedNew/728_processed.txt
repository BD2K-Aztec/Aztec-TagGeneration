bigbwa: approaching the burrowsâ€“wheeler aligner to big data technologies bigbwa is a new tool that uses the big data technology hadoop to boost the performance of the burrowswheeler aligner (bwa). important reductions in the execution times were observed when using this tool. in addition, bigbwa is fault tolerant and it does not require any modification of the original bwa source code.burrowswheeler aligner (bwa) is a very popular software for mapping sequence reads to a large reference genome. it consists of three algorithms: bwa-backtrack , bwa-sw and bwa-mem . the first algorithm is designed for short illumina sequence reads up to 100 bp, whereas the others are focused on longer reads. bwa-mem, which is the latest, is preferred over bwa-sw for 70 bp or longer reads as it is faster and more accurate. in addition, bwa-mem has shown better performance than other several state-of-art read aligners for mapping 100 bp or longer reads. sequence alignment is a very time-consuming process. this problem becomes even more noticeable as millions and billions of reads need to be aligned. for instance, new sequencing technologies, such as illumina hiseqx ten, generate up to 6 billion reads per run, requiring more than 4 days to be processed by bwa on a single 16core machine. therefore, ngs professionals demand scalable solutions to boost the performance of the aligners in order to obtain the results in reasonable time. in this article, we introduce bigbwa, a new tool that takes advantage of hadoop as big data technology to increase the performance of bwa. the main advantages of our tool are the following. first, the alignment process is performed in parallel using a tested and scalable technology, which reduces the execution times dramatically. second, bigbwa is fault tolerant, exploiting the fault tolerance capabilities of the underlying big data technology on which it is based. and finally, no modifications to bwa are required to use bigbwa. as a consequence, any release of bwa (future or legacy) will be compatible with bigbwa.performance: bigbwa was tested using data from the 1000 genomes project (for details). measurements were performed on a five-node aws cluster with 16 cores per node (intel xeon e5-2670 at 2.5 ghz cpus), running hadoop 2.6.0. detailed information about the experimental setup is provided in the supplementary material. performance results for bigbwa and the other evaluated tools only take into considerationall the datasets were extracted from the 1000 genomes project .highlighted the best tool for a particular number of cores. for fair comparison with the other tools, bigbwa obtains these results using bwa version 0.5.10. tool versions: pbwa 0.5.9 and seal 0.4.0.highlighted the best tool for a particular number of cores. these results were obtained using bwa version 0.7.12. the alignment process time, which was calculated as the average of 5 runs per data point after one warm-up execution.shows a comparison with seal and pbwa for the bwa-backtrack algorithm. in this case, bigbwa clearly outperforms these tools, especially when the number of cores used is high. in this way, speedups of 36.4 were reached with respect to the sequential case (using the original bwa tool as reference). it can also be observed that the scalability of seal is worse, caused by the overhead introduced by pydoop with respect to the use of jni. performance of bwa-mem is shown in. it was measured using only bwa (threaded version) and bigbwa, because seal and pbwa do not support this algorithm. we have also included results for a hybrid version that uses bigbwa in such a way that each mapper processes the inputs using bwa with two threads. results show that, with a small number of cores, bwa behaves slightly better than bigbwa. note that bwa is limited to execute on just one cluster node and, therefore, we cannot provide results using more than 16 cores. considering 16 cores, bigbwa is always the best solution but, due to the memory assigned per map task in our cluster configuration, only 13 concurrent tasks can be executed on one node. in this way, bigbwa always distributes the tasks between two nodes when using 16 cores. in addition, bigbwa shows good behavior in terms of scalability for all the datasets considered, executing up to 36.6 faster than the sequential case. additional performance results are shown in the supplementary material. correctness: we verified the correctness of bigbwa by comparing its output file with the one generated by bwa. differences range from 0.06 to 1 on uniquely mapped reads (mapping quality greater than zero), similarly to the differences shown by the threaded version of bwa with respect to the sequential case.  
