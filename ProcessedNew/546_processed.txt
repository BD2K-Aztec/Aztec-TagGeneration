genome analysis a statistical framework for power calculations in chip-seq experiments motivation: chip-seq technology enables investigators to study genome-wide binding of transcription factors and mapping of epige-nomic marks. although the availability of basic analysis tools for chip-seq data is rapidly increasing, there has not been much progress on the related design issues. a challenging question for designing a chip-seq experiment is how deeply should the chip and the control samples be sequenced? the answer depends on multiple factors some of which can be set by the experimenter based on pilot/prelim-inary data. the sequencing depth of a chip-seq experiment is one of the key factors that determine whether all the underlying targets (e.g. binding locations or epigenomic profiles) can be identified with a targeted power. results: we developed a statistical framework named cssp (chip-seq statistical power) for power calculations in chip-seq experiments by considering a local poisson model, which is commonly adopted by many peak callers. evaluations with simulations and data-driven computational experiments demonstrate that this framework can reliably estimate the power of a chip-seq experiment at different sequencing depths based on pilot data. furthermore, it provides an analytical approach for calculating the required depth for a targeted power while controlling the false discovery rate at a user-specified level. hence, our results enable researchers to use their own or publicly available data for determining required sequencing depths of their chip-seq experiments and potentially make better use of the multiplexing functionality of the sequencers. evaluation of power for multiple public chip-seq datasets indicate that, currently, typical chip-seq studies are powered well for detecting large fold changes of chip enrichment over the control sample, but they have considerably less power for detecting smaller fold changes.next-generation sequencing technologies produce tens of millions of sequence reads during each instrument run and are used to answer questions central to human diseases. multiple nih consortia (encode, modencode, 1000 genomes, roadmap epigenome) are pursuing mapping of transcription factor (tf) binding and epigenome in multiple tissues and developmental stages with chip-seq applications . analysis of chip-seq experiments involves comparing sequence reads from a chip sample to an appropriate control sample (e.g. chromatin input) to identify genomic loci/regions that exhibit enrichment in the chip sample compared with the input sample. although there are 430 algorithms for analyzing data from chip-seq experiments (reviewed in), there has been little and mostly empirically driven efforts on the design of these experiments . identification of biologically interesting genomic regions can be hindered by background noise. detection of these regions can be improved by sequencing more reads. the total number of reads from a sequencing experiment is referred to as the sequencing depth. sequencing depths so far have been set empirically because of lack of a formal statistical framework, e.g. the encode consortium suggested using a minimum sequencing depth of 20 million (m) mapped reads for sequence-specific tfs ; however,recently concluded with empirical studies that the regularly adopted sequencing depth of 1520 m reads in humans may not be high enough.explored the impact of sequencing depths of chip samples using saturation analysis. this analysis evaluated the effect of sequencing depth on the number of peaks discovered by identifying peaks from reads sub-sampled at varying proportions from the original chip sample. the proportion of sub-sample peaks that overlap the peaks from the full set is plotted against the sub-sample depth. when this curve reaches a horizontal asymptote, it indicates that the set of detected enrichment sites has stabilized at the current depth. although this computational approach is useful for evaluating the available sequencing depth of a chip sample, it has three major drawbacks: (i) it is not suited for addressing how many more reads are needed if saturation has not been reached at the available depth [e.g. in a recent encode publication , rna pol ii, which mainly interacts with dna across genes, exhibited a nearly linear gain in the number of peaks through 50 m reads with no indication of how many more reads are needed for saturation]; (ii) it only evaluates saturation based on the chip sample and discards the control sample; and (iii) it only allows investigating saturation from the point of either a minimum fold-enrichment or false discovery rate (fdr), but not both. to whom correspondence should be addressed.addressing the question of sequencing depth requires (i) defining a statistical criterion that can quantify the information loss of an experiment because of its apparent sequencing depth and (ii) determining the sequencing depth needed to control the information loss based on a pilot, possibly undersequenced, dataset. from a statistical point of view, chip-seq peak calling procedures can be cast as multiple testing problems because they aim to assess whether data for each candidate locus is supported by the background noise distribution or the chip signal. therefore, the information loss is naturally connected to the concept of the testing power. as a result, both of the aforementioned issues can be considered within a power calculation framework where the sequencing depth plays the role of sample size. power computations require modeling distribution of both the background reads and chip signal in a way that reflects the stochastic nature of read accumulation at each genomic locus as a function of sequencing depth. although a number of models were proposed for locus-specific read counts, none of them explicitly accounted for read accumulation.considered models with locally poisson distributed background and did not model chip signal.proposed a flexible model taking into account the genome structure and overdispersion. however, this model used the input sample as a covariate and did not explicitly parametrize the model in terms of sequencing depths.proposed a hierarchical bayesian t-mixture model to identify local concentration of directional reads, but did not consider the relationship between read accumulation and sequencing depth.adopted a signal-to-noise model, parameters of which followed some arbitrary prior distribution to account for intrinsic read bias. although such a prior distribution, if estimated, could be utilized to model the background distribution at varying sequencing depths, the work ofexclusively focused on the normalization aspect of chip-seq analysis. we developed cssp (chip-seq statistical power) framework for statistical power calculation by considering a local poisson model for the read generation process. we assume that background reads in the chip and the input samples are generated by local poisson processes with shared gamma prior distributions. the corresponding gamma parameters are modeled as functions of the local genome structure, including mappability and gc content. the local poisson parameters for the enrichment signals follow convolution of gamma distributions. this model preserves the local structure of themodel while keeping the negative binomial distribution as the marginal signal distribution as in. such a local structure is key for capturing dynamics of the counting process for individual genomic locus as a result of increasing sequencing depths. we introduce a conditional power definition that uses the practically used notion of fold change of chip signal over the control input sample. we show with data-driven computational experiments that our approach can be used to determine (i) the apparent conditional power for a given sequencing depth; (ii) the required sequencing depth to achieve a target power while controlling the fdr at a specified level. simulation experiments based on a deeply sequenced escherichia coli dataset indicate that power predictions of our model agree well with the observed empirical power. using data from pilot studies, we can reliably estimate power for larger sequencing depths; thus, the cssp framework has significant implications for designing chip-seq experiments with the multiplexing functionality. finally, we study the power of multiple encode datasets with varying sequencing depths. our results illustrate that, although the power varies considerably with the signal-to-noise ratios of the datasets, the current sequencing depths have high power for proteindna interactions with large effect sizes and are generally adequate for smaller effect sizes. our calculations are further supported by the data quality metrics proposed by the encode project .we first evaluated our cssp framework in a simulation study to assess the consistency of our parameter estimates, power and fdr control (supplementary materials section c). then we performed sub-sampling experiments based on two deeply sequenced datasets [e.coli fnr chip-seq dataset of myers et al. (2013) and mouse gata1 chip-seq dataset of wu et al.] to demonstrate the consistency and power of our cssp framework. we used multiple human ctcf chip-seq datasets from the encode consortium to evaluate the impact of laboratory and laboratory-specific batch effects on power estimation. finally, we investigated eight encode datasets to assess the power of currently available typical chip-seq studies.the sequencing depths of most, if not all, initial published experiments have been limited by practical considerations, such as cost or instrument availability. with decreasing sequencing costs, considerations are shifting from how many sequences should be obtained for a single experiment, to how many experiments one can perform in a single lane. therefore, power calculations are extremely important for chip-seq experiments. we have developed the cssp framework to enable such power calculations. this framework can be applied to compute power at a wide range of sequencing depths with varying fold change and minimum intensity thresholds. our extensive computational experiments demonstrated the consistency in predicting power from pilot data and its practical implications. to the best of our knowledge, this is the first model that enables power analysis for chip-seq data through an analytical approach. it is worth noting that although our calculations mostly emphasize the sequencing depth n y , other parameters including e 0 and j , which indicate the signal-to-noise ratio of the data, as well as the data quality are also important factors of the power analysis. these parameters are fixed when comparing datasets obtained under the same experimental conditions. however, for comparing datasets with different experimental conditions, such as tf and cell line, effects of data quality and strengths of enrichment signals should bear equal emphasis. our limited investigation of the laboratory and batch effects indicated that laboratory effects are larger than batch effects within a laboratory, and that pilot data from the same laboratory would yield more unbiased power prediction than pilot data from another laboratory. although the analytical calculations in the cssp framework depend on the peak calling procedure implied by our model, theoverlap proportion was calculated as the proportion of spp peaks that are among the cssp peaks. the spp peaks were constructed by extending each peak of spp by the estimated half window size in both of the 5 0 and 3 0 directions.  
