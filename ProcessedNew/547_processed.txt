sequence analysis a statistical framework for snp calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data motivation: most existing methods for dna sequence analysis rely on accurate sequences or genotypes. however, in applications of the next-generation sequencing (ngs), accurate genotypes may not be easily obtained (e.g. multi-sample low-coverage sequencing or somatic mutation discovery). these applications press for the development of new methods for analyzing sequence data with uncertainty. results: we present a statistical framework for calling snps, discovering somatic mutations, inferring population genetical parameters and performing association tests directly based on sequencing data without explicit genotyping or linkage-based imputation. on real data, we demonstrate that our method achieves comparable accuracy to alternative methods for estimating site allele count, for inferring allele frequency spectrum and for association mapping. we also highlight the necessity of using symmetric datasets for finding somatic mutations and confirm that for discovering rare events, mismapping is frequently the leading source of errors. availability:the 1000 genomes project (1000 genomes) sets an excellent example on how to design a sequencing project to get the maximum output pertinent to human populations. an important lesson from this project is to sequence many human samples at relatively low coverage instead of a few samples at high coverage. we adopt this strategy because with higher coverage, we will mostly reconfirm information from other reads, but with more samples, we will be able to reduce the sampling fluctuations, gain power on variants present in multiple samples and get access to many more rare variants. on the other hand, sequencing errors counteract the power in variant calling, which necessitates a minimum coverage. the optimal balancing point is broadly regarded to be in the 26 fold range per sample , depending on the sequencing error rate, level of linkage disequilibrium (ld) and the purpose of the project. a major concern with this design is that at 26 fold coverage per sample, non-reference alleles may not always be covered by sequence reads, especially at heterozygous loci. calling variants from each individual and then combining the calls usually yield poor results. the preferred strategy is to enhance the power of variant discovery by jointly considering all samples . this strategy largely solves the variant discovery problem, but acquiring accurate genotypes for each individual remains unsolved. without accurate genotypes, most of the previous methods [e.g. testing hardyweinberg equilibrium (hwe) and association mapping] would not work. to reuse the rich methods developed for genotyping data, the 1000 genomes project proposes to impute genotypes utilizing ld across loci . suppose at a site a, one sample has a low coverage. if some samples at a have high coverage and there exists a site b that is linked with a and has sufficient sequence support, we can transfer information across sites and between individuals, and thus make a reliable inference for the low-coverage sample at a. the overall genotype accuracy can be greatly improved. however, imputation is not without potential concerns. first, imputation cannot be used to infer the regional allele frequency spectrum (afs) because imputation as of now can only be applied to candidate variant sites, while we need to consider non-variants to infer afs. second, the effectiveness of imputation depends on the pattern of ld, which may lead to potential bias in population genetical inferences. third, the current imputation algorithms are slow. for a thousand samples, the fastest algorithm may be slower than read mapping algorithms, which is frequently the bottleneck of analyzing ngs data (h.m.kang, personal communication). considering more samples and using more accurate algorithms will make imputation even slower. these potential concerns make us reconsider if imputation is always preferred. we notice that we perform imputation mainly to reuse the methods developed for genotyping data, but would it be possible to derive new methods to solve classical medical and population genetical problems without precise genotypes? another application of ngs that requires genotype data is to discover somatic mutations or germline mutations between a few related samples . for such an application, samples are often sequenced to high coverage. although it is not hard to achieve an error rate one per 100 000 bases , mutations occur at a much lower rate, typically of the order of 10 6 or even 10 7. naively calling genotypes and then comparing samples frequently would not work well , because subtle uncertaintypage: 2988 29872993  
