sequence analysis lordec: accurate and efficient long read error correction motivation: pacbio single molecule real-time sequencing is a third-generation sequencing technique producing long reads, with comparatively lower throughput and higher error rate. errors include numerous indels and complicate downstream analysis like mapping or de novo assembly. a hybrid strategy that takes advantage of the high accuracy of second-generation short reads has been proposed for correcting long reads. mapping of short reads on long reads provides sufficient coverage to eliminate up to 99 of errors, however, at the expense of prohibitive running times and considerable amounts of disk and memory space. results: we present lordec, a hybrid error correction method that builds a succinct de bruijn graph representing the short reads, and seeks a corrective sequence for each erroneous region in the long reads by traversing chosen paths in the graph. in comparison, lordec is at least six times faster and requires at least 93 less memory or disk space than available tools, while achieving comparable accuracy. availability and implementaion: lordec is written in c++, tested on linux platforms and freely available at http://atgc.lirmm.fr/lordec.sequencing, the determination of dna or rna sequences, now belongs to the basic experiments in life sciences. compared with the sanger method, the so-called next-generation sequencing technologies (of the second, third or even fourth generations) have drastically lowered its cost and increased its efficiency, making genome-wide and transcriptome-wide sequencing feasible. numerous types of omics experiments, beyond de novo genome sequencing and assembly, have been invented and rely on high-throughput sequencing. all currently available technologies produce reads that represent only a piece of the target molecule sequence. processing these reads requires aligning them against other sequences: for instance, while mapping them against a reference genome, or when computing overlaps among reads during assembly. optimal, and sometimes suboptimal, alignments are retained for further analysis. the strength of an alignment (and hence its usefulness) is mostly controlled by two factors: its percentage of identity and its length. clearly, errors introduced during the sequencing process, sequencing errors, blur the signal in an alignment by introducing mismatches or by breaking it into shorter ones. weaker alignments may not pass subsequent filters and are lost for downward analyses. the finer the analysis, the higher the necessity to capture the information available in all alignments: for instance, when trying to bridge a gap in a less covered region of genome during assembly, or to reconstruct the sequence of a less expressed rna. to counteract sequencing errors, error correction algorithms have been found effective for de novo assembly , and so they are often incorporated in assembly pipelines [see e.g. euler sr , allpaths-lg and soapdenovo2 (.  
