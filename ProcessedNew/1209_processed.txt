cross-study validation for the assessment of prediction algorithms motivation: numerous competing algorithms for prediction in high-dimensional settings have been developed in the statistical and machine learning literature. learning algorithms and the prediction models they generate are typically evaluated on the basis of cross-validation error estimates in a few exemplary datasets. however, in most applications, the ultimate goal of prediction modeling is to provide accurate predictions for independent samples obtained in different settings. cross-validation within exemplary datasets may not adequately reflect performance in the broader application context. methods: we develop and implement a systematic approach to cross-study validation, to replace or supplement conventional cross-validation when evaluating high-dimensional prediction models in independent datasets. we illustrate it via simulations and in a collection of eight estrogen-receptor positive breast cancer microarray gene-expression datasets, where the objective is predicting distant metastasis-free survival (dmfs). we computed the c-index for all pair-wise combinations of training and validation datasets. we evaluate several alternatives for summarizing the pairwise validation statistics, and compare these to conventional cross-validation. results: our data-driven simulations and our application to survival prediction with eight breast cancer microarray datasets, suggest that standard cross-validation produces inflated discrimination accuracy for all algorithms considered, when compared to cross-study validation. furthermore, the ranking of learning algorithms differs, suggesting that algorithms performing best in cross-validation may be suboptimal when evaluated through independent validation. availability: the survhd: survival in high dimensions package (http:// www.bitbucket.org/lwaldron/survhd) will be made available through bioconductor. contact:cross-validation and related resampling methods are de facto standard for ranking supervised learning algorithms. they allow estimation of prediction accuracy using subsets of data that have not been used to train the algorithms. this avoids over-optimistic accuracy estimates caused by re-substitution.this characteristic has been carefully discussed in. it is common to evaluate algorithms by estimating prediction accuracy via cross-validation for several datasets, with results summarized across datasets to rank algorithms . this approach recognizes possible variations in the relative performances of learning algorithms across studies or fields of application. however, it is not fully consistent with the ultimate goal, in the development of models with biomedical applications, of providing accurate predictions for fully independent samples, originating from institutions and processed by laboratories that did not generate the training datasets. it has been observed that accuracy estimates of genomic prediction models based on independent validation data are often substantially inferior to cross-validation estimates . in some cases this has been attributed to incorrect application of cross-validation; however even strictly performed crossvalidation may not avoid over-optimism resulting from potentially unknown sources of heterogeneity across datasets. these include differences in design, acquisition and ascertainment strategies , hidden biases, technologies used for measurements, and populations studied. in addition, many genomics studies are affected by experimental batch effects . quantifying these heterogeneities and describing their impact on the performance of prediction algorithms is critical in the practical implementation of personalized medicine procedures that use genomic information. there are potentially conflicting, but valid, perspectives on what constitutes a good learning algorithm. the first perspective is that a good learning algorithm should perform well when trained and applied to a single population and experimental setting, but it is not expected to perform well when the resulting model is applied to different populations and settings. we call such an algorithm specialist, in the sense that it can adapt and specialize to the population at hand. this is the mainstream perspective for assessing prediction algorithms and is consistent with validation procedures performed within studies . however, we argue that it does not reflect the reality that samples of convenience and uncontrolled specimen collection are the norm in genomic biomarker studies . we promote another perspective: a good learning algorithm should be generalist, in the sense that it yields models that may be suboptimal for the training population, or not fully representative of the dataset at hand, but that perform reasonably well to whom correspondence should be addressed.  
