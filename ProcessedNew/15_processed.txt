systems biology a bayesian approach to targeted experiment design motivation: systems biology employs mathematical modelling to further our understanding of biochemical pathways. since the amount of experimental data on which the models are parameterized is often limited, these models exhibit large uncertainty in both parameters and predictions. statistical methods can be used to select experiments that will reduce such uncertainty in an optimal manner. however, existing methods for optimal experiment design (oed) rely on assumptions that are inappropriate when data are scarce considering model complexity. results: we have developed a novel method to perform oed for models that cope with large parameter uncertainty. we employ a bayesian approach involving importance sampling of the posterior predictive distribution to predict the efficacy of a new measurement at reducing the uncertainty of a selected prediction. we demonstrate the method by applying it to a case where we show that specific combinations of experiments result in more precise predictions. availability and implementation: source code is available at:computational models can be used to predict (un)measured behaviour or system responses and formalize hypotheses in a testable manner. to be able to make predictions parameters are required. despite the development of new quantitative experimental techniques, data are often relatively scarce. consequently the modeller is faced with a situation where large regions of parameter space can describe the measured data to an acceptable degree (;). this is not a problem when the predictions required for testing the hypothesis (which we shall refer to as predictions of interest) are well constrained . when this is not the case more data will be required. optimal experiment design (oed) methods can be used to determine which experiments would be most useful in order to perform statistical inference. classical design criteria are often based on linearization around a best fit parameter set and to whom correspondence should be addressed. pertain to effectively constraining the parameters or predictions . however, when data are scarce considering the model complexity or the model is strongly non-linear, such methods are not appropriate . this makes investigating the role of parameter uncertainty in oed a relevant topic to explore. we propose a method for experimental design that overcomes these issues by adopting a probabilistic approach which incorporates prediction uncertainty. our method enables the modeller to target experimental efforts in order to selectively reduce the uncertainty of predictions of interest. using our approach, multiple experiments can be designed simultaneously revealing potential benefits that might arise from specific combinations of experiments. we focus on biochemical networks that can be modelled using a system of ordinary differential equations. these models comprise of equations f ( x(t), u(t), p) which contain parameters p (constant in time), inputs u(t) and state variables x(t). given a set of parameters, inputs and initial conditions x(0) these equations can subsequently be simulated. measurements y(t) are performed on a subset and/or a combination of the total number of states in the model. measurements are hampered by measurement noise while many techniques used in biology (e.g. western blotting) necessitate the use of scaling and offset parameters q . we define as = p, q, x 0 , which lists all the required variables to simulate the model.in order to perform inference and experiment design an error model is required. for ease of notation we shall demonstrate our method using a gaussian error model. if we consider m time series of length n 1 , n 2 , ..., n m hampered by such noise, we obtain equation (4) for the probability density function of the output data. here y t is the true system with true parameters t , where i,j indicates the sd of a specific data point and k serves as a normalization constant.using bayes theorem, we obtain an expression for the posterior probability distribution over the parameters . the posterior probability distribution is given by normalizing the author(s) 2012. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/ by-nc/3.0), which permits unrestricted non-commercial use, distribution, and reproduction in any medium, provided the original work is properly cited.to illustrate our method, we apply it to a model of the jak-stat signalling pathway . the model is based on a number of hypothesized steps (see). the first reaction describes the activation of the erythropoietin receptor which subsequently phosphorylates cytoplasmic stat (x 1 ). then phosphorylated stat (x 2 ) dimerises (x 3 ) and is imported into the nucleus (x 4 ). here dissociation and dephosphorylation occurs which are associated with a time delay. similar to the implementation given in the original paper, the driving input function was approximated by a spline interpolant, while the delay was approximated using a linear chain approximation (x 5 ,...,x 13 ).in order to infer the posterior distribution data from the paper by swameye et al.both reported in arbitrary units (which necessitates two scaling parameters). the initial cytoplasmic concentration of stat is unknown while all other forms of stat are assumed zero at the start of the simulation. given the data, not all parameters are identifiable . we used uniform priors in logspace for the kinetic parameters and a gaussian ( = 200 nm, = 20 nm) for the initial condition. parameter two was bounded between ranges, since this parameter was non-identifiable from the data . we simulated two chains starting at different initial values up to one million parameter sets and assessed convergence by visually inspecting differences between batches of samples. the uncertainty in model parameters propagates as an uncertainty in the predicted responses of the state variables. ppds were simulated for all states as well as the summations of states already measured. to simulate the ppds the chain of parameter sets was thinned to 10 000 samples using equidistant thinning. since the error model in this case is additive gaussian noise, there is no need to explicitly simulate measurement noise. this can be taken into account by multiplying the sd of the measurement by 2 (see supplementary materials for more information). an example is shown inrevealing the relation between two predictions at different time points. for a complete overview of the ppds for all states, see the supplementary materials. the relation between the ppds of different states was explored. this relation between two states at the indicated time points is shown in more detail in both scatter plot and 2d histogram form in. the former shows the actual samples from the ppd for one point in time. here each dot represents a simulated value for one parameter set from the mcmc chain. as shown in the figure, these different states are often non-linearly related at specific points in time. the associated 2d histogram corresponds to the same information interpreted as probability density. considering state 3 as observable and state 4 as prediction, while assuming a measurement accuracy of = 10/ 2 for x 3 , it can be observed that a significant decrease in variance can be attained during the rise of state 3. measuring state 3 at the peak value however, results in a smaller variance reduction. a few things can be observed. in order for the measurement to be useful, there should be a correlation between the measurement and the prediction of interest. additionally, the uncertainty in both should be large enough. since all predictions of state 3 start with an initial condition of zero, this implies that the uncertainty at this point is low. therefore, an additional measurement at t = 0 would not yield appreciable variance reduction which is also reflected by the fact that the svr starts at a value of zero. in order to demonstrate the flexibility of our method it was decided to perform oed for a quantity that depends on the model predictions in a highly non-linear fashion, namely the time to peak for the concentration of dimerized stat in the nucleus (state 4). the time to peak was computed for the state 4 prediction for each parameter set from the posterior parameter distribution. we assumed that all states except state 4 are measurable with an accuracy of = 10/ 2. as potential measurements we also included the two sums of states as measured in earlier experiments. the experiment space was sampled using a monte carlo approach, uniformly sampling the experiment space. this sampling is shown inwhere the svr is shown for several combinations of two measurements. in this figure each axis corresponds to a potential measurement. different model outputs (potential measurements) are separated using grid lines, while. the relation between the two states at the indicated time points is shown in both scatter plot and 2d histogram form. the former shows the actual samples from the ppd for one point in time. here the dots represent simulated values belonging to different parameter sets from the mcmc chain. in the histogram the colour indicates the number of samples in a particular region which is proportional to the probability density.where the different model outputs are numbered. numbers 1 to 3 correspond to the first three states whereas 4 and 5 correspond to the sums of states on which the original ppd was parametrized. note that each block on each axis corresponds to an entire time series. the block corresponding to experiments involving state 1 is shown enlarged in (b). variance reduction is computed using the importance sampling method. the interval between each pair of lines corresponds to an entire time series. the colour value indicates the svr for that specific experiment. recall that the original dataset contained measurements of two sums of model states. these two observables correspond to outputs 5 and 6 in, which indicates that additional measurements on these would provide very little additional variance reduction. interestingly, performing the experimental design for two measurements revealed that the largest reduction in variance could be obtained by measuring state one at an early and late time point. this result underlines the benefit of being able to combine multiple measurements in the oed. furthermore, the analysis clearly revealed that the timing of this first time point is crucial. however, if accurate timing is not possible in the experiment one could consider measuring state three and one instead. here smaller reductions are attained but the timing accuracy required for a a b. comparison of two methods for calculating the variance reduction. variance reduction of the peak time of dimerized stat (x 4 ) with respect to two new measurements. (a) lvr. (b) difference between the variance reduction computed by means of lvr and importance sampling (shown in). reasonable reduction is less stringent. additionally, we investigated how the bounds of the priors on the non-identifiable kinetic rates affected our experimental design by widening them. this revealed that the evrs obtained when measuring state 2 or 3 in combination with state 1 were more robust (for more information see the supplementary materials). since both error models in this case are gaussian, the same analysis can be performed using the lvr (which for t = 1000 samples is about 100-fold faster). the resulting sampling is shown in. qualitatively, the results agree well with those inrevealing its applicability as an initial sampling step. information gained from an initial lvr sweep can subsequently be used to sample only relevant regions of the experiment design space. another example can be found in the supplementary materials.  
