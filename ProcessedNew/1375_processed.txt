data and text mining deviance residuals-based sparse pls and sparse kernel pls regression for censored data motivation: a vast literature from the past decade is devoted to relating gene profiles and subject survival or time to cancer recurrence. biomarker discovery from high-dimensional data, such as transcrip-tomic or single nucleotide polymorphism profiles, is a major challenge in the search for more precise diagnoses. the proportional hazard regression model suggested by cox (1972), to study the relationship between the time to event and a set of covariates in the presence of censoring is the most commonly used model for the analysis of survival data. however, like multivariate regression, it supposes that more observations than variables, complete data, and not strongly correlated variables are available. in practice, when dealing with high-dimensional data, these constraints are crippling. collinearity gives rise to issues of over-fitting and model misidentification. variable selection can improve the estimation accuracy by effectively identifying the subset of relevant predictors and enhance the model interpretability with parsimonious representation. to deal with both collinearity and variable selection issues, many methods based on least absolute shrinkage and selection operator penalized cox proportional hazards have been proposed since the reference paper of tibshirani. regularization could also be performed using dimension reduction as is the case with partial least squares (pls) regression. we propose two original algorithms named splsdr and its non-linear kernel counterpart dksplsdr, by using sparse pls regression (spls) based on deviance residuals. we compared their predicting performance with state-of-the-art algorithms on both simulated and real reference benchmark datasets. results: splsdr and dksplsdr compare favorably with other methods in their computational time, prediction and selectivity, as indicated by results based on benchmark datasets. moreover, in the framework of pls regression, they feature other useful tools, including biplots representation, or the ability to deal with missing data. therefore, we view them as a useful addition to the toolbox of estimation and prediction methods for the widely used coxs model in the high-dimensional and low-sample size settings. availability and implementation: the r-package plsrcox is available on the cran and is maintained by fr ed eric bertrand. http://cran.r-project.org/web/packages/plsrcox/index.html.overall, dksplsdr and, even more, splsdr compare favorably with the benchmark methods on both simulated and real datasets.in the simulation study, splsdr turned out to be the best method to recover the linear link according to the issw performance measure and for the three simulation schemes (, linear link row panel). more generally in that setting, the models featuring components had better performance measures than the lasso and elastic net-based ones. similar results can be observed for the iaucsurvroc criterion (supplementary, linear link row panel) with lesser advantage to splsdr and dksplsdr. in both cases, simulations study show that neither splsdr or dksplsdr tend to wrongly recover a link between the response and the explanatory variable when there is none (supplementary figs s1 and s8, no link row panel), whereas lasso and elastic net-based methods do for the factorial simulation scheme and the iaucsurvroc criterion (supplementary, no link row panel). whatever the real dataset, the overall patterns of the splsdr and dksplsdr algorithms follow the general patterns of predictability of the benchmark methods, e.g. a low increase in predictability for the metzeler dataset or a global step-wise decrease on the romain datasets. these patterns suggest that the performances of the different methods may depend on the real but unknown data dimension. the splsdr or dksplsdr almost always rank among the 1 to 4 best methods with higher predictability, often being even 1st or 2nd, both on short and long term predictions (see tables 2 and 3). this is particularly true in cases where a large predictability heterogeneity is to be noted among the benchmark algorithms, such as for the garber and the wang datasets. last but not least, splsdr and dksplsdr not only automatically handle missing data, the study of the robustness of these two algorithms to the amount and type of missing databeing beyond the scope of this article, but also provide nice data exploration tools such as biplots representation of individuals and descriptors, by projecting the dataset on the first spls components (see). in a word, we view splsdr and dksplsdr as a useful addition to the toolbox of estimation and prediction methods for the widely used coxs model in the high-dimensional and low-sample size settings.  
