hi-corrector: a fast, scalable and memory-efficient package for normalizing large-scale hi-c data genome-wide proximity ligation assays, e.g. hi-c and its variant tcc, have recently become important tools to study spatial genome organization. removing biases from chromatin contact matrices generated by such techniques is a critical preprocessing step of subsequent analyses. the continuing decline of sequencing costs has led to an ever-improving resolution of the hi-c data, resulting in very large matrices of chromatin contacts. such large-size matrices, however, pose a great challenge on the memory usage and speed of its normalization. therefore, there is an urgent need for fast and memory-efficient methods for normalization of hi-c data. we developed hi-corrector, an easy-to-use, open source implementation of the hi-c data normalization algorithm. its salient features are (i) scalabilitythe software is capable of normalizing hi-c data of any size in reasonable times; (ii) memory efficiencythe sequential version can run on any single computer with very limited memory, no matter how little; (iii) fast speedthe parallel version can run very fast on multiple computing nodes with limited local memory. availability and implementation: the sequential version is implemented in ansi c and can be easily compiled on any system; the parallel version is implemented in ansi c with the mpi library (a standardized and portable parallel environment designed for solving large-scale scientific problems). the package is freely available atthe recent development of genome-wide proximity ligation assays such as hi-c and its variant tcc has significantly facilitated the study of spatial genome organization. the raw chromatin interaction data obtained by hi-c methods can have both technical and biological biases . therefore, correcting biases in the hi-c data is an important preprocessing step. among several recently developed methods , the iterative correction (abbreviated as ic) algorithm has been used most widely by recent studies due to its conceptual simplicity, parameter-free algorithm and ability to account for unknown biases, although its assumption of the equal visibility across all loci may require further exploration. mathematically, the ic algorithm is a matrix scaling or balancing method that transforms a symmetric matrix into one that is doubly stochastic, meaning that the row and column sums of the matrix are equal to 1. however, the hi-c chromatin interaction matrix is of the massive size o(n 2 ), where n is the number of genomicwe compared three algorithms (ic, ic-mes and ic-mep) on the tcc/hi-c data of two human cell types: gm12878 and hesc . the whole genome is partitioned into the equal-size regions (or bins); the bin size is the mainall algorithms were terminated after 10 iterations for the purpose of performance comparison, since each iteration has almost the same running time. memory includes only the memory allocated for computation in each processor, not system overhead. the elapsed time format is hours : minutes : seconds.fast, scalable and memory-efficient normalization for hi-c dataindicator of hi-c data resolution. the results are listed in. in the experiment with 20k bp resolution data, the basic ic algorithm requires a minimum memory of 86 gb. the algorithm icmes can run with just 4 gb memory (a common memory configuration in office computers) and complete the same work in reasonable time (within 4 h). ic-mep can dramatically speed up the computation using more processors (about 6 min with 48 processors), while using only 1 gb of memory in each processor. for the 10k bp data, none of hpc computer nodes (with 128 gb memory limit) can load the full matrix (about 343 gb) for the basic ic algorithm. but ic-mes and ic-mep can use 2 gb memory to quickly get the results (even in half hour using 48 processors). details are provided in the supplemental materials.with the rapidly increasing resolution of hi-c datasets, the size of the chromatin contact map will soon exceed the memory capacity of general computers. we developed hi-corrector, a scalable and memory-efficient package for bias removal in hic data. hicorrector can run on any single computer or a computer cluster with limited memory size to complete the task. we performed experiments on high-resolution hic data from two human cell types to show that the package can process very large data sets in reasonable time using the single processor, and in very short time with multiple processors. the experiments further demonstrate the scalability of our package with the observation shown inthat the more processors used, the faster it is. therefore, hi-corrector is a timely resource addressing the challenge of normalizing high-resolution hi-c data.  
