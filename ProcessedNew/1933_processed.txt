genome analysis gatb: genome assembly & analysis tool box motivation: efficient and fast next-generation sequencing (ngs) algorithms are essential to analyze the terabytes of data generated by the ngs machines. a serious bottleneck can be the design of such algorithms , as they require sophisticated data structures and advanced hardware implementation. results: we propose an open-source library dedicated to genome assembly and analysis to fasten the process of developing efficient software. the library is based on a recent optimized de-bruijn graph implementation allowing complex genomes to be processed on desk-top computers using fast algorithms with low memory footprints.the analysis of next-generation sequencing (ngs) data remains a time-and space-consuming task. many efforts have been made to provide efficient data structures for indexing the terabytes of data generated by the fast sequencing machines (suffix array, burrowswheeler transform, bloom filter, etc.). genome assemblers such as velvet , abyss , soapdenovo2 , spades or mappers such as bwa or variant detection such as crac for instance make an intensive use of these data structures to keep their memory footprint as low as possible. at the same time, parallelism has been largely investigated to reduce execution time. many strategies such as gpu implementation , cloud deployment , algorithm vectorization , multithreading, etc., have demonstrated high potentiality on ngs processing. the overall efficiency of ngs software depends on a smart combination of data representation and use of the available processing units. developing such software is thus a real challenge, as it requires a large spectrum of competence from high-level data structure and algorithm concepts to tiny details of implementation. the gatb library aims to ease the design of ngs algorithms. it offers a panel of high-level optimized building blocks to speedup the development of ngs tools related to genome assembly and/or genome analysis. the underlying data structure is a memory efficient de-bruijn graph , and the general parallelism model is multithreading. the gatb library targets standard computing resources such as current multicore processor (laptop computer, small server) with a few gigabytes of memory. hence, from the high-level c+ + functions available in the gatb library, ngs programing designers can rapidly elaborate their own software based on state-of-the-art algorithms and data structures of the domain. based on the same idea, other bioinformatics libraries exist, from which domain-specific tools can be elaborated. the ngs++ library is specifically tailored for developing applications that work with genomic regions and features, such as epigenomics marks, gene features and data that are associated with bed type files. the seqan library is a general-purpose library targeting standard sequence processing. advanced data structures such as de-bruijn graphs are not included in seqan. khmer is a library and toolkit for doing k-mer-based ngs dataset analysis. as with gatb, most of khmer relies on an underlying probabilistic data structure (bloom filter). the khmer library can be used in various k-mer processing such as abundance filtering, error trimming, graph size filtering or partitioning.to demonstrate the efficiency of the gatb library, a few software implemented from gatb are briefly presented. the idea is to give a quick overview of the application spectrum of the gatb library and some performance numbers. minia is a short-read de-bruijn assembler capable of assembling large and complex genomes into contigs on a desktop computer. the assembler produces contigs of similar length and accuracy to other de-bruijn assemblers e.g. velvet . as an example, a boa constrictor constrictor (1.6 gb) dataset (illumina 2 120 bp reads, 125 coverage) from assemblathon 2 can be processed in $45 h and 3 gb of memory on a standard computer (3.4 ghz 8-core processor) using a single core, yielding a contig n50 of 3.6 kb (prior to scaffolding and gap-filling). bloocoo is a k-mer spectrum-based read error corrector, designed to correct large datasets with low memory footprints. it uses the disk streaming k-mer counting algorithm contained in the gatb library and inserts solid k-mers in a bloom filter. the correction procedure is similar to the musket multistage approach . bloocoo yields similar results while requiring far less memory: for example, it can correct whole human genome re-sequencing reads at 70 coverage with 54 gb of memory (see supplementary file 1 for extra information on bloocoo). discosnp aims to discover single nucleotide polymorphism from non-assembled reads and without a reference genome. from one or several datasets a global de-bruijn graph is constructed, then scanned to locate specific snp graph patterns . a coverage analysis on these particular locations can finally be performed to validate and assign scores to detected biological elements. applied on a mouse dataset (2.88 gb, 100 bp illumina reads), discosnp takes 34 h and requires 4.5 gb ram. in the same spirit, the takeabreak software discovers inversion variants from non-assembled reads. it directly finds particular patterns in the de-bruijn graph and provides execution performances similar to discosnp .  
