sequence analysis turtle: identifying frequent k-mers with cache-efficient algorithms motivation: counting the frequencies of k-mers in read libraries is often a first step in the analysis of high-throughput sequencing data. infrequent k-mers are assumed to be a result of sequencing errors. the frequent k-mers constitute a reduced but error-free representation of the experiment, which can inform read error correction or serve as the input to de novo assembly methods. ideally, the memory requirement for counting should be linear in the number of frequent k-mers and not in the, typically much larger, total number of k-mers in the read library. results: we present a novel method that balances time, space and accuracy requirements to efficiently extract frequent k-mers even for high-coverage libraries and large genomes such as human. our method is designed to minimize cache misses in a cache-efficient manner by using a pattern-blocked bloom filter to remove infrequent k-mers from consideration in combination with a novel sort-and-compact scheme, instead of a hash, for the actual counting. although this increases theoretical complexity, the savings in cache misses reduce the empirical running times. a variant of method can resort to a counting bloom filter for even larger savings in memory at the expense of false-negative rates in addition to the false-positive rates common to all bloom filter-based approaches. a comparison with the state-of-the-art shows reduced memory requirements and running times. availability and implementation: the tools are freely available for download atk-mers play an important role in many methods in bioinformatics because they are at the core of the de bruijn graph structure that underlies many of todays popular de novo assemblers . they are also used in assemblers based on the overlaplayout-consensus paradigm like celera and arachne as seeds to find overlap between reads. several read correction tools use k-mer frequencies for error correction. their main motivation for counting k-mers is to filter out or correct sequencing errors by relying on k-mers that appear multiple times and can thus be assumed to reflect the true sequence of the donor genome. in contrast, k-mers that appear only once are assumed to contain sequencing errors. melsted and pritchard (2011) and marcais and kingsford (2011) make a more detailed compelling argument about the importance of k-mer counting. in a genome of size g, we expect up to g unique k-mers. this number can be smaller because of repeated regions (which produce the same k-mers) and small k, as smaller k-mers are less likely to be unique, but is usually close to g for reasonable values of k. however, depending on the amount of sequencing errors, the total number of k-mers in the read library can be substantially larger than g. for example, in the dm dataset , the total number of 31-mers is $289.20 m, whereas the number of 31-mers occurring at least twice is $131.82 m. the size of the genome is 122 mb (megabase pairs). this is not surprising because one base call error in a read can introduce up to k false kmers. consequently, counting the frequency of all k-mers, as done by jellyfish , which is limited to k 31, requires o(n) space where n is the number of k-mers in the read library. this makes the problem of k-mer frequency counting time and memory intensive for large read libraries like human. we encounter similar problems for large libraries while using khmer , which uses a bloom filter-based approach for counting frequencies of all k-mers. ideally, the frequent k-mer identifier should use o(n) space where n is the number of frequent k-mers (n ( n). the approach taken by bfcounter achieves something close to this optimum by ignoring the infrequent k-mers with a bloom filter and explicitly storing only frequent k-mers. this makes bfcounter more memory-efficient compared with jellyfish. however, the running time of bfcounter is large for two reasons. first, it is not multi-threaded. second, both the bloom filter and the hash table used for counting incur frequent cache misses. the latter has recently been identified as a major obstacle to achieving high performance on modern architectures, motivating the development of cache-oblivious algorithms and data structures , which optimize the cache behavior without relying on information of cache layout and sizes. additionally, bfcounter is also limited to a count range of 0255, which will often be exceeded in single-cell experiments because of the large local coverage produced by whole genome amplification artifacts. a different approach is taken by dsk to improve memory efficiency. dsk makes many passes over the read file and uses temporary disk space to trade off the memory requirement. althoughclaimed dsk to be faster than bfcounter, on our machine to whom correspondence should be addressed. using an 18 tb raid-6 storage system; dsk required more wallclock time compared with bfcounter. therefore, we consider dsk without dedicated high-performance disks, e.g. solid state, and bfcounter to be too slow for practical use on large datasets. a disk-based sorting and compaction approach is taken by kmc , which was published very recently, and it is capable of counting k-mers of large read libraries with a limited amount of memory. however, in our test environment, we found it to be slower than the method described here. we present a novel approach that reduces the memory footprint to accommodate large genomes and high-coverage libraries. one of our tools (scturtle) can report frequent 31mers with counts (with a very low false-positive rate) from a human read set with 135.3 gb using 109 gb of memory in 52 h using 19 worker threads. like bfcounter, our approach also uses a bloom filter to screen out k-mers with frequency one (with a small false-positive rate), but in contrast to bfcounter, we use a pattern-blocked bloom filter . the expected number of cache misses for each inquiry/update in such a bloom filter is one. the frequency of the remaining k-mers is counted with a novel sorting and compaction-based algorithm. our compaction step is similar to run-length encoding . note that this is similar to the strategy of kmc, which was developed as a concurrent and independent work. though the complexity of sorting in our compression step is on log n, it has sequential and localized memory access that helps in avoiding cache misses and will run faster than an o(n) algorithm that has o(n) cache misses as long as log n is much smaller than the penalty issued by a cache miss. for larger datasets, where o(n) space is not available, the aforementioned method will fail. we show that it is possible to get a reasonable approximate solution to this problem by accepting small false-positive and false-negative rates. the method is based on a counting bloom filter implementation. the error rates can be made arbitrarily small by making the bloom filter larger. because the count is not maintained in this method, it reports only the k-mers seen more than once (with a small false-positive and false-negative rate), but not their frequency. we call the first tool scturtle and the second one cturtle.identifying correct k-mers out of the k-mer spectrum of a read library is an important step in many methods in bioinformatics. usually, this distinction is made by the frequency of the k-mers. fast tools for counting k-mer frequencies exist, but for large read libraries, they may demand a significant amount of memory, which can make the problem computationally unsolvable on machines with moderate amounts of memory resource ( 128 gb or even with 256 gb for large datasets). simple memory-efficient methods, on the other hand, can be timeconsuming. unfortunately, there is no single tool that achieves a reasonable compromise between memory and time. here we present a set of tools that make some compromises and simultaneously achieve memory and time requirements that are matching the current state-of-the-art in both aspects. with our first tool (scturtle), we achieve memory efficiency by filtering k-mers of frequency one with a bloom filter. our pattern-blocked bloom filter implementation is more time-efficient compared with a regular bloom filter. we present a novel strategy based on sorting and compaction for storing frequent kmers and their counts. because of its sequential memory access pattern, our algorithm is cache-efficient and achieves good running time. however, because of the bloom filters, we incur a small false-positive rate. the second tool (cturtle) is designed to be more memory-efficient at the cost of giving up the frequency values and allowing both false-positive and false-negative rates. the implementationnote: the tools ran with fast mod and 31 worker threads. each reported number is an average of five runs.note: for the large datasets, because of memory constraints, the exact counts for all k-mers could not be obtained, and therefore, these rates could not be computed. is based on a counting bloom filter that keeps track of whether a k-mer was observed and whether it has been stored in external media. this tool does not report the frequency count of the kmers. both tools allow a k-mer size of up to 64. they also allow the user to decide how much memory should be consumed. of course, there is a minimum memory requirement for each dataset, and the amount of memory directly influences the running time and error rate. however, we believe, with the proper compromises, the approximate frequent k-mer extraction problem is now computationally feasible for large read libraries within reasonable wall-clock time using a moderate amount of memory.  
