bless: bloom filter-based error correction solution for high-throughput sequencing reads motivation: rapid advances in next-generation sequencing (ngs) technology have led to exponential increase in the amount of genomic information. however, ngs reads contain far more errors than data from traditional sequencing methods, and downstream genomic analysis results can be improved by correcting the errors. unfortunately, all the previous error correction methods required a large amount of memory, making it unsuitable to process reads from large genomes with commodity computers. results: we present a novel algorithm that produces accurate correction results with much less memory compared with previous solutions. the algorithm, named bloom-filter-based error correction solution for high-throughput sequencing reads (bless), uses a single minimum-sized bloom filter, and is also able to tolerate a higher false-positive rate, thus allowing us to correct errors with a 40 memory usage reduction on average compared with previous methods. meanwhile, bless can extend reads like dna assemblers to correct errors at the end of reads. evaluations using real and simulated reads showed that bless could generate more accurate results than existing solutions. after errors were corrected using bless, 69 of initially unaligned reads could be aligned correctly. additionally, de novo assembly results became 50 longer with 66 fewer assembly errors.recent advances in next-generation sequencing (ngs) technologies have made it possible to rapidly generate high-throughput data at a much lower cost than traditional sanger sequencing technology . ngs technologies enable cost-efficient genomic applications, including de novo assembly of many non-model organisms , identifying functional elements in genomes , and finding variations within a population . in addition to short read length, a main challenge in analyzing ngs data is its higher error rate than traditional sequencing technology , and it has been demonstrated that error correction can improve the quality of genome assembly and population genomics analysis . previous error correction methods can be divided into four major categories : (i) k-mer spectrum-based , (ii) suffix tree-/array-based (, b), (iii) multiple sequence alignment (msa)-based and (iv) hidden markov model (hmm)-based . however, none of these previous methods has successfully corrected errors in ngs reads from large genomes without consuming a large amount of memory that is not accessible to most researchers (see detailed discussions in the supplementary methods). previous evaluations showed that some error correction tools require4128 gigabyte (gb) of memory to correct errors in genomes with 120 mb, and the others need tens of gb of memory . for a human genome, previous approaches would need hundreds of gb of memory. even if a computer with hundreds of gb of memory is available, running such memoryhungry tools degrades the efficiency of the computer. while the error correction tool runs, we cannot do any other job using the computer if most of the memory is occupied by the error correction tool. this can be a critical problem for data centers, where a large amount of data should be processed in parallel. in several works, bloom filters or counting bloom filters were used to save a k-mer spectrum, which includes all the strings of length k (i.e. k-mers) that exist more than a certain number of times in reads (, b). although bloom filter is a memory-efficient data structure, the memory reduction by previous bloom filter-based methods did not reach their maximum potential because of the following four reasons: (i) the size of a bloom filter should be proportional to the number of distinct k-mers in reads, and the number of distinct k-mers was conservatively estimated, thus could be much higher than the actual number. (ii) they could not remove the effect of false positives from bloom filters. to make the false-positive rate of the bloom filters small, the size of bloom filters were made large. (iii) because they could not distinguish error-free k-mers from erroneous ones before a bloom filter was constructed, both of the k-mers needed to be saved in bloom filters. (iv) multiple bloom filters (or counting bloom filters) were needed to count the multiplicity of each k-mer. besides the large memory consumption of the existing methods, another problem encountered during the error correction process is that there exist many identical or very similar subsequences in a genome (i.e. repeats). because of these repeats, an erroneous subsequence can sometimes be converted to multiple error-free subsequences, making it difficult to determine the right choice. in this article, we present a new bloom filter-based error correction algorithm, called bless. bless belongs to the k-mer spectrum-based method, but it is designed to remove the aforementioned limitations that previous k-mer spectrum-based solutions had. our new approach has three important new features:(1) bless is designed to target high memory efficiency for error correction to be run on a commodity computer. the k-mers that exist more than a certain number of times in reads are sorted out and programmed into a bloom filter.(2) bless can handle repeats in genomes better than previous k-mer spectrum-based methods, which leads to higher accuracy. this is because bless is able to use longer k-mers compared with previous methods. longer k-mers resolve repeats better.(3) bless can extend reads to correct errors at the end of reads as accurately as other parts of the reads. sometimes an erroneous k-mer may be identified as an error-free one because of an irregularly large multiplicity of the k-mer. false positives from the bloom filter can also cause the same problem. bless extends the reads to find multiple k-mers that cover the erroneous bases at the end of the reads to improve error correction at the end of the reads.to identify erroneous k-mers in reads, we need to count the multiplicity of each k-mer. counting k-mers without extensive memory is challenging . bless uses the disk-based k-mer counting algorithm like disk streaming of k-mers (dsk) and k-mer counter (kmc) . however, bless needs to save only half of the k-mers that dsk does in hash tables, because it does not distinguish a kmer and its reverse complement. to evaluate the performance of bless, this study used real ngs reads generated with the illumina technology as well as simulated reads. these reads were corrected using bless as well as six previously published methods. our results show that the accuracy of bless is the best while it only consumes 2.5 of the memory usage of all the compared methods on average. our results further show that correcting errors using bless allowed us to align 69 of previously unaligned reads to the reference genome accurately. bless also increased ng50 of scaffolds by 50 and decreased assembly errors by 66 based on the results from velvet .to assess the performance of bless, we corrected errors in five different read sets from various genomes using bless and six other error correction methods. all the evaluations were done on a server with two intel xeon x5650 2.67 ghz processors, 24 gb of memory and scientific linux. the version and parameters of all tools used in the experiments can be found in the supplementary document.current ngs technologies produce errors in reads, which can influence the quality of downstream analysis. many methods have been developed to correct such sequencing errors. however, most previous methods cannot correct errors in large genomes. even if a few methods succeeded in correcting large genomes, their outputs still contain many uncorrected errors. in addition, the memory requirement for the existing tools has been still too large for most researchers, who might only have access to computers with a moderate amount of memory. in this work, we present a novel error correction algorithm for ngs reads, called bless, which has two novel features: (i) bless consumes much less memory than previous methods. bless can sort out minimum k-mers needed to correct errors, and program the k-mers in a minimum-sized bloom filter. this makes bless consume much less memory than any other error correction method including previous bloom filter-based ones. (ii) bless also generates more accurate results for reads from genomes with many short repeats. this is mainly because bless is not limited by the choices of the length of the k-mer (see software options in the supplementary document for details). while the maximum k is usually 2030 in other methods, bless is able to remove ambiguities in the error correction process by choosing large numbers for k without increasing memory usage. furthermore, bless is efficient at correcting errors close to the ends of the reads. bless corrects errors at the ends of the reads by extending the ends. bless was compared with previous top performers using real and simulated reads. the experimental results showed that bless generated more accurate results than previous algorithms while consuming only 2.5 of the memory usage of the compared methods on average. moreover, running bless improved the length and accuracy of de novo assembly results for all the three widely used assemblers, velvet, soapdenovo and sga. bless also made 69 of unaligned reads exactly aligned to reference sequences. as pointed out before, bless can choose large values for k. the only drawback to large k values is that the average multiplicity of such k-mers drops. if the average multiplicity of k-mers is too low, we cannot precisely distinguish erroneous k-mers using their multiplicity. nevertheless, it is important to note thatbecause the dna sequencing cost keeps dropping and the throughput keeps increasing, we expect to solve this problem by increasing the depth of reads. the memory usage of bless is proportional only to the number of solid k-mers. because the solid k-mers eventually represent the k-mers that exist in the genome sequence, the number of solid k-mers remains constant even as the number of input reads escalates. therefore, memory consumption of bless will not increase even when read depth increases as shown in supplementary table s1. there are two future avenues to pursue. first, although the runtime of bless is already competitive as shown in, supporting multiple threads will improve blesss runtime further. blesss wall-clock time decomposition for d4 is depicted in supplementary. most of the time is spent counting the number of distinct solid k-mers and correcting errors. the runtime of both processes can be improved through parallelization. in the counting step, k-mers are distributed into n files, and each file is processed in succession. this step can be parallelized without degrading memory usage. the error correction process of a read is independent of other reads, and therefore the error correction part can be easily parallelized. second, developing a method to automatically choose k may be added. recent work showed that a k-mer multiplicity histogram can be made in a short time by sampling reads and an optimal k value can be found using the histograms. this sampling-based approach will also work for bless. it will be helpful to reduce the runtime because it can prevent users from running bless multiple times with different k values.  
