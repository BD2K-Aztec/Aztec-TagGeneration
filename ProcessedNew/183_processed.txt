a high-throughput framework to detect synapses in electron microscopy images motivation: synaptic connections underlie learning and memory in the brain and are dynamically formed and eliminated during development and in response to stimuli. quantifying changes in overall density and strength of synapses is an important prerequisite for studying con-nectivity and plasticity in these cases or in diseased conditions. unfortunately, most techniques to detect such changes are either low-throughput (e.g. electrophysiology), prone to error and difficult to automate (e.g. standard electron microscopy) or too coarse (e.g. magnetic resonance imaging) to provide accurate and large-scale measurements. results: to facilitate high-throughput analyses, we used a 50-year-old experimental technique to selectively stain for synapses in electron microscopy images, and we developed a machine-learning framework to automatically detect synapses in these images. to validate our method, we experimentally imaged brain tissue of the somatosensory cortex in six mice. we detected thousands of synapses in these images and demonstrate the accuracy of our approach using cross-validation with manually labeled data and by comparing against existing algorithms and against tools that process standard electron mi-croscopy images. we also used a semi-supervised algorithm that leverages unlabeled data to overcome sample heterogeneity and improve performance. our algorithms are highly efficient and scalable and are freely available for others to use.the mammalian brain can contain hundreds of millions of neurons, each with thousands of specialized connections called synapses that enable indirect communication between cells. estimates for the number of synapses in the mammalian brain ranges into the trillions. synapses are essential for the transfer of information across neuronal ensembles, and individual synapses can be modulated by patterns of incoming neural activity, a phenomenon thought to underlie learning and memory. changes in the relative strength and number of synapses can be regulated by a myriad of factors, including developmental age , sensory experience , drug addiction , estrus cycle and brain pathology. for example, in a form of autism linked to mutation of the fragile x gene, spine density in the neocortex is elevated , a feature that has also been observed in mice carrying the same genetic mutation . rett syndrome, another neurodevelopmental disorder, is characterized by smaller brain size caused by deficits in synaptogenesis that results in fewer spines. similarly, in alzheimers disease and other dementias, cognitive deficits are associated with reduced synapse density in the hippocampus, a brain structure critical for learning . understanding how connectivity across neurons can change is thus an important question that drives contemporary neuroscience research. because synapse distribution is a useful and diagnostic criterion to evaluate circuit function in learning and disease, there have been a variety of methods used to estimate synaptic connectivity or overall synapse numbers. electrophysiological methods to estimate connectivity and the number of inputs per cell can be informative (e.g.), but these approaches are low-throughput and can typically only capture tens or hundreds of connections in reasonable amounts of time . mri-based techniques can be used to study network function at the level of brain regions or voxels, but they do not provide enough spatial resolution to estimate neural connectivity . anatomically, synapse densities are measured via light-microscopy to identify specialized substructures called spines that stud the dendrites of neurons or using electron microscopy (em) to identify ultrastructural features that correspond to pre-and post-synaptic elements. traditional approaches have used cumbersome manual detection to count synapses in these images (e.g.) and were thus constrained to small-scale measurements or required the use of specialized transgenic animals limiting their usage for studying plasticity and development in wild-type mice. since the early 1990s, bioimage informatics has emerged as an important area in the analysis of biological images . imaging datasets are usually much larger than other highthroughput biological datasets (e.g. confocal microscopy data can range in the hundreds of gigabytes for a single imaging session). accurately identifying elements of interest (molecules, cells, synapses, etc.) within these massive datasets requires the development of sophisticated and efficient computational models. this often involves a classification-based strategy in which a (small) manually labeled training set is used to learn a general model that can be used to analyze a larger collection of images automatically. the key computational challenges involve the reliability and speed at which the analysis is done, as well as dealing with the heterogeneity of biological structures and noise to whom correspondence should be addressed. the author 2013. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. for commercial re-use, please contact journals.permissions@oup.com present in each image. electron microscopy data suffer particularly from these problems and often contain undesired variation in intensity and contrast within and across samples and preparations. this presents a major computational challenge because model parameters learned from one sample may not generalize to other samples. although reconstruction and segmentation of conventional em images has helped answer important questions about brain structure and function , these approaches have yet to reach the point of full automation , which has limited their scale and accuracy .  
