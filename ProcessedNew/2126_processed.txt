genetics and population analysis greater power and computational efficiency for kernel-based association testing of sets of genetic variants motivation: set-based variance component tests have been identified as a way to increase power in association studies by aggregating weak individual effects. however, the choice of test statistic has been largely ignored even though it may play an important role in obtaining optimal power. we compared a standard statistical test a score testwith a recently developed likelihood ratio (lr) test. further, when correction for hidden structure is needed, or gene gene interactions are sought, state-of-the art algorithms for both the score and lr tests can be computationally impractical. thus we develop new computationally efficient methods. results: after reviewing theoretical differences in performance between the score and lr tests, we find empirically on real data that the lr test generally has more power. in particular, on 15 of 17 real datasets, the lr test yielded at least as many associations as the score testup to 23 more associationswhereas the score test yielded at most one more association than the lr test in the two remaining datasets. on synthetic data, we find that the lr test yielded up to 12 more associations, consistent with our results on real data, but also observe a regime of extremely small signal where the score test yielded up to 25 more associations than the lr test, consistent with theory. finally, our computational speedups now enable (i) efficient lr testing when the background kernel is full rank, and (ii) efficient score testing when the background kernel changes with each test, as for genegene interaction tests. the latter yielded a factor of 2000 speedup on a cohort of size 13 500. availability: software available atwith next-generation sequencing data from larger and larger cohorts now being collected, the possibility of detecting even weaker genetic associations with disease is increasing. such weak signal could provide invaluable insights into biological and disease mechanisms, as well as yield biomarkers for diagnosis and personalized treatment. however, even with large datasets becoming available, studies to detect important genetic signal remain underpowered, especially those rare variantsthe most underpowered type of association whose signal lies in tests. one approach to help alleviate this power problem is to group together genetic markers and then to test them jointly in a single test. such an approach helps increase power in two ways. first, it can reduce the number of tests performed and hence the multiple testing penalty incurred. second, the test aggregates weak signal within a set, and can also tag unmarked variants. although a variety of competing methods for set tests have been proposed , some of the most influential and widely used methods are those that use a sequence-based kernel in a variance component model . improving power in these kernel-based models is the focus of this article. in particular, the main contribution of this article is improving power in two ways:(1) the statistical test used, showing that the non-standard likelihood ratio (lr) test in this setting can yield substantially more associations, and(2) several exact algebraic reformulations that yield dramatic improvements in runtime for certain classes of set tests, enabling far larger datasets to be analyzed. for example, on data from the wellcome trust case control consortium (wtccc), a genegene interaction scoretest speedup achieved running time $2000 times faster than na ve computation of the test.in the statistical genetics literature to date, practically no consideration has been given to the choice of statistical test for kernel-based set association tests. in particular, the choice of (frequentist) statistical test in this setting has uniformly been the score test , with the one exception being our recent work on how to conduct set tests in the presence of confounders, where an lr approach was used .also used an to whom correspondence should be addressed. the author 2014. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. approximate bayes factor as a complement to the use of a score test. from a purely computational perspective, use of the score test would seem more convenient and efficient, as it requires parameter estimation only for the null model, whereas the lr test requires parameter estimation for both the null and the alternative model. in terms of power, various theoretical results claim the superiority of either the lr test or the score test, under different conditions. however, because these conditions are rarely met for real data, and because it is unclear how robust the theoretical results are to deviations from the required conditions, there is no clear theoretical guidance on which test to choose in practice. therefore, here we conducted a systematic comparison between the two tests, using synthetic data, rare and common variants, with both case-control and continuousvalued phenotypes, and under various types of model misspecification. in so doing, we were able to assess the relative performance of the score and lr tests across a wide variety of settings when the ground truth was known. finally, we applied the two tests to real data for 17 phenotypes to determine which of our synthetic settings were most likely applicable, finding that, overall, the lr test performed substantially better than the score test. in addition to our systematic comparison of the score and lr tests in the standard setting, we also consider richer scenarios in which, for example, one may want to correct for confounding factors arising from family relatedness or population structure , or testing for genegene interactions between variants from pairs of sets (e.g. genes) . in such settings, there are two variance components one consisting of a background kernel (e.g. to correct for confounding factors, or for main effects in a test for genegene interactions) and an additional component in the alternative model built from the set of interest. when the null model includes a background kernel, the time to run tests can be prohibitive. in particular, for the case where (i) the background kernel has full rank (as has been done traditionally when correcting for confounding factors), and (ii) where the background kernel is low rank but changing with each test (as in testing for gene gene interactions), runtimes of state-of-the art lr and score tests can be dramatically decreased. thus, we developed computational improvements and demonstrate their effectiveness through timing experiments.  
