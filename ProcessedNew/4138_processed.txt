scalable metagenomic taxonomy classification using a reference genome database motivation: deep metagenomic sequencing of biological samples has the potential to recover otherwise difficult-to-detect microorganisms and accurately characterize biological samples with limited prior knowledge of sample contents. existing metagenomic taxonomic classification algorithms, however, do not scale well to analyze large metagenomic datasets, and balancing classification accuracy with computational efficiency presents a fundamental challenge. results: a method is presented to shift computational costs to an off-line computation by creating a taxonomy/genome index that supports scalable metagenomic classification. scalable performance is demonstrated on real and simulated data to show accurate classification in the presence of novel organisms on samples that include viruses, prokaryotes, fungi and protists. taxonomic classification of the previously published 150 giga-base tyrolean iceman dataset was found to take 520 h on a single node 40 core large memory machine and provide new insights on the metagenomic contents of the sample. availability: software was implemented in c and is freely available at http://sourceforge.net/projects/lmatmetagenomics is a powerful tool for assessing the functional and taxonomic contents in biological samples. early shotgun metagenomics projects used giga-bases of genetic data to demonstrate accurate sample surveys with less bias than previous methods. the potential to detect even lower abundance organisms and provide more accurate surveys across a broad spectrum of biological environments is being advanced now by sequencers reported to generate up to 1.3 mega-bases per second (calculated by dividing total base output by total number of sequencer hours run for the hiseq 2500 rapidrun mode. excludes library and sample preparation time). increased sequencing throughput presents a major scaling challenge to existing shotgun metagenomic classification algorithms . the ability for an algorithm to scale can be measured by the difference between sample classification run time and sequencer run time and assumes sufficient computing resources for each sequencer run. scaling is being addressed through the use of larger compute clusters, which can be managed by a third party service (cloud computing) . as sequencer use grows, however, algorithms that run on a single node and scale with sequencer output could be paired with individual sequencers and eliminate the need for high bandwidth network connections, which are not always available. in this article, we attempt to meet the scaling goal, running fast and accurate taxonomic sample classification on a single compute node to match analysis throughput with sequencer output. two major design choices were made, which present possible limitations: (i) a larger than typically used single address space memory resource is exploited (0.51 terabytes) and (ii) larger search seeds are used than default sensitive blast settings for matching reads to a reference database. relaxing conventional memory constraints allows a reference genome database to be annotated with taxonomic information and indexed to support fast metagenomic taxonomy classification of every sequencer read for all microbial taxa, including virus, prokaryotes, fungi and protists. decreasing memory costs make this approach accessible to many practitioners because the cost of a single large memory compute node remains a fraction of the initial sequencer cost and need not require specialized system administration expertise. large search seed sizes can potentially limit the ability to detect novel organisms but nonetheless is proving to be more effective, as the number of microbes with representative reference genomes grows for environments like the human microbiome . our goal is to efficiently assign taxonomic labels to the reads down to the species level for reads with reference representation and maintain accuracy in the presence of novel organisms by avoiding overly specific (e.g. species and strain) taxonomic assignments. this alleviates the computational bottleneck by limiting the number of unlabeled reads subjected to additional computational interrogation. the results show comparable or better accuracy than existing methods, and even with novel genomes in a sample, accurate and scalable classification is obtained in the vast majority of cases. the methods are made available as an open source software package, livermore metagenomics analysis toolkit (lmat). to whom correspondence should be addressed. the author(s) 2013. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. existing bioinformatic approaches address scalability in three ways: query size reduction, reference database size reduction and faster database search. query size reduction is achieved with metagenomic assembly and clustering, which merges overlapping and redundant reads into longer contiguous genomic segments . metagenomic assembly improves the strength of the taxonomic signal contained in individual short reads but careful parameter settings are required to avoid mis-assembly, and assembly costs could remain high . reference database size reduction is achieved through the use of genetic markers storing only the more informative sequences . marker-based approaches offer efficient summarization of metagenomic contents, but only cover a portion of the query set, leaving novel and other informative reads buried within the larger pool of unclassified reads, which could require additional examination . a less lossy approach reduces sequence redundancy by storing only the genetic differences among reference genomes. this approach was shown to speed up blast and blat genome database searches . faster database search methods apply larger search seeds, and examples include blat , bwa and other read mapping tools , but analyzing the search results remains a challenge with some approaches selecting the lowest common ancestor (lca) of multiple matches and others using variants of a best match selection procedure to improve rank specificity of the reported taxonomic label. moreover, parameter settings of the search tools can dramatically alter the outcome of the reported label and must be considered carefully . our approach uses faster search using larger seeds (k-mers) with a non-redundant search of taxonomic identifiers associated with the k-mers found in the reference genome database. our kmer/taxonomy database supports efficient retrieval of detailed taxonomic information and allows for an exhaustive comparison between competing taxonomic assignments using a novel rankflexible classification procedure. our new classification algorithm invokes variants of lca and best match selection depending on the context of the search results. the approach differs from compositional binning methods , as it uses larger values for k (1720) and, unlike alignment search, each k-mer is mapped to the individual source genomes minus the genome position. the method compensates for the lack of positional information by resolving the multiple k-mer/taxonomy associations recovered during search to assign each read the most rank-specific taxonomy identifier possible.lmat leverages large single address space memory to efficiently and accurately assign taxonomic labels to individual reads in large metagenomic datasets even in the presence of novel organisms. although the classification method is highly automated, attention to three parameters should be highlighted: minimum read label score, minimum difference between the best selected read label score and the competing alternatives and maximum number of taxonomic labels retrieved per k-mer. although the first two parameter settings were preset early in the development stage, application to the tyrolean iceman dataset required revisiting these parameters. the data featured especially short reads (as short as 25 nt), degraded dna, likely leading to lower quality scores and higher error rates and substantial human contamination with respect to analyzing the microbial contents. as a result, the default read label threshold needed to be lowered from 1 to 0 to avoid ignoring a larger fraction of the reads. interestingly, 25 nt length reads classified as human were assigned a read label score of 0 but could still be classified as human when no competing alternatives were found. however, if the sample came from a truly unknown source, the 25 nt reads with read label scores near 0 would require additional validation. the minimum difference threshold also needed to be increased when a large percentage of reads were initially classified as toxoplasma gondii. on further examination, the matches identified human contamination in the genomes from lmats reference database. ideally, these reads would be classified as superkingdom eukaryota on the first pass (identified as both human and t.gondii). the random model, however, led to a slightly lower score for the human label compared with equivalent t.gondii label. thus, while default parameter settings should be acceptable for many analysis cases, there may be conditionswhere awareness of these parameter settings is needed. the final user-defined parameter setting limits the number of candidate taxonomy identifier considered and was set to 50 early on to ensure efficient run times with the understanding that accuracy costs are incurred for genetic fragments associated with a complex taxonomic hierarchy. lmats fast run times allow the software to be run initially with default settings and quickly rerun on targeted subsets of reads as needed. the reported lmat tests focus on identifying known organisms in complex samples, which the results show still presents a major challenge. the use of relatively short reads (25100 bases) indicate that even known sequences can be difficult to taxonomically classify when they represent short genetic elements conserved among multiple taxa. lmat analysis on human clinical samples exhibits a high read label rate allowing the much smaller pool of unlabeled reads to be interrogated for more distant evolutionary relationships with other tools. for some environmental samples, more of the microbial contents are expected to be highly divergent from the reference database and populated with greater amounts of non-microbial eukaryotic dna. in these cases lower, rates of read labeling are obtained. the supplementary material shows the high scoring read label rates (score !1) for four different environmental samples range from 10 to 60, but relaxing the default minimum read label score threshold increases the read label rate to450 in all cases. we find that the species identified by reads with low scores are frequently the same as those identified by high confidence, high scoring reads. one can pull out additional lower scoring reads for taxa that are likely present as indicated by the higher scoring reads, thus diminishing the fraction of unclassified reads. the flexibility of adjusting the score threshold without rerunning the whole analysis enables lmat to rapidly screen large environmental datasets and obtain a large fraction of labeled reads. although the full library should be fast enough to run on large metagenomes, marker libraries still have a speed advantage. recently published marker library approaches like metaphlan rely on bacterial marker genes, which cannot be applied directly to other microbial contents. other marker libraries like sequedex rely on an individual marker sequence to contain the taxonomic signal in a single contiguous genetic element. by contrast, our method resolves the taxonomic signal across the entire read with a scoring procedure. individual k-mers may contain part of the signal with the intersection of taxonomic labels from multiple k-mers yielding a stronger signal, and thus accuracy should improve with sequencer read length. the reference database size is a function of genetic and taxonomic diversity allowing near neighbors to be added with only a limited increase in the database size. as the number of neighbor strains increase, strain level discrimination run time costs grow. although our method reports strain level discrimination, accuracy was measured at the species level because minimum genome coverage affects accuracy and the available synthetic test sets focus on species discrimination. future work will include designing better strain discrimination tests and expand the database to include functional annotation.  
