succinct data structures for assembling large genomes motivation: second-generation sequencing technology makes it feasible for many researches to obtain enough sequence reads to attempt the de novo assembly of higher eukaryotes (including mammals). de novo assembly not only provides a tool for understanding wide scale biological variation, but within human biomedicine, it offers a direct way of observing both large-scale structural variation and fine-scale sequence variation. unfortunately, improvements in the computational feasibility for de novo assembly have not matched the improvements in the gathering of sequence data. this is for two reasons: the inherent computational complexity of the problem and the in-practice memory requirements of tools. results: in this article, we use entropy compressed or succinct data structures to create a practical representation of the de bruijn assembly graph, which requires at least a factor of 10 less storage than the kinds of structures used by deployed methods. moreover, because our representation is entropy compressed, in the presence of sequencing errors it has better scaling behaviour asymptotically than conventional approaches. we present results of a proof-of-concept assembly of a human genome performed on a modest commodity server. availability: binaries of programs for constructing and traversing the de bruijn assembly graph are available from http://www.genomics.a central problem in sequence bioinformatics is that of assembling genomes from a collection of overlapping short fragments thereof. these fragments are usually the result of sequencing the determination by an instrument of a sampling of subsequences present in a sample of dna. the number, length and accuracy of these sequences varies significantly between the specific technologies, as does the degree of deviation from uniform sampling, and all these are constantly changing as new technologies are developed and refined . nonetheless, it is typically the case that we have anywhere from hundreds of thousands of sequences several hundred bases in length to hundreds of millions of sequences a few tens of bases in length with error rates between 0.1 and 10, depending on the technology. to whom correspondence should be addressed.the two main techniques used for reconstructing the underlying sequence from the short fragments are based on overlap-layout consensus models and de bruijn graph models. the former was principally used with older sequencing technologies that tend to yield fewer longer reads, and the latter has become increasingly popular with second-generation sequencing technologies, which yield many more shorter sequence fragments. irrespective of the technique, it has been shown [e.g. bythat the problem of sequence assembly is computationally hard, and as the correct solution is not rigorously defined, all practical assembly techniques are necessarily heuristic in nature. it is not our purpose here to discuss the various assembly techniqueswe restrict our attention to certain aspects of de bruijn graph assemblywe refer the reader tofor a fairly comprehensive review of assemblers and assembly techniques. space consumption is a pressing practical problem for assembly with de bruijn graph-based algorithms and we present a representation for the de bruijn assembly graph that is extremely compact. the representations we present use entropy compressed or succinct data structures. these are representations, typically of sets or sequences of integers that use an amount of space bounded closely by the theoretical minimum suggested by the zero-order entropy of the set or sequence. these representations combine their space efficiency with efficient access. in some cases, query operations can be performed in constant time, and in most cases they are at worst logarithmic. succinct data structures are a basic building block;shows more complex discrete data structures such as trees and graphs that can be built using them. some of the tasks for which they have used include web graphs , xpath indexing , partial sums and short read alignment .we have created a set of programs that construct and manipulate the de bruijn assembly graph representation we have described. these do not constitute a complete assembler, but represent the kinds of traversal and manipulation of the graph that are required to build an assembler. the proof-of-concept assembly procedure is as follows, with each step being performed by a separate program that takes an on-disk representation of the data and produces a new on-disk representation of the data:(1) extract -mers (forwards and reverse complements) from the sequence reads and sort them into lexographic order. the result of the sort operation is a list from which we can extract -mer/count pairs from which we construct the the sparse array for the graph structure and the succinct representation of the counts. on large datasets, this can be done in parts, and the resulting partial graphs are merged to form the complete graph.(2) perform a left-to-right traversal of the list of edges/counts and discard low frequency edges which almost certainly correspond to errors.(4) perform depth first traversal to read of non-branching paths within the graph to report as contigs.the first step demonstrates the feasibility of building the graph representation; the second, that it is possible to do trivial processing efficiently; the third, that graph traversal can be done to produce a modified representation (in this case eliminating paths in the graph that probably correspond to errors); and the fourth that meaningful contigs can be obtained. a more detailed description of these steps including pseudo-code is provided in section 3 of the supplementary page: 484 479486the reported time for the abyss assembly was 15 h, compared with our elapsed time of 50 h. it is not clear fromwhether the reported time is aggregate time, or elapsed (wall) time, though the latter seems more likely. materials. we believe that this proof-of-concept demonstrates the feasibility of our method, though a complete assembler would need to do significantly more processing on the graph (e.g. bubble removal), should use read-coherence to resolve local ambiguities and should make use of pairing information to resolve repeats. we have run this proof-of-concept assembly pipeline on the sequence data from a yoruban individual from, sample number na18507, with k = 27. the assembly was performed using a single computer with 82 ghz opteron cores and 32 gb ram. the size of the graph (edges and counts) at the stages of the pipeline are shown in. each step produces a set of files containing the representation of the graph. these files are then brought into memory by the program for the next step using memory-mapped i/o. the complete graph, at the end of the first step, is 52 gb, which is larger than the 32 gb ram on the machine, but the next step (removing low frequency edges) does a sequential pass over these structures to produce a new, smaller set. so although the process virtual size is considerably larger than main memory, the accesses have extreme locality, so the overall behaviour is efficient.report results of assembling the same data with abyss. in, we reproduce the results reported there for the assembly not using the pairing information from the reads, along with the results from our proof-of-concept assembly. importantly, we have included the scope of the computing resources used in both cases. unsurprisingly, our pipeline, lacking bubble elimination and read-coherent disambiguation of branches, mostly produces only short contigs. curiously, the longest contig at about 22 kb does not match the reference human genome at all, but is an exact match in to the epsteinbarr virus, which is an artifact of the preparation of the cell line from which the sequence data were obtained. that this is the longest contig is unsurprising, since viral sequences are not diploid like the human genome, and therefore are less prone to bubbles due to heterozygosity; and viral sequences tend to contain far less repetition than the human genome, and will therefore have much less branching in their de bruijn graph representation.we have claimed that the number of bits per edge should be monotonically decreasing with the number of edges. this is clearly not the case in the results in: the graph containing all the edges present in the sequence data uses more bits per edge. the analysis in section 3 gives a lower bound for the number of bits required for the graph. for the 12 billion edges in our complete graph, this suggests that about 22 bits per edge (or 30.7 gb in total) are required. from, we see that for the complete graph 28.5 bits are required. this translates to about 6.5 bits (or 10 gb) of space used beyond the the theoretical minimum. as discussed in section 4 of the supplementary materials, this is an artifact of our implementation, which could be eliminated, but in absolute terms is very minor. to put it in perspective, 28.5 bits per edge is dramatically less than the 64 bits required for a pointer, and even a hashing-based approach would require at least 35 bits per edge. 2 other entropycompressed bit vector representations may bring the space usage of the graph closer to the theoretical minimum. we have presented a practical and efficient representation of the de bruijn assembly graph, and demonstrated the kind of the operations that an assembler needs to perform but of course there is much more to doing de novo assembly with de bruijn graph methods than we have presented. a combinatoric number of eulerian paths exist in the de bruijn assembly graph, among which true paths must be identified [this is the eulerian superpath problem described by. this is usually done in the first instance by using the sequence reads to disambiguate paths. in the second instance, this is done by using paired sequence reads (e.g. pairedend and mate-pair sequence reads), in a process usually called scaffolding. the algorithms described in the literature can either be implemented directly on our representation or, in most cases, adapted.  
