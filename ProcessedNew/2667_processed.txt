genome analysis large-scale compression of genomic sequence databases with the burrowsâ€“wheeler transform motivation: the burrowswheeler transform (bwt) is the foundation of many algorithms for compression and indexing of text data, but the cost of computing the bwt of very large string collections has prevented these techniques from being widely applied to the large sets of sequences often encountered as the outcome of dna sequencing experiments. in previous work, we presented a novel algorithm that allows the bwt of human genome scale data to be computed on very moderate hardware, thus enabling us to investigate the bwt as a tool for the compression of such datasets. results: we first used simulated reads to explore the relationship between the level of compression and the error rate, the length of the reads and the level of sampling of the underlying genome and compare choices of second-stage compression algorithm. we demonstrate that compression may be greatly improved by a particular reordering of the sequences in the collection and give a novel implicit sorting strategy that enables these benefits to be realized without the overhead of sorting the reads. with these techniques, a 45 coverage of real human genome sequence data compresses losslessly to under 0.5 bits per base, allowing the 135.3 gb of sequence to fit into only 8.2 gb of space (trimming a small proportion of low-quality bases from the reads improves the compression still further). this is 4 times smaller than the size achieved by a standard bwt-based compressor (bzip2) on the untrimmed reads, but an important further advantage of our approach is that it facilitates the building of compressed full text indexes such as the fm-index on large-scale dna sequence collections. availability: code to construct the bwt and sap-array on large genomic datasets is part of the beetl library, available as a github repository at https://github.com/beetl/beetl.in this article, we present strategies for the lossless compression of the large number of short dna sequences that comprise the raw data of a typical sequencing experiment. much of the early work on the compression of dna sequences was motivated by the notion that the compressibility of a dna sequence could serve as a measure of its information content and to whom correspondence should be addressed. hence as a tool for sequence analysis. this concept was applied to topics such as feature detection in genomes and alignment-free methods of sequence comparison a comprehensive review of the field up to 2009 is given by. however, grumbach and tahi in 1994 have been echoed by many subsequent authors in citing the exponential growth in the size of nucleotide sequence databases as a reason to be interested in compression for its own sake. the recent and rapid evolution of dna sequencing technology has given the topic more practical relevance than ever. the outcome of a sequencing experiment typically comprises a large number of short sequencesoften called readsplus metadata associated with each read and a quality score that estimates the confidence of each base.and deorowicz and grabowski (2011) both describe methods for compressing the fastq file format in which such data are often stored. the metadata is usually highly redundant, whereas the quality scores can be hard to compress, and these two factors combine to make it hard to estimate the degree of compression achieved for the sequences themselves. however, both schemes employ judicious combinations of standard text compression methods such as huffman and lempel-ziv, with which it is hard to improve substantially upon the naive method of using a different 2-bit code for each of the four nucleotide bases. for example, gencompress obtains 1.92 bits per base (henceforth bpb) compression on the escherichia coli genome. an experimenter wishing to sequence a diploid genome such as a human might aim for 20-fold average coverage or more, with the intention of ensuring a high probability of capturing both alleles of any heterozygous variation. this oversampling creates an opportunity for compression that is additional to any redundancy inherent in the sample being sequenced. however, in a wholegenome shotgun experiment, the multiple copies of each locus are randomly dispersed among the many millions of reads in the dataset, making this redundancy inaccessible to any compression method that relies on comparison with a small buffer of recently seen data. this can be addressed by reference-based compression , which saves space by sorting aligned reads by the position they align to on a reference sequence and expressing their sequences as compact encodings of the differences between the reads and the reference. however, this is fundamentally a lossy strategy that achieves best compression by retaining only reads that closely match the reference, limiting the scope for future reanalyses such as realignment to a refined referencereads simulated from the e.coli genome (k12 strain) allowed us to assess separately the effects of coverage, read length and sequencingwe compared different compression schemes, both on the raw input sequences and the bwt. rlo denotes a reverse lexicographical ordering of the reads, sap is the dataset where all the reads are ordered according to the same-asprevious array. the x-axis gives the coverage level whereas the y-axis shows the number of bits used per input symbol. gzip, bzip2, ppmd (default) and ppmd (large) show compression achieved on the raw sequence data. bwt, bwt-sap and bwt-rlo give compression results on the bwt using ppmd (default) as second-stage compressor. error on the level of compression achieved. first, a 60 coverage of error-free 100 base reads was subsampled into datasets as small as 10.shows a summary plot of the compression ratios at various coverage levels for compression both on the original reads and the bwt transform. we found the ppmd mode (-m0=ppmd) of 7-zip to be a good choice of second-stage compressor for the bwt strings (referred to asin the following). rlo-sorting the datasets led to a bwt that was slightly more compressible than the sap-permuted bwt, but the difference was small (0.36 bpb versus 0.38 bpb at 60, both over 30 less than the 0.55 bpb taken up by the compressed bwt of the unsorted reads). in contrast, when gzip (http://www.gzip.org, jean-loup gailly and mark adler), bzip2 and default ppmd were applied to the original reads, each gave a compression level that was consistent across all levels of coverage and none was able to compress 2 bits per base. however, a sweep of the ppmd parameter space yielded a combination-mo=16-mmem=2048m that attained 0.50 bpb on the 60 dataset (in the following we will refer to this parameter setting as). this is because the e.coli genome is small enough to permit several-fold redundancy of the genome to be captured in the 2 gb of working space that this combination specifies. for a much larger genome such as human, this advantage disappears.of the reads less well than ppmd(default), as well as being several times slower. we also investigated the effects of sequencing errors on the compression ratios by simulating 40 datasets of 100 bp reads with different rates of uniformly distributed substitution error, finding that an error rate of 1.2 approximately doubled the size of the compressed bwt (0.90 bpb, compared with 0.47 bpb for error-free data at the same coverage). we were interested in the behaviour of bwt-based compression techniques as a function of the read length. to this end, we fixed a coverage of 40 and simulated error-free e.coli reads of varying lengths. as the read length increased from 50 bp to 800 bp, the size of the compressed bwts shrank from 0.54 bpb to 0.32 bpb. this is not surprising since longer reads allow repetitive sequences to be grouped together, which could otherwise potentially be disrupted by suffixes of homologous sequences. finally, we assessed the performance of the compressors on a typical human genome resequencing experiment (available at http://www.ebi.ac.uk/ena/data/view/era015743), containing 135.3 gb of 100-base reads, or 45 coverage of the human genome. in addition to this set of reads, we created a second dataset by trimming the reads based on their associated quality scores according to the scheme described in bwa . setting a quality score of 15 as the threshold removes 1.3 of the input bases. we again constructed the corresponding datasets in rlo and sap order.shows the improvement in terms of compression after the trimming. eliminating 1.3 of the bases improves the compression ratio by 4.5, or compressing the entire 133.6 gb down to 7.7 gb.analyzing the srx001540 dataset allowed us to compare our results with recoil, but the reads are noisier and shorter than more recent datasets and, at under 3, the oversampling of the genome is too  
