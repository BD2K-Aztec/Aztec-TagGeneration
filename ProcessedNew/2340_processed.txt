idocomp: a compression scheme for assembled genomes motivation: with the release of the latest next-generation sequencing (ngs) machine, the hiseq x by illumina, the cost of sequencing a human has dropped to a mere $4000. thus we are approaching a milestone in the sequencing history, known as the $1000 genome era, where the sequencing of individuals is affordable, opening the doors to effective personalized medicine. massive generation of genomic data, including assembled genomes, is expected in the following years. there is crucial need for compression of genomes guaranteed of performing well simultaneously on different species, from simple bacteria to humans, which will ease their transmission, dissemination and analysis. further, most of the new genomes to be compressed will correspond to individuals of a species from which a reference already exists on the database. thus, it is natural to propose compression schemes that assume and exploit the availability of such references. results: we propose idocomp, a compressor of assembled genomes presented in fasta format that compresses an individual genome using a reference genome for both the compression and the decompression. in terms of compression efficiency, idocomp outperforms previously proposed algorithms in most of the studied cases, with comparable or better running time. for example, we observe compression gains of up to 60 in several cases, including h.sapiens data, when comparing with the best compression performance among the previously proposed algorithms. availability: idocomp is written in c and can be downloaded from: http://www.stanford.edu/ iochoa/idocomp.html (we also provide a full explanation on how to run the program and an example with all the necessary files to run it.).in 2000, us president bill clinton declared the success of the human genome project , calling it the most important scientific discovery of the 20th century (although it was not until 2003 that the human genome assembly was completed). it was the end of a project that took almost 13 years to complete and cost 3 billion dollars ($1 per base pair). fortunately, sequencing cost has drastically decreased in recent years. while in 2004 the cost of sequencing a full-human genome was around $20 million, in 2008 it dropped to a million, and in 2014 to a mere $4000 (www.genome.gov/sequencingcosts). thanks to illuminas latest ngs machine, the hiseq x, we are approaching the $1000 human genome milestone. the rate of this price drop is surpassing moores law, which suggests that efficient compression will be crucial for sustaining this growth. as an example, the sequencing data generated by the 1000 genomes project (www.1000genoms.org) in the first 6 months exceeded the sequence data accumulated during 21 years in the ncbi genbank database .the compression algorithms proposed previously in the literature can be classified into two main categories: (i) compression of raw ngs data (namely fastq and sam/bam files) and(ii) compression of assembled data, i.e. the compression of fasta files containing assembled genomes. see the articles byand deorowicz and grabowski (2013a) for an extended review. moreover, within each of these categories the compression can be made either with or without a reference. we focus here on what will likely quickly become a prevalent mode: compression of assembled genomes, with a reference. specifically, we consider pair-wise compression, i.e. compression of a target genome given a reference available both for the compression and the decompression. although there exists a need for compression of raw sequencing data (fastq, sam/bam), compression of assembled genomes presented in fasta format is also important. for example, whereas an uncompressed human genome occupies around 3 gb, its equivalent compressed form is in general smaller than 10 mb, thus easing the transfer and download of genomes, e.g. it can be attached to an email. moreover, with the improvements in the sequencing technology, increasing amounts of assembled genomes are expected in the near future.next we show the performance of the proposed algorithm idocomp in terms of both compression and running time, and compare the results with the previously proposed compression algorithms. as mentioned, we consider pair-wise compression for assessing the performance of the proposed algorithm. specifically, we consider the compression of a single genome (target genome) given a reference genome available both at the compression and the decompression. we use the target and reference pairs introduced into assess the performance of the algorithm. although in all the simulations the target and the reference belong to the same species, note that this is not a requirement of idocomp, which also works for the case where the genomes are from different species. to evaluate the performance of the different algorithms, we look at the compression ratio, as well as at the running time of both thenote: each row specifies the species, the number of chromosomes they contain and the target and the reference assemblies with the corresponding locations from which they were retrieved. t. and r. stand for the target and the reference, respectively. compression and the decompression. we compare the performance of idocomp with those of gdc, green, and grs. when performing the simulations, we run both green and grs with the default parameters. the results presented for gdc correspond to the best compression among the advanced and the normal variant configurations, as specified in the supplementary data presented in. note that the parameter configuration for the h.sapiens differs from that of the other species. we modify it accordingly for the different datasets. regarding idocomp, all the simulations are performed with the same parameters (default parameters), which are hard-coded in the code (please refer to the supplementary data, section iv for more information regarding the values of the default parameters used for the simulations in idocomp, as well as for the versions and options used for the other algorithms.). for ease of exhibition, for each simulation we only show the results of idocomp, gdc and the best among green and grs. the results are summarized in. for each species, the target and the reference are as specified in. to be fair across the different algorithms, especially when comparing the results obtained in the small datasets, we do not include the cost of storing the headers in the computation of the final size for any of the algorithms. the last two columns show the gain obtained by our algorithm idocomp with respect to the performance of green/grs and gdc. for example, a reduction from 100 kb to 80 kb represents a 20 gain (improvement). note that with this metric a 0 gain means the file size remains the same, a 100 improvement is not possible, as this will mean the new file is of size 0, and a negative gain means that the new file is of bigger size. as seen in, the proposed algorithm outperforms in compression ratio the previously proposed algorithms in all cases. moreover, we observe that whereas green/grs seem to achieve better compression in those datasets that are small and gdc in the h.sapiens datasets, idocomp achieves good compression ratios in all the datasets, regardless of their size, the alphabet and/or the species under consideration. in cases of bacteria (small size dna), the proposed algorithm obtains compression gains that vary from 30 against green/grs to up to 64 when compared with gdc. for the s.cerevisae dataset, also of small size, idocomp does 55 (61) better in compression ratio than grs (gdc). for the case of medium size dna (c.elegans, a.thaliana, o.sativa and d.melanogaster) we observe similar results. idocomp again outperforms the other algorithms in terms of compression, with gains up to 92. finally, for the h.sapiens datasets, we observe that idocomp consistently performs better than green, with gains above 50 in four out of the six datasets considered, and up to 91. with respect to gdc, we also observe that idocomp produces better compression results, with gains that vary from 3 to 63. based on these results, we can conclude that gdc and the proposed algorithm idocomp are the ones that produce better compression results on the h.sapiens genomes. in order to get more insight into the compression capabilities of both algorithms when dealing with human genomes, in the supplementary data, section vii we provide more simulation results. specifically, we simulate twenty pair-wise compressions, and show that on average idocomp employs 7.7 mb per genome, whereas gdc employs 8.4 mb. moreover, the gain of idocomp with respect to gdc for the considered cases is on average 9.92. regarding the running time, we observe that the compression and the decompression time employed by idocomp is linearly dependent on the size of the target to be compressed. this is not the case of green, for example, whose running times are highly variable. in gdc we also observe some variability in the time needed for compression. however, the decompression time is more consistent among the different datasets (in terms of the size), and it is in general the smallest among all the algorithms we considered. idocomp and green take approximately the same time to compress and decompress. overall, idocomps running time is seen to be highly competitive with that of the existing methods. however, note that the time needed to generate the suffix array is not counted in the compression time of idocomp, whereas the compression time of the other algorithms may include construction of index structures, like in the case of gdc (see the supplementary data, section v for details on the construction of the suffix arrays.).finally, we briefly discuss the memory consumption of the different algorithms. we focus on the compression and decompression of the h.sapiens datasets, as they represent the larger files and thus the memory consumption in those cases is the most significant. idocomp employs around 1.2 gb for compression, and around 2 gb for decompression. green consumes around 1.7 gb both for compression and decompression. finally, the algorithm gdc employs 0.9 gb for compression and 0.7 gb for decompression.inspection of the empirical results of the previous section shows the superior performance of the proposed scheme across a wide range of datasets, from simple bacteria to the more complex humans, without the need of adjusting any parameters. this is a clear advantage over algorithms like gdc, where the configuration must be modified depending on the species being compressed. although idocomp has some internal parameters, namely, l, d, k and q, the default values that are hard-coded in the code perform very well for all the datasets, as we have shown in the previous section. (see the post-processing step in section 2 for more details.) however, the user could modify these parameters data-dependently and achieve better compression ratios. future work will explore the extent of the performance gain (which we believe will be substantial) due to optimizing for these parameters. we believe that the improved compression ratios achieved by idocomp are due largely to the post-processing step of the algorithm, which modifies the set of instructions in a way that is beneficial to the entropy encoder. in other words, we modify the elements contained in the sets so as to facilitate their compression by the arithmetic encoder. moreover, the proposed scheme is universal in the sense that it works regardless of the alphabet used by the fasta files containing the genomic data. this is also the case with gdc and green, but not with previous algorithms like grs or rlz-opt which only work with a, c, g, t, and n as the alphabet. it is also worth mentioning that the reconstructed files of both idocomp and gdc are exactly the original files, whereas the reconstructed files under green do not include the header and the sequence is expressed in a single line (instead of several lines). another advantage of the proposed algorithm is that the scheme employed for compression is very intuitive, in the sense that the compression consists mainly of generating instructions composed of the sequence of matches m and the two sets s and i that suffice to reconstruct the target genome given the reference genome. this information by itself can be beneficial for researchers and gives insight into how two genomes are related to each other. moreover, the list of snps generated by our algorithm could be compared with available datasets of known snps. for example, the ncbi dbsnp database contains known snps of the h.sapiens species. finally, regarding idocomp, note that we have not included inthe time needed to generate the suffix array of the reference, only that needed to load it into memory, which is already included in the total compression time. (we refer the reader to the supplementary data, section v for information on the time needed to generate the suffix arrays.) the reason is that we devise these algorithms based on pair-wise compression as the perfect tool for compressing several individuals of the same species. in this scenario, one can always use the same reference for compression, and thus the suffix array can be reused as many times as the number of new genomes that need to be compressed.  
