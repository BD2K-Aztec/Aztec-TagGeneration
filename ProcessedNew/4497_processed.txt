genome analysis swiftlink: parallel mcmc linkage analysis using multicore cpu and gpu motivation: linkage analysis remains an important tool in elucidating the genetic component of disease and has become even more important with the advent of whole exome sequencing, enabling the user to focus on only those genomic regions co-segregating with mendelian traits. unfortunately, methods to perform multipoint linkage analysis scale poorly with either the number of markers or with the size of the pedigree. large pedigrees with many markers can only be evaluated with markov chain monte carlo (mcmc) methods that are slow to converge and, as no attempts have been made to exploit parallelism, massively underuse available processing power. here, we describe swiftlink, a novel application that performs mcmc linkage analysis by spreading the computational burden between multiple processor cores and a graphics processing unit (gpu) simultaneously. swiftlink was designed around the concept of explicitly matching the characteristics of an algorithm with the underlying computer architecture to maximize performance. results: we implement our approach using existing gibbs samplers redesigned for parallel hardware. we applied swiftlink to a real-world dataset, performing parametric multipoint linkage analysis on a highly consanguineous pedigree with east syndrome, containing 28 members, where a subset of individuals were genotyped with single nucleotide polymorphisms (snps). in our experiments with a four core cpu and gpu, swiftlink achieves a 8.5 speed-up over the single-threaded version and a 109 speed-up over the popular linkage analysis program simwalk. availability: swiftlink is available at https://github.com/ajm/ swiftlink. all source code is licensed under gplv3.exact linkage analysis algorithms scale exponentially with the size of the input. beyond a critical point, the amount of work that needs to be done exceeds both available time and memory. the elstonstewart algorithm calculates successive conditional likelihoods between family members, permitting the algorithm to scale linearly with the size of the pedigree (more precisely, the number of meioses), but can only analyse a handful of markers jointly. the landergreen hidden markov model (hmm) approach , on the other hand, can analyse many markers, but only in pedigrees of limited size. in between these two extremes, approaches using bayesian networks, for example superlink , scale to greater numbers of markers than the elstonstewart algorithm and to larger pedigrees than the landergreen algorithm, but still cannot handle both large pedigrees and many markers. in the situation where we have a large, perhaps consanguineous, pedigree typed with many markers, we are forced to either abbreviate the input or else use an approximation like markov chain monte carlo (mcmc). although mcmc helps make the problem tractable, it can take a long time to converge. the problem of slow running time is compounded by the fact that existing software is single-threaded and, therefore, not designed to take advantage of all the processing power available in even a single modern pc. this underuse will become more acute as multicore processors feature increasing numbers of cores and graphics processing units (gpus) become more general purpose. parallelism has previously been applied to accelerate exact linkage algorithms in the applications fastlink, across networks of workstations , parallel-genehunter, on computer clusters and superlink-online, on grid middleware . although parallel implementations of exact linkage algorithms perform analyses faster and can scale to larger problems owing to accessing memory in multiple machines, they still have the same fundamental scaling properties as their underlying algorithms. problems outside of the scope of exact algorithms must be analysed with mcmc-based linkage programs such as to whom correspondence should be addressed. the author 2012. published by oxford university press. all rights reserved. for permissions, please e-mail: journals.permissions@oup.com simwalk2 and morgan , which have both been shown, where possible, to produce results of comparable accuracy to exact algorithms . to the best of our knowledge, there have been no published attempts to parallelize mcmc-based linkage analysis. in general, parallelizing mcmc is considered a hard problem because the states of the markov chain form a strict sequential ordering. successful parallel mcmc implementations tend to focus either on parallelizing expensive likelihood calculations (for example,) or on multiple chain schemes (for example,). the approach we took with swiftlink maximizes the use of hardware resources by combining both previous approaches, allowing different parts of the calculation to be distributed across both cpu and gpu. the markov chain is run in parallel on the cpu, where each step in the chain is performed by one of two multithreaded block gibbs samplers based on the locus sampler and the meiosis sampler . expensive likelihood calculations required to estimate lod (logarithm of odds) scores are performed on the gpu, if one is available. high performance is achieved on the gpu by maximizing hardware use, requiring us to identify substantial independent calculations. this article is organized as follows: we elaborate on the details of the gibbs samplers used to form the markov chain, what aspects of each sampler were changed to facilitate execution on parallel hardware and how swiftlink orchestrates these actions to maximize hardware use. we report experimental results comparing swiftlink with other mcmc-based linkage analysis software on a highly consanguineous pedigree and end with discussion and an outline of future work.the single-threaded variants of both locus and meiosis samplers have been shown previously to perform well on simulated datasets that can be analysed using exact linkage algorithms . in this article, we focus on a real-world case study to gain an appreciation for the runtimes inherent in linkage studies that require mcmc. our case study, shown in, is a highly consanguineous six-generation pedigree with east syndrome. east syndrome is an autosomal recessive monogenic disorder related to improper renal tubular salt handling where patients additionally present with the following symptoms: infantile-onset seizures, ataxia and sensorineural deafness. genotyping with snps (single nucleotide polymorphisms) was performed on six members of the pedigree using affymetrix genechip human mapping 10k snp chips including all affected individuals and the parents of the three affected siblings (individuals 2328 in). in this article, we only consider chromosome 1, which included 780 snps (0.35 cm average spacing). the disease trait iscontaining 28 members over six generations assumed to have an allele frequency of 0.0001 and to have complete penetrance. we compare the runtime performance of simwalk (version 2.91), morgan (version 3.1, lm_linkage program) and swiftlink in several configurations. simwalk was always run with default parameters and, as the runtime can be prohibitively slow for large numbers of markers, each dataset was run in windows of 50 markers (this size was chosen based on experience with past projects). even with this advantage, simwalk still had to be run on a computer cluster, whereas morgan and swiftlink were run on a single desktop pc. we report simwalks total runtime assuming sequential execution. morgan, by default, is compiled at optimization level 0, causing unnecessary slowdown. to ensure a fair comparison, we recompiled morgan at optimization level 2, which we refer to as morgan fast. following the parameters used in a previous study , both morgan and swiftlink were run for a total of 100 000 iterations, comprising 10 000 iterations of burn-in and 90 000 iterations of simulation. plocus sampler was set to 0:5. the markov chain was initialized with 1000 runs of sequential imputation. during simulation, we scored every 10th sample and calculated lod scores at 10 equidistant points between each consecutive pair of markers. swiftlink and morgan were both run on the same computer running ubuntu 12.04.1 lts edition with linux kernel 3.2.0-30 for 64-bit. the computer has an amd phenom ii with four processor cores (clocked at 3.2 ghz) and 4 gb ram (clocked at 1.6 ghz). gpu performance was tested with cuda 5.0 (release candidate 1) on an nvidia gtx 580. each experiment was repeated 10 times, all results are averages over all runs.shows typical results for the region of interest for the east pedigree on chromosome 1 from each of the four parallel versions of swiftlink tested. the most likely linked region found by all programs corresponded to the 840 kilobase region identified in the original study containing the kcnj10 gene with similar maximum lod scores for the region of interest .shows the different performance characteristics of simwalk, morgan and swiftlink. in addition to these results, we anecdotally ran simwalk once with all markers, which took $42 days, making simwalk by far the slowest. even with the advantage of running in 50 marker windows, simwalk had the worst runtime of the three programs tested. all versions of swiftlink were faster than both morgan and morgan fast, with single-threaded swiftlink showing a 1.6 speed-up over morgan fast. this difference in runtime between single-threaded programs can probably be attributed to differences in the internal representation used for pedigree peeling; swiftlink peels a genotype network, whereas morgan appears to peel an allele network. while allele networks ultimately scale better than genotype networks, in our experience, a majority of pedigrees will be smaller than is necessary for this to make a difference. at its fastest, using four cpu cores and gpu, swiftlink is 13.6 faster than morgan fast and 109.4 faster than simwalk analysing the east pedigree . swiftlink achieves a high degree of parallelism, as 53 of the programs total runtime was spent on serial tasks that could not be parallelized. using all four cores of the cpu and the gpu, we achieve up to almost an order of magnitude (8.5) speed-up over the single-threaded implementation. compared with four cpu cores, the addition of a gpu provides a 2.4 speed-up. we had expected higher performance, but the cpu is a bottleneck which does not run the markov chain fast enough to keep up with the gpu. we expect the situation to improve with cpus with more cores and with improvements to our cpu code (so far we have not used simd instructions). to demonstrate how underused the gpu is, the lod scoring code in isolation performs 24 faster than a single-threaded cpu implementation.swiftlink (cpux1) swiftlink (cpux2) swiftlink (cpux4) swiftlink (cpux4+gpu). results from all four tested versions of swiftlink showing region of interest from chromosome 1 of the east pedigreethe methods presented in this article produce almost an order of magnitude speed-up of mcmc linkage analysis compared with an optimized single-threaded implementation on fairly minimal hardware. compared with existing mcmc linkage software, swiftlink achieves up to two orders of magnitude speed-up when using four cpu cores and a gpu concurrently. we believe swiftlink will be especially useful to researchers in, for example, clinical research settings, where access and/or experience with computer clusters is limited. in addition, swiftlink has been specifically designed to fit into existing workflows by supporting the standard linkage file formats (ped, map and data files) as other popular mulitpoint linkage analysis programs, e.g. allegro and genehunter. a majority of linkage projects include only small-to-medium size pedigrees and can therefore be analysed using exact algorithms. many large pedigrees can be successfully abbreviated to run using an exact algorithm; however, for cases where there are few genotyped individuals or in cases like east syndrome , where one of the branches only contains a single genotyped individual, any abbreviation will dramatically reduce the power of the study. this is a clear niche where swiftlink can be used to great effect. unlike many other articles detailing gpu applications, we did not show results for both 32-and 64-bit. as performance on the gpu is affected so strongly by register usage and therefore occupancy, 32-bit code tends to run faster as pointers take up half the space and more blocks can be run concurrently. however, cpu code in general performs faster in 64-bit thanks to wider instructions. in the current version of swiftlink, the gpu is underused because the cpu is a bottleneck, therefore it only makes sense for us to run in 64-bit mode. in the future, we will be aiming to improve swiftlink by removing the bottlenecks from the slower execution of the markov chain on the cpu. additional speed-ups could be made by using simd instructions as well as multithreading or by offloading some of the excess work to the gpu. it is not clear how to balance the load dynamically between both platforms and whether the additional complexity is warranted for a speed-up that can be provided more simply with additional cpus, e.g. with more cores locally or in a computer cluster.  
