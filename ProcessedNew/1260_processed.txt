gene expression data-based filtering for replicated high-throughput transcriptome sequencing experiments during the past 5 years, next-generation high-throughput sequencing (hts) technology has become an essential tool for genomic and transcriptomic studies. in particular, the use of hts technology to directly sequence the transcriptome, known as rna sequencing (rna-seq), has revolutionized the study of gene expression by opening the door to a wide range of novel applications. unlike microarray data, which are continuous, rna-seq data represent highly heterogeneous counts for genomic regions of interest (typically genes) and often exhibit zero-inflation and a large amount of overdispersion among biological replicates; as such, a great deal of methodological research (e.g.) has recently focused on appropriate normalization and analysis techniques that are adapted to the characteristics of rna-seq data; seefor a review of rna-seq technology and analysis procedures. as with data arising from previous technologies, such as microarrays or serial analysis of gene expression, hts data are often used to conduct differential analyses. in recent years, several approaches for gene-by-gene tests using gene-level hts data have been proposed, with the most popular making use of poisson , overdispersed poisson or negative binomial distributions . because a large number of hypothesis tests are performed for gene-by-gene differential analyses, the obtained p-values must be adjusted to address the fact that many truly null hypotheses will produce small p-values simply by chance; to address this multiple testing problem, several well-established procedures have been proposed to adjust p-values to control various measures of experiment-wide false positives, such as the false-discovery rate. although such procedures may be used to control the number of false positives that are detected, they are often at the expense of the power of an experiment to detect truly differentially expressed (de) genes, particularly as the number of genes in a typical hts dataset may be in the thousands or tens of thousands. to reduce this impact, several authors in the microarray literature have suggested the use of data filters to identify and remove genes that appear to generate an uninformative signal and have no or little chance of showing significant evidence of differential expression; only hypotheses corresponding to genes that pass the filter are subsequently tested, which in turn tempers the correction needed to adjust for multiple testing. in recent work,advocate for the use of independent data filtering, in which the filter and subsequent test statistic pairs are marginally independent under the null hypothesis, and the dependence structure among tests remains largely unchanged pre-and post-filter, ensuring that post-filter p-values are true p-values. for such an independent filter to be effective, it must be positively correlated with the test statistic under the alternative hypothesis; indeed, it is this correlation that leads to an increase in detection power after filtering. in addition, bourgon et al. demonstrate that non-independent filters for which dependence exists between the filter and test statistic (e.g. making use of condition labels to filter genes with average expression in at least one condition less than a given threshold), can in some cases lead to a loss of control of experiment-wide error rates. several ad hoc data filters for rna-seq data have been used in recent years, including filtering genes with a total read count smaller than a given threshold and filtering genes with at least one zero count in each experimental condition ; however, selecting an arbitrary threshold to whom correspondence should be addressed. the author 2013. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. for commercial re-use, please contact journals.permissions@oup.com value to filter genes in this way does not account for the overall sequencing depth or variability of a given experiment. one exception to these ad hoc filters is the work of, in which a comparison between expression levels of exonic and intergenic regions was used to find a threshold for detectable expression above background in various human and mouse tissues, where expression was estimated as reads per kilobase per million mapped reads (rpkm) . the threshold of 0.3 rpkm identified in this work has in turn been applied to several other studies (e.g. canovas). however, to our knowledge, although filters for read counts are routinely used in practice, little attention has been paid to the choice of the type of filter or threshold used or its impact on the downstream analysis. in this article, we propose a novel data-based procedure to choose an appropriate filtering threshold based on the calculation of a similarity index among biological replicates for read counts arising from replicated high-throughput transcriptome sequencing data. this technique provides an intuitive datadriven way to filter rna-seq data and to effectively remove genes with low constant expression levels. our proposed filtering threshold may be useful in a variety of applications for rna-seq data, including differential expression analyses, clustering and co-expression analyses, and network inference.in the following, we apply the normalization approach proposed byfor mean-and maximum-based filters, although other types of normalization may be appropriate for some data. for gene-by-gene comparisons between two conditions, we illustrate the use of the proposed filter in conjunction with the model proposed in the deseq bioconductor package (version 1.8.3), which has been developed to model count data with a small number of replicates in the presence of overdispersion ; p-values are adjusted for multiple testing using the benjaminihochberg procedure to control the false-discovery rate. we note that the filtering method proposed here may also be used in conjunction with other popular methods, e.g. edger . see the supplementary materials for additional discussion of the normalization and statistical testing methods used in this work. in practice, there may be some question about the appropriate point in the analysis pipeline to apply data filters: should normalized data first be filtered, then normalization factors re-estimated and the model fit (i.e. mean and dispersion parameters estimated)? should normalization factors and model parameters be estimated based on the full data, and the data filtered only at the end of the analysis pipeline? the difference between the two options is non-trivial, particularly as the differential analysis approaches implemented in the deseq and edger packages both borrow information across genes (whether all or only those passing the filter) to obtain per-gene parameter estimates. in this work, we present results based on the application of filters applied as late in the pipeline as possible, i.e. after library size and dispersion parameter estimation; a more detailed discussion of this issue is included in the supplementary materials.  
