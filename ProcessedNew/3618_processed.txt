data and text mining positive and negative forms of replicability in gene network analysis motivation: gene networks have become a central tool in the analysis of genomic data but are widely regarded as hard to interpret. this has motivated a great deal of comparative evaluation and research into best practices. we explore the possibility that this may lead to overfitting in the field as a whole. results: we construct a model of research communities sampling from real gene network data and machine learning methods to characterize performance trends. our analysis reveals an important principle limiting the value of replication, namely that targeting it directly causes easy or unin-formative replication to dominate analyses. we find that when sampling across network data and algorithms with similar variability, the relationship between replicability and accuracy is positive (spearmans correlation, r s $0.33) but where no such constraint is imposed, the relationship becomes negative for a given gene function (r s $ 0.13). we predict factors driving replicability in some prior analyses of gene networks and show that they are unconnected with the correctness of the original result, instead reflecting replicable biases. without these biases, the original results also vanish replicably. we show these effects can occur quite far upstream in network data and that there is a strong tendency within proteinprotein interaction data for highly replicable interactions to be associated with poor quality control.increasingly, biologists have turned to computational methods to sift through the vast array of pre-existing genomics data for validation that a gene has a molecular role in the phenotype of interest or to prioritize a candidate as disease causal . these computational methods usually fit under the rubric of machine learning and use network data that represent the interaction of genes or their products. many of these computational methods depend on a form of guilt by association, in which a gene is inferred to possess a particular function based on its similarity to other genes with that function . the most common form of similarity used in these tasks is that of genomic sequence similarity which is easily implemented through supervised use of blast and comparatively straightforward to interpret. while sequence-based analysis is essentially routine within biology, one of the promises of systems biology has been to extend the form of association used to relate genes to potentially subtler relationships, such as proteinprotein interaction (ppi), co-expression, genetic interaction or phylogenetic profiles. systems-based prediction of gene function has found particular application in the interpretation of disease-causal variants due to the difficulty of finding overlaps in v c the author 2015. published by oxford university press. all rights reserved. for permissions, please e-mail: journals.permissions@oup.com known functions among candidate genes . however, progress in the context of both data and methodology has been surprisingly uncertain . the need for better assessment of methods in function inference and network analysis is widely recognized and has led to numerous field-wide evaluations, often called critical assessments . the two principal goals of critical assessments are (i) to make the performances of individual methods less prone to overfitting and (ii) for comparisons between methods to be within the same framework. overfitting is minimized since participants are truly blind to the success of their method prior to assessment and thus cannot tailor their solutions to the benchmarking metric. gene networks possess unusually prominent consensus resources [e.g. the gene ontology (go) , biogrid (, making evaluation within a welldefined framework possible. by reducing overfitting and making methods directly comparable, critical assessments endeavor to make science more replicable; their outputs and comparative evaluations can be trusted to generalize. the difficulty of characterizing the features in gene networks that drive successful uses has contributed to making replicability in their output, which can be more easily measured, particularly important to evaluation within their critical assessments [e.g. the dream challenges and the critical assessment of protein function annotation algorithms, cafa challenge (. in performing this evaluation, critical assessments are simply performing a more topdown version of the usual scientific process of refinement through replicability . while this may be desirable in some ways, it creates a new potential for overfitting for the field in its entirety. we decided to explore this possibility by simulating multiple gene function prediction tasks and outcomes and hence the field of gene network analysis as a whole. in our model of research in gene network analysis, each separate researcher is represented by an individually developed machine learning algorithm with access to particular data. the algorithms are both diverse and in common use for diverse bioinformatics problems and thus reasonably reflecting ordinary practice. the data resources (or library) given to these algorithms are similarly diverse and frequently used sources of human gene network information, varying from individual expression profiles to consensus pathway information. we refer to a specific combination of algorithm and data as a researcher . for example, a researcher may consist of the algorithm random walk with restarts using specific co-expression data. the individual sampled resources do not represent partial data sets but rather ones which are at least as comprehensive as is typical of any given study. because it is a central characteristic by which we judge results, our focus is on using these model researchers to understand replicability in gene network analysis. after deriving general principles through our simulations, we focus on two important applications affecting the interpretation of disease genes and ppi data in current research, with a focus on psychiatric genetics.our approach to formalizing replicability is to treat it exactly parallel to how performance is conventionally assessed. in general, performance is measured by determining if a researcher can correctly predict some unknown (or held back) result; likewise, we measure replicability by measuring how well a researcher correctly predicts the consensus across other researchers. that is, there are conventional metrics for assessing whether a given researchers answer is similar to the truth; in measuring replicability, we perform the identical assessment but treat the consensus output among other researchers as the truth against which a given researcher is evaluated. we assess this using the area under the receiver operating characteristic curve (auroc) in both cases (see section 2 for further details). note that all of our analysis is readily reproducible, by which we simply mean that analyses can be re-done, which we differentiate from replicability involving independent analysis, the phenomena we are modelling.  
