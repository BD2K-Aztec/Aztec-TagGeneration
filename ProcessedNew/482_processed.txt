a robust clustering algorithm for identifying problematic samples in genome-wide association studies high-throughput genotyping arrays provide an efficient way to survey single nucleotide polymorphisms (snps) across the genome in large numbers of individuals. downstream analysis of the data, for example in genome-wide association studies (gwas), often involves statistical models of genotype frequencies across individuals. the complexities of the sample collection process and the potential for errors in the experimental assay can lead to biases and artefacts in an individuals inferred genotypes. rather than attempting to model these complications, it has become a standard practice to remove individuals whose genome-wide data differ from the sample at large. here we describe a simple, but robust, statistical algorithm to identify samples with atypical summaries of genome-wide variation. its use as a semi-automated quality control tool is demonstrated using several summary statistics, selected to identify different potential problems, and it is applied to two different genotyping platforms and sample collections. availability: the algorithm is written in r and is freely available at www.well.the advent of new technologies, which can simultaneously genotype hundreds of thousands of single nucleotide polymorphisms (snps) across the genome, has permitted large-scale studies of human genetic variation. a major application of these technologies is to undertake genome-wide association studies (gwas) to identify snps that correlate with phenotypes such as disease. an important step in providing convincing evidence of association is to argue that the observed correlation is not an artefact of either the sampling strategy (for example, hidden population structure) or systematic biases in inferring genotypes (for example, differences in call rates). in doing so, it has become standard practice to calculate summaries of genome-wide variation that are not expected to vary to whom correspondence should be addressed. a list of participants and affiliations appear in supplementary material.systematically between study individuals, and then to identify and remove outlying individuals. under the correct statistical model, losing data (that is collected at some expense) nearly always results in reduced statistical power to detect real effects. however, when the model fails to capture the data generating process, inclusion of outlying individuals often leads to an increase in false positives. exclusion of individuals prior to analysis is a trade-off between loss of power due to reduced sample size, and the benefit of controlling the number of false positives. the typical approach to identify potentially problematic samples is to calculate summary statistics of genome-wide data and then, by visualizing their distributions across individuals, to manually choose a threshold based on their values for the majority of the data. to automate this process requires an algorithm to infer the distribution of normal study individuals, therefore allowing inference of outliers. for the approach to be applicable in many settings (different summary statistics, genotyping platforms and sample collections) requires a robust model for the outlying individuals.results are shown inand supplementary figures s1s3 for, 58c and ukbs samples genotyped on affymetrix and illumina platforms, respectively. as well as being statistically principled, in practice, it is helpful that, once the prior distributions have been specified, identification of outliers is automatic. empirically, it appears to make sensible inference for a range of normal and outlier distributions, suggesting it is useful for quality control in gwas  
