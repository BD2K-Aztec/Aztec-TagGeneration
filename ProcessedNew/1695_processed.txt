data and text mining exploration and retrieval of whole-metagenome sequencing samples motivation: over the recent years, the field of whole-metagenome shotgun sequencing has witnessed significant growth owing to the high-throughput sequencing technologies that allow sequencing genomic samples cheaper, faster and with better coverage than before. this technical advancement has initiated the trend of sequen-cing multiple samples in different conditions or environments to explore the similarities and dissimilarities of the microbial communities. examples include the human microbiome project and various studies of the human intestinal tract. with the availability of ever larger databases of such measurements, finding samples similar to a given query sample is becoming a central operation. results: in this article, we develop a content-based exploration and retrieval method for whole-metagenome sequencing samples. we apply a distributed string mining framework to efficiently extract all informative sequence k-mers from a pool of metagenomic samples and use them to measure the dissimilarity between two samples. we evaluate the performance of the proposed approach on two human gut metagenome datasets as well as human microbiome project metagenomic samples. we observe significant enrichment for diseased gut samples in results of queries with another diseased sample and high accuracy in discriminating between different body sites even though the method is unsupervised. availability and implementation: a software implementation of the dsm framework is available at https://github.com/hiitmetagenomics/metagenomics is the study of microbial communities in their natural habitat using genomics techniques . it is undergoing a boom owing to the proliferation of highthroughput sequencing technologies. many studies focus at targeted sequencing of specific marker genes such as the 16s rrna gene in bacteria, but recently there has been a growing interest in whole-metagenome sequencing (e.g. human microbiome). although targeted studies provide data for phylogenetic profiling at a lower cost, whole metagenomes provide much more information, for example, about the collective metabolism and the population genetics of the community . recent studies have also found associations between features of whole human gut metagenomes and type ii diabetes . new data are accumulating rapidly, with a popular web-based mg-rast server listing almost 3000 public whole metagenomes. analyzing whole-metagenome shotgun (wms) sequencing data is very challenging. the original sample typically contains genetic material from hundreds to thousands of bacterial species of different abundances , most of which have not been fully sequenced previously. after sequencing, we obtain a huge collection of short sequence reads whose species of origin is unknown. although significant progress has been made, analysis relying on either the limited previously annotated genomes, or assembling the reads into novel more complete genomes, remains difficult and inefficient, and potentially susceptible to annotation biases. in this article, we introduce an efficient purely data-driven feature extraction and selection method as well as similarity measures for wms sequencing datasets, and apply them in retrieval of similar datasets. such content-based retrieval is an extremely powerful tool for exploration of the data and generating hypotheses of disease associations, as previously demonstrated with gene expression data . retrieval from existing databases makes it possible to automatically explore a much greater variety of hypotheses than relying solely on the more common specifically designed focused studies. content-based similarity measures and retrieval of similar metagenomic datasets have been suggested previously , based on quantifying abundances over a relatively small number of predetermined features requiring existing annotation. up to some thousands of known taxa, genes or metabolic pathways have been used. we introduce similarity measures that are based solely on raw sequencing reads, and hence, unbiased and insensitive to the quality of the existing annotation. a similar measure has been previously suggested by, but only for pairwise comparisons using a method that is computationally too expensive to scale to even modestly large to whom correspondence should be addressed. the author 2014. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. datasets. furthermore, instead of considering all sequences of particular length, also known as k-mers, as has been done earlier for other tasks and by, we employ an efficient distributed string mining (dsm) algorithm to find informative subsequences that can be of any length. to deal with the large number of features, some feature selection is necessary. previous approaches for detecting relevant features in metagenomic data have been based on direct comparison of two classes of samples. again, most of these methods work on up to some thousands of features , with the notable exception of one study where quantification and association testing was done for 44.3 million predefined genes. without feature selection, one can use short k-mers or limit to a set of k-mers that are likely to be informative, such as k-mers associated with well-characterised protein families . although there are no previous examples of unsupervised feature selection for metagenomics, it is a common practice in information retrieval with text documents ; a particularly relevant method assesses the entropy of the distribution of documents in which a specific term occurs . we evaluate the performance of the proposed unsupervised, unconstrained retrieval method on synthetic data, as well as metagenomic samples from human body sites . to evaluate the performance of the retrieval engine, we use external validation based on a ground truth similarity between two samples. to simplify this process, we consider a binary similarity, which is crude but easily accessible. the human gut samples in come from studies exploring the change in bacterial species composition between healthy persons and either inflammatory bowel disease (ibd) or type ii diabetes. we utilize disease state to construct a binary ground truth. thus, we study if, given the metagenomic sample of a person with a disease, the retrieval finds metagenomic samples related by having the same disease. in the body site data (human microbiome), we use the body sites as ground truth to investigate whether it is possible to identify the bacterial communities at different body sites in an unsupervised setting without the need of reference genomes. it should be noted that especially for the gut data, two samples may be related in other ways too. the external validation with one simple ground truth nonetheless provides an objective platform for comparing different methods. given that the method is unsupervised and hence completely oblivious of the disease labels, if such retrieval is successful, it is a promising starting point for developing methods for leveraging data from earlier patients in early detection of disease and personalized medicine. of a retrieval task, (iii) compute a distance metric using the filtered k-mer frequencies, and (iv) execute these steps fast without explicitly storing the frequency values.summarizes the method.we evaluated the retrieval performance on three human metagenomics datasets:(1) metahit , 124 metagenomic samples from 99 healthy people and 25 patients with ibd syndrome. each sample has on average 65 ae 21 million reads. our goal was to retrieve ibd-positive patients.(2) t2d phase ii , 199 metagenomic samples from 100 healthy people and 99 patients with type ii diabetes. each sample has on average 47 ae 11 million reads. our goal was to retrieve patients with diabetes. we chose to explore the phase ii data instead of the phase i data, as the former has higher coverage; about 40 more reads than the latter.). of 690 samples that passed the qc assessment (http://www.hmpdacc.org/ hmasm/), we discarded 255 samples that had 51 of the number of reads of the largest sample.to recapitulate, for metahit and t2d-p2, our goal is to observe if given a positive sample, e.g. from a patient with a particular disease, one can retrieve relevant samples, i.e. with similar disease; whereas for hmp, our goal is to observe if given a sample from a particular body site, one can retrieve relevant samples, i.e. samples from the same body site. for all data, we applied a quality threshold of 30 and ignored any base pairs with quality less than the threshold.gives an overview of the computational resources required for each dataset. additionally, number of k-mers used by different methods for each dataset are available in the supplementary. retrieval of samples with similar annotation: we applied the proposed approach and a number of alternatives for retrieval of similar samples from the same dataset and evaluated by how many of the retrieved samples had the same annotation: class label, disease state or body site. a comparison of the obtained map values averaged over queries by all positive samples is shown in. the results show the performance achieved by the optimized metric. the alternatives we considered were (i) retrieval performance based on the proposed distances between frequencies of 21-mers appearing in known protein families (figfams) with added pseudocounts but without entropy filtering ; (ii) retrieval based on bray curtis dissimilarity between relative species abundancesin the wake of collecting multiple samples from similar environments, information retrieval for metagenomic samples is expected to become a handy tool in metagenomics research. in this article, we have addressed the problem of retrieving relevant metagenomic samples given a query sample from the same collection. the novelty of the proposed approach is that it is unsupervised, and does not rely on the availability of reference databases. we, for proposed approach all, individual ks as well as figfam-based distance metric. the metrics are optimized/averaged over 101 equally spaced threshold values between 0 and 1. each error bar line shows the map value along with the standard error. the grey horizontal line shows retrieval by chance: map computed over zero dissimilarity metric. an arrow (if present) over a method implies whether the performance of the corresponding method (top: average metric, bottom: optimized metric) is better or worse than when entropy filtering is employed: the stars denote significance level: 0550.001550.01550.05. we observe that filtering has a positive impact on the retrieval performance. comparison of best performances for different k-mer lengths. the figures show the performance over queries by all positive samples as a violin plot. all methods use the optimized metric chosen over 101 equally spaced threshold values between 0 and 1: the box denotes the map value. the horizontal lines show retrieval by chance: avep computed over zero dissimilarity metric. straight line is the mean, and dotted lines are 5 and 95 quantiles, respectively, when number of relevant samples differ for different queries. an arrow (if present) over a method implies whether the corresponding method performs significantly better or worse than all: the stars denote significance level: 0550.001550.01550.05. we observe that the considering all k-mers usually perform equally well with respect to considering a single k  
