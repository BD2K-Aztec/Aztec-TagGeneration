sequence analysis scalce: boosting sequence compression algorithms using locally consistent encoding motivation: the high throughput sequencing (hts) platforms generate unprecedented amounts of data that introduce challenges for the computational infrastructure. data management, storage and analysis have become major logistical obstacles for those adopting the new platforms. the requirement for large investment for this purpose almost signalled the end of the sequence read archive hosted at the national center for biotechnology information (ncbi), which holds most of the sequence data generated world wide. currently, most hts data are compressed through general purpose algorithms such as gzip. these algorithms are not designed for compressing data generated by the hts platforms; for example, they do not take advantage of the specific nature of genomic sequence data, that is, limited alphabet size and high similarity among reads. fast and efficient compression algorithms designed specifically for hts data should be able to address some of the issues in data management, storage and communication. such algorithms would also help with analysis provided they offer additional capabilities such as random access to any read and indexing for efficient sequence similarity search. here we present scalce, a boosting scheme based on locally consistent parsing technique, which reorganizes the reads in a way that results in a higher compression speed and compression rate, independent of the compression algorithm in use and without using a reference genome. results: our tests indicate that scalce can improve the compression rate achieved through gzip by a factor of 4.19when the goal is to compress the reads alone. in fact, on scalce reordered reads, gzip running time can improve by a factor of 15.06 on a standard pc with a single core and 6 gb memory. interestingly even the running time of scalce gzip improves that of gzip alone by a factor of 2.09. when compared with the recently published beetl, which aims to sort the (inverted) reads in lexicographic order for improving bzip2, scalce gzip provides up to 2.01 times better compression while improving the running time by a factor of 5.17. scalce also provides the option to compress the quality scores as well as the read names, in addition to the reads themselves. this is achieved by compressing the quality scores through order-3 arithmetic coding (ac) and the read names through gzip through the reordering scalce provides on the reads. this way, in comparison with gzip compression of the unordered fastq files (including reads, read names and quality scores), scalce (together with gzip and arithmetic encoding) can provide up to 3.34 improvement in the compression rate and 1.26 improvement in running time. availability: our algorithm, scalce (sequence compression algorithm using locally consistent encoding), is implemented in c with both gzip and bzip2 compression options. it also supports multithreading when gzip option is selected, and the pigz binary is available. it is available at http://scalce.sourceforge.net. contact:although the vast majority of high throughput sequencing (hts) data are compressed through general purpose methods, in particular gzip and its variants, the need for improved performance has recently lead to the development of a number of techniques specifically for hts data. available compression techniques for hts data either exploit (i) the similarity between the reads and a reference genome or (ii) the similarity between the reads themselves. once such similarities are established, each read is encoded by the use of techniques derived from classical lossless compression algorithms such as lempel-ziv-77 (which is the basis of gzip and all other zip formats) or lempel-ziv-78 . compression methods that exploit the similarity between individual reads and the reference genome use the reference genome as a dictionary and represent individual reads with a pointer to one mapping position in the reference genome, together with additional information about whether the read has some differences with the mapping loci. as a result, these methods require (i) the availability of a reference genome and (ii) mapping of the reads to the reference genome. unfortunately, genome mapping is a time-wise costly step, especially when compared with the actual execution of compression (i.e. encoding the reads) itself. furthermore, these methods necessitate the availability of a reference genome both for compression and decompression. finally, many large-scale sequencing projects such as the genome 10k project focus on species without reference genomes. compression methods that exploit the similarity between the reads themselves simply concatenate the reads to obtain a single sequence:apply modification of) and to whom correspondence should be addressed.we evaluated the performance of the scalce algorithm for boosting gzip on a single core 2.4 ghz intel xeon x5690 pc (with network storage and 6 gb of memory). we used four different datasets in our tests: (i) pseudomonas aeruginosa rna-seq library (51 bp, single lane), (ii) p.aeruginosa genomic sequence library (51 bp, single lane), (iii) whole genome shotgun sequencing (wgs) library generated from the genome of the hapmap individual na18507 (100 bp reads at 40 genome coverage) and (iv) a single lane from the same human wgs dataset corresponding to $1.22 genome coverage (sequence read archive id: srr034940). we removed any comments from name section (any string that appears after the first space). also the third row should contain a single character (/) separator character. the reads from each dataset were reordered through scalce and three separate files were obtained for (i) the reads themselves, (ii) the quality scores and (iii) the read names (each maintaining the same order). note that lcp reordering is useful primarily for compressing the reads themselves through gzip. the quality scores were compressed via the scheme described above. finally the read names were compressed through gzip as well. the compression rate and run time achieved by gzip software alone, only on the reads from the p.aeruginosa rna-seq library (dataset 1) is compared against those achieved by scalce followed by gzip in. the compression rates achieved by the gzip software alone in comparison with gzip following scalce on the combination of reads, quality scores and read names are presented in. the run times for the two schemes (again on reads, quality scores and read names all together) are presented in. when scalce is used with arithmetical coding of order 3 with lossless qualities, it boosts the compression rate of gzip between 1.42-and 2.13-fold (when applied to reads, quality scores and read names), significantly reducing the storage requirements for hts data. when arithmetical coding of order 3 is used with 30 loss without reducing the mapping accuracyimprovements in compression rate are between 1.86 and 3.34. in fact, the boosting factor can go up to 4.19 when compressing the reads only. moreover, the speed of the gzip compression step can be improved by a factor of 15.06. interestingly, the total run time for scalce gzip is less than the run time of gzip by a factor of 2.09. furthermore, users can tune the memory available to scalce through a parameter to improve the run time when a large main memory is available. in our tests, we limited the memory usage to 6 gb. note that our goal here is to devise a fast boosting method, scalce, which in combination with gzip gives compression. original (left) and transformed (right) quality scores for two random reads that are chosen from na18507 individual. the original scores show much variance, where the transformed quality scores are smoothened except for the peaks at local maxima, that help to improve the compression ratiorates much better than gzip alone. it is possible to get better compression rates through mapping-based strategies, but these methods are several orders of magnitude slower than scalce gzip. we tested the effects of the lossy compression schemes for the quality scores, used by scalce as well as cram tools, to single nucleotide polymorphism (snp) discovery. for that, we first mapped the na18507 wgs dataset with the original quality values to the human reference genome (grch37) using the bwa aligner , and called snps using the gatk software . we repeated the same exercise with the reads after 30 lossy transformation of the base pair qualities with scalce. note that the parameters for bwa and gatk we used in these experiments were exactly the same. we observed almost perfect correspondence between two experiments. in fact, 499:95 of the discovered snps were the same ; not surprisingly, most of the difference was due to snps in mapping to common repeats or segmental duplications. we then compared the differences of both snp callsets with dbsnp release 132 in. in addition, we carried out the same experiment with compressing/decompressing of the alignments with cram tools. as shown in, quality transformation of the cram tools introduced about 2.5 errors in snp calling (97.5 accuracy) with respect to the calls made for the original data (set as the gold standard). one interesting observation is that 70.7 of the new calls after scalce processing matched to entries in dbsnp where this ratio was only 62.75 for the new calls after cram tools quality transformation. moreover, 57.95 of the snps that scalce lost are found in dbsnp, and cram tools processing caused removal of 18.4 times more potentially real snps than scalce.as a final benchmark, we compared the performance of scalce with mapping-based reordering before gzip compression. we first mapped one lane of sequence data from the genome of na18507 (same as above) to human reference genome (grch37) using bwa , and sorted the mapped reads using samtools , and reconverted the map-sorted bam file back to fastq using) combined with bzip2 by a factor between 1.09 and 2.07, where running time is improved by a factor between 3.60 and 5.17 (see). scalce (on full fastq files) also outperforms dsrc compression ratio on complete fastq files by a factor between 1.09 and 1.18 (see).  
