repeat-and error-aware comparison of deletions motivation: the number of reported genetic variants is rapidly growing, empowered by ever faster accumulation of next-generation sequencing data. a major issue is comparability. standards that address the combined problem of inaccurately predicted breakpoints and repeat-induced ambiguities are missing. this decisively lowers the quality of consensus callsets and hampers the removal of duplicate entries in variant databases, which can have deleterious effects in downstream analyses. results: we introduce a sound framework for comparison of deletions that captures both tool-induced inaccuracies and repeat-induced ambiguities. we present a maximum matching algorithm that outputs virtual duplicates among two sets of predictions/annotations. we demonstrate that our approach is clearly superior over ad hoc criteria, like overlap, and that it can reduce the redundancy among callsets substantially. we also identify large amounts of duplicate entries in the database of genomic variants, which points out the immediate relevance of our approach. availability and implementation: implementation is open source and available from https://next-generation sequencing (ngs) technology has led to generation of big data also in biology. the rapid accumulation of ngs data has triggered the development of an equally overwhelming amount of tools for their exploration, including many tools for the prediction of structural variants, see the reviews by. an immediately arising, pressing concern are cross-genome and cross-project comparability of variant call sets (e.g. from the 1000 genomes project consortium, 2010; the genome of the). while resolution of related issues is instrumental for successful genetics research, there are only little statistically and algorithmically rigorous approaches addressing this. here we suggest a formal framework for identifying duplicate deletion calls. a major source of problems about ngs data formats and tool evaluation is the repetitiveness inherent to the majority of genomes . the resulting read mapping ambiguity can decisively hamper the exact allocation of structural variations (svs) as well as insertions and deletions (indels) within a genome. on top of that, another major source of prediction inaccuracies are the technical and theoretical, tool-specific limitations. these are often well-known. for instance, sequencing errors, ambiguities during the alignment of reads to a reference, or point mutations close to the breakpoints might cause split-read approaches to be inaccurate by a few bases. paired-end mapping approaches, on the other hand, infer the deletion size according to the difference of the expected fragment length and the resulting distance of the mapped reads. since the original fragment length is only known approximately, the deletion size predictions are less accurate than for split-read aligners. in addition, the location of the deletion needs to be confined to the region between the paired ends, which introduces further inaccuracies. see for examplefor details on the different kinds of approaches; popular split-read aligners are pindel and platypus , while breakdancer and clever are paired-end mapping approaches. as we will outline in the following, the simultaneous presence of repeats and (even slightly) inaccurate breakpoint predictions makes it difficult to compare two deletion calls, as done when merging call sets created by different tools (as e.g. in the recent consensus caller by) or when searching variant databases (e.g.). since indels and svs play important roles in cancer and many other diseases , and in general populations , alleviating such issues can make a highly beneficial contribution to the field.  
