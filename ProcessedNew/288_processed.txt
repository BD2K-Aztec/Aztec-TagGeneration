sequence analysis analyzing genome coverage profiles with applications to quality control in metagenomics motivation: genome coverage, the number of sequencing reads mapped to a position in a genome, is an insightful indicator of irregularities within sequencing experiments. while the average genome coverage is frequently used within algorithms in computational gen-omics, the complete information available in coverage profiles (i.e. histograms over all coverages) is currently not exploited to its full extent. thus, biases such as fragmented or erroneous reference gen-omes often remain unaccounted for. making this information accessible can improve the quality of sequencing experiments and quantitative analyses. results: we introduce a framework for fitting mixtures of probability distributions to genome coverage profiles. besides commonly used distributions, we introduce distributions tailored to account for common artifacts. the mixture models are iteratively fitted based on the expectation-maximization algorithm. we introduce use cases with focus on metagenomics and develop new analysis strategies to assess the validity of a reference genome with respect to (meta-) gen-omic read data. the framework is evaluated on simulated data as well as applied to a large-scale metagenomic study, for which we compute the validity of 75 microbial genomes. the results indicate that the choice and quality of reference genomes is vital for metagenomic analyses and that validation of coverage profiles is crucial to avoid incorrect conclusions. availability: the code is freely available and can be downloaded from http://sourceforge.net/projects/fitgcp/.genome coverage, the number of sequencing reads mapped to a specific position within a reference genome, contains valuable information about reference genome and the mapping process and is easily accessible. therefore, it is frequently consulted in bioinformatics analyses to improve decisions in algorithms or to provide meaningful information to the user. for instance, experimental design methods guide the experimentalist to achieve a specific average sequencing depth (i.e. genome coverage). after sequencing, the obtained reads can be mapped to a reference genome. quality control tools analyze the mapping data and report measures such as coverage information, mapping quality or error rate to the user. for example, qualimap visualizes the coverage profile and the coverage over the whole genome together with the gc content, which allows detecting biases in the sequencing process. if no reference genome is available, the reads can be assembled to complete genomes or at least longer contiguous sequences (contigs). the latter is nowadays possible for metagenomic data, i.e. datasets containing reads of many different species with different abundances. the assembler metavelvet uses the coverage information in the de bruijn graph to connect contigs of similar coverage, as they are more likely to belong to the same organism. in addition to these examples, local coverage information is also used for detecting copy number alterations in genomes (e.g.). despite these versatile applications of genome coverage, a vast amount of information commonly remains unused. most current methods either use the average coverage over a certain sequence or describe the coverage profile using single probability distributions such as the negative binomial or gamma distribution. yet, to the best of our knowledge, more complex models such as mixtures of distributions are not used to fit genome coverage profiles (gcps). here, we suggest that more complex models can improve current methods and can open doors for new analysis strategies. we see one application of complex coverage distribution models in metagenomics, where reference-based methods have become increasingly popular with the advent of high-throughput sequencing technologies . however, there are two major problems with reference genomes. first, the process of assembling and finishing reference genomes is time consuming and cumbersome and many reference genomes remain unfinished in the draft stage with varying qualities depending on the used sequencing technologies . draft genomes are typically a set of assembled contigs, where many contigs may be erroneous or, if assembled from metagenomic data, belong to different organisms. the second problem is of biological nature; evolution in the microbial world proceeds at high pace due to short replication times, and new subtypes or even species emerge perpetually. this causes different microbial species to have high genomic similarities. therefore, the coverage is generally far from homogenous when mapping metagenomic reads to a reference genome; describing it with a single uni-modal to whom correspondence should be addressed. distribution would not be appropriate. here, more complex models can have the power to disentangle and quantify different contributors to the genome coverage. in this article, we present a framework for fitting complex mixtures of probability distributions to gcps. we demonstrate in simulated experiments that the proposed framework can produce reliable and robust results and present a real data experiment, where we use our framework to reanalyze the data presented in a large-scale metagenomic study.we introduced gcps as a means to extract quantitative information from mapping data. by fitting mixtures of probabilitydistributions to the gcp, we obtain valuable information about the reference genomes and the mapping process, such as the fraction of the genome that could not be covered by reads or if there is more than one organism contributing to the coverage. this makes the proposed framework a powerful tool for the analysis of mapping data without restriction to the application. the introduced gdv score is a simple, yet powerful, measure for how well a reference genome fits to the mapped reads. especially in metagenomics, reference genomes are typically not required to fit perfectly to the data; nevertheless, the degree of divergence should not become too large. as one example, we observed a gdv score of 0.82 in the experiment in section 3.2, where we mapped e.coli reads to a s.flexneri genome. this illustrates a relatively high biological divergence between data and reference despite a high gdv score. we assessed gdv scores in a real metagenomic experiment conducted by qin et al. (2010) and observed surprisingly low scores for genomes that were originally considered to be present in the dataset; only 9 of 75 reference genomes achieved scores 40.8. this is an imposing example for high discrepancy between metagenomic data and reference genomes, which we presume to be a common challenge of metagenomic experiments. one of the major reasons might be the quality of the reference genomes: as microbes from metagenomic experiments are typically not cultivable, their genomes must be assembled from environmental samples, which is significantly more complicated and error prone than assembly from pure samples. in the experiment at hand, 37 of 75 reference genomes consisted of 4100 (up to 1700) separate contigs, only six genomes were one contiguous sequence. the framework proposed and applied in this work makes these flaws quantifiable. the first experiment showed that the iterative algorithm is able to fit complex mixtures of highly specialized probability distributions to gcps. the impact of the tail distributions became apparent, as they significantly reduced the fit error. the second experiment showed that quantities calculated on fitted gcps are robust toward influences of the average genome coverage. there, we observed stable estimates of the gdv score over a wide range of coverages, starting at average coverages below 0:2. although our method is robust toward the average coverage, it can be sensitive to the mapping parameters: more restrictive mapper settings typically yield a lower gdv score and a higher influence of the tail distributions. this has to be considered when comparing gdv scores over different experiments. the iterative algorithm encounters limitations in extreme cases, for example, when the average coverage is very low, but locally extremely high. this can be the case when a genome is not present in the data, but shares a gene with other highly abundant genomes. then, the algorithm may fail to fit the low-coverage distribution, as intended by the user, but tries to fit the extremely high noise contributions. in other cases, the standard start parameters are inappropriate, such that the algorithm ends up in a local probability maximum instead of fitting the distribution as intended. these problems demonstrate that visual inspection of the fit is necessary and this is supported by the framework. common strategies used for the em algorithm are also possible, such as the initialization with different or manually determined starting parameters.  
