ms-reduce: an ultrafast technique for reduction of big mass spectrometry data for high-throughput processing motivation: modern proteomics studies utilize high-throughput mass spectrometers which can produce data at an astonishing rate. these big mass spectrometry (ms) datasets can easily reach peta-scale level creating storage and analytic problems for large-scale systems biology studies. each spectrum consists of thousands of peaks which have to be processed to deduce the peptide. however, only a small percentage of peaks in a spectrum are useful for peptide deduction as most of the peaks are either noise or not useful for a given spectrum. this redundant processing of non-useful peaks is a bottleneck for streaming high-throughput processing of big ms data. one way to reduce the amount of computation required in a high-throughput environment is to eliminate non-useful peaks. existing noise removing algorithms are limited in their data-reduction capability and are compute intensive making them unsuitable for big data and high-throughput environments. in this paper we introduce a novel low-complexity technique based on classification, quantization and sampling of ms peaks. results: we present a novel data-reductive strategy for analysis of big ms data. our algorithm, called ms-reduce, is capable of eliminating noisy peaks as well as peaks that do not contribute to peptide deduction before any peptide deduction is attempted. our experiments have shown up to 100 speed up over existing state of the art noise elimination algorithms while maintaining comparable high quality matches. using our approach we were able to process a million spectra in just under an hour on a moderate server. availability and implementation: the developed tool and strategy has been made available to wider proteomics and parallel computing community and the code can be found at https://github. com/pcdslab/msreducemass spectrometry (ms) is an analytical chemistry technique which is used for determining the type and amount of constituents of a mixture. ms has found its application in the field of biomedical research. among all the applications of ms in biology and medicine protein identification and quantization has proved to be the most widely used. ms based proteomics is very frequently used for profiling of exosomes , toxicological screening (k, 2013), evolutionary biology and numerous other applications . wide variety of computational techniques such as estimation of false positive rates , protein quantification from large datasets , phosphopeptide filtering , phosphorylation site assignments , spectrum-to-peptide matching , and denovo peptide identification are required to make this ms data useful. with the introduction of modern mass spectrometers such as thermo orbitrap, thousands of spectra can be generated in just a single run of experiment . an ms2 spectrum consists of mass-to-charge ratio and associated intensities for each peak depicting their abundance in the sample under consideration. on an average total number of peaks for one spectrum may range up to 4000 and for 60k human proteins the number of distinct peaks that need to be compared is close to 240 million (assuming that there is no redundancy). this number is just for a single human proteome and with projects like peptide atlas the number of distinct human observations are close to 35 000 which makes the total number of peaks equal to 8:4 10 12. note that this number does not include other species, distinct experimental conditions or novel post-translational modifications which exponentially increases the number of peaks that needs to be processed. the current computational analysis techniques have not been designed for such massive datasets. the current peptide identification techniques (e.g.) assume that each peak that is encountered is useful in making peptide deductions. this leads to processing much more number of peaks than are necessary to make a peptide deduction . the processing of peaks that are noise and/or do not contribute in deduction of peptides makes the processing of these large datasets time consuming. we assert that in order to process big ms data we should be able to eliminate noisy peaks and the peaks that do not contribute to peptide deduction before an in-depth analysis of the spectra. this will clearly result in faster processing of the ms/ms spectra and will save overhead for peptide searches by reducing the number of peaks to be analyzed. only processing the peaks that are useful rather than performing intensive per-peakcomputations will result in tremendous time and space-advantages. to the best of authors knowledge there is no algorithm available which can perform the noise removal function without performing an in-depth analysis on spectra. further, we are not aware of any procedure that can eliminate non-noisy and yet non-essential peaks that do not contribute to peptide deduction. in this paper we introduce a novel algorithm, called msreduce, for ultrafast reduction of ms/ms data in pre-processing stage. the proposed algorithm is a low-complexity procedure based on random sampling, approximate classification and quantization making it highly scalable with increasing number of spectra. further, user defined reduction ratio makes it suitable for a variety and sizes of ms datasets. our experiments show peptide deduction accuracy of up to 95 with reduction in the data size of up to 70. our results also indicate that we are able to process 1 000 000 spectra in under 1 h on a sequential machine making it highly efficient for big datasets. comparable reduction tools took over 3 days for the same dataset on a similar machine.analysis of high-throughput ms based proteomics data is an essential task in systems biology. data from multiple experiments can scale from million to a billion spectra and this data volume can easily reach tera-to peta-byte level. the big data from modern mass spectrometers creates scaling problems for existing software designed for much smaller datasets. although these algorithms are useful for interpretation of simple spectra, the search and match routine becomes computationally intractable for complex peptides. the big data volume that one gets from these high-throughput machines is enormous and low scalability of conventional tools cannot keep up with the rate of data generation. hence dimensionality reduction techniques that can reduce the number of peaks that needs to be processed are essential for fast and efficient processing of ms data for system-wide studies. in this paper we presented a novel dimensionality reduction technique, called ms-reduce, for pre-processing big ms datasets. to our knowledge, the proposed strategy is first attempt at data reduction of ms data for high-throughput environments. our low-computational cost strategy is based on classification, quantization and sampling of ms data peaks. an approximate classification of spectra followed by a quantization step results in binning of peaks. each quantum of a spectrum contains peaks within a particular intensity range. then a random sampling step is performed on these bins to obtain the peaks which form the final reduced spectrum. our strategy is linear in time complexity with increasing number of spectra which is confirmed by our experiments. we also show that msreduce can process up to a million spectra in 47 min as compared to the de-noising algorithm, which processes the same number of spectra in about 3 days. we performed rigorous testing of the algorithm using experimental datasets and compared its performance with two of the existing algorithms. the implemented software will be available for free academic use at the authors webpages.  
