fulcrum: condensing redundant reads from high-throughput sequencing studies motivation: ultra-high-throughput sequencing produces duplicate and near-duplicate reads, which can consume computational resources in downstream applications. a tool that collapses such reads should reduce storage and assembly complications and costs. results: we developed fulcrum to collapse identical and near-identical illumina and 454 reads (such as those from pcr clones) into single error-corrected sequences; it can process paired-end as well as single-end reads. fulcrum is customizable and can be deployed on a single machine, a local network or a commercially available mapreduce cluster, and it has been optimized to maximize ease-of-use, cross-platform compatibility and future scalability. sequence datasets have been collapsed by up to 71, and the reduced number and improved quality of the resulting sequences allow assemblers to produce longer contigs while using less memory. availability and implementation: source code and a tutorial are available at http://pringlelab.stanford.edu/protocols.html under a bsd-like license. fulcrum was written and tested in python 2.6, and the single-machine and local-network modes depend on a modified version of the parallel python library (provided).ultra-high-throughput-sequencing (uhts) methods are being used both for resequencing projects and for de novo sequencing and assembly of both genomes and transcriptomes . during resequencing, reads can generally be aligned to specific locations in the previously sequenced genome. in contrast, de novo assembly often requires assemblers to take the input of many gigabases of sequence and return a much smaller result, ranging from tens of megabases for transcriptome assemblies to a few gigabases for many eukaryotic genome assemblies. some assemblers (e.g. velvet) store read information in ram during processing, limiting the amount of raw sequence that can be processed . uhts often produces reads that are exact duplicates of each other due to enzyme biases, pcr amplification, or (in transcriptomes) the presence of highly expressed sequences. in addition, genetic to whom correspondence should be addressed. polymorphisms, pcr errors and sequencing errors can result in near-duplicate reads, precluding the use of naive hashing algorithms to efficiently find and collapse all duplicates. collapsing identical and near-identical reads into single consensus reads with high quality scores can significantly reduce the physical memory and/or processing time required for assembly. previously described methods for finding identical and near-identical reads require large amounts of physical memory, long processing times or both . indeed, if the number of reads (n) is large (as is typically the case), performing a full n n comparison on a single computer is effectively precluded by the amount of processing time required. in addition, these methods were designed to process single-end reads and cannot process the increasingly common paired-end reads. in order to increase the number of reads that can be collapsed by the popular algorithm fastx-collapse , one could first preprocess the data with an error-correction program like shrec , hybrid-shrec or quake . however, shrec and hybrid-shrec used several gb of ram just to process relatively small microbial datasets, whereas quake required very large memories and extended times to process human datasets. this need for computational resources is expected, because these programs are designed to solve the difficult problem of error correction in non-duplicated reads. however, substantially fewer resources are required if error correction is only performed on near-duplicate reads, and even the collapse of near-duplicates can provide a significant performance advantage for subsequent assembly. in the past decade, n n comparisons performed over multiple networked computers have been used to process very large datasets, such as in page-ranking websites. mapreduce has been helpful in solving these problems because of its power, low cost and ease of use. in addition, investigators without extensive computational resources of their own can rent time from one of several commercial entities. because of its wide utility, we included support for mapreduce in fulcrum, a pipeline for collapsing identical and nearidentical reads that can be run on a single machine, a local network or a rented network of arbitrary size, depending on the demands of the project.  
