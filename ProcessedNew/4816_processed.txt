genome analysis using genome query language to uncover genetic variation motivation: with high-throughput dna sequencing costs dropping 5$1000 for human genomes, data storage, retrieval and analysis are the major bottlenecks in biological studies. to address the large-data challenges, we advocate a clean separation between the evidence collection and the inference in variant calling. we define and implement a genome query language (gql) that allows for the rapid collection of evidence needed for calling variants. results: we provide a number of cases to showcase the use of gql for complex evidence collection, such as the evidence for large structural variations. specifically, typical gql queries can be written in 510 lines of high-level code and search large datasets (100 gb) in minutes. we also demonstrate its complementarity with other variant calling tools. popular variant calling tools can achieve one order of magnitude speed-up by using gql to retrieve evidence. finally, we show how gql can be used to query and compare multiple datasets. by separating the evidence and inference for variant calling, it frees all variant detection tools from the data intensive evidence collection and focuses on statistical inference. availability: gql can be downloaded fromas sequencing costs drop, we envision a scenario where every individual is sequenced, perhaps multiple times in their lifetime. there is already a vast array of genomic information across various large-scale sequencing projects including the 1000 genome project (1000 genomes) and the cancer genome atlas (tcga) . in many of these projects, a re-sequencing strategy is applied in which whole genomes are sequenced redundantly with coverage between 4 and 40. the clone inserts ($500 bp) and sequenced reads ( 150 bp) are typically short and are not de novo assembled. instead, they are mapped back to a standard reference to decipher the genomic variation in the individual relative to the reference. even with advances in single-molecule sequencing and genomic assembly , we are many years away from having finished and error-free assembled sequences from human donors. at least in the near to mid-term, we expect that the bulk of sequencing will follow the resequencing/mapping/variant calling approach (e.g.). mapped reads (represented by bam files) from a single individual sequenced with 40 coverage are relatively inexpensive to generate, but they are storage intensive ($100gb). as sequencing becomes more accessible, and larger numbers of individuals are sequenced, the amount of information will increase rapidly; this will pose a serious challenge to available community resources. although raw archiving of large datasets is possible , the analysis of this huge amount of data remains a challenge. to facilitate access, some of the large datasets have been moved to commercially available cloud platforms. for example, the 1000 genome data are available on amazons ec2 (1000genomescloud, 2012). the genomes on amazon can be analyzed remotely using appropriate software frameworks like galaxy [that allow for the pipelining/integration of multiple analysis tools (, as well as tools like genome analysis toolkit (gatk) and samtools . the promise of this approach is that much of the analysis can be done remotely, without the need for extensive infrastructure on the users part. even with these developments, a significant challenge remains. each individual genome is unique, and the inference of variation, relative to a standard reference remains challenging. in addition to small indels and substitutions (the so-called single-nucleotide variations or snvs), an individual might have large structural changes, including, but not limited to, insertions, deletions, inversions , translocations of large segments (10 2 10 6 bp in size) , incorporation of novel viral and microbial elements and recombination-mediated rearrangements . further, many of these rearrangements may overlap leading to more complex structural variations. the detection of these variations remains challenging even for the simplest snvs, and there is little consensus on the best practices for the discovery of more complex rearrangements. for large, remotely located datasets, it is often difficult to create a fully customized analysis. it is often desirable to download the evidence (reads) required for the detection of variation to a local machine, and experiment with a collection of analysis tools for the actual inference. in that case, we are back again to the problem of building a large local infrastructure, including clusters and large disks, at each analysis site in addition to the resources in the cloud. as an example, we consider the use of paired-end sequencing and mapping (pem) for identifying structural variation. in to whom correspondence should be addressed. pem, fixed length inserts are selected for sequencing at both ends, and the sequenced sub-reads are mapped to a reference genome. without variation, the distance and orientation of the mapped reads match the a priori expectation. however, if a region is deleted in the donor relative to the reference, ends of the insert spanning the deleted region will map much further apart than expected. similarly, the expected orientation of the read alignments for illumina sequencing is (,). a (,) orientation is suggestive of an inversion event. using pem evidence, different callers still use different inference mechanisms. gasv arranges overlapping discordantly mapping pair-end reads on the cartesian plane and draws the grid of possible break point locations under the assumption that the discordancy is a result of a single sv. breakdancer finds all areas that contain at least two discordant pair-end reads, and it uses a poisson model to evaluate the probability that those areas contain a sv as a function of the total number of discordant reads of each of those areas. variationhunter reports that regions of sv are the ones that minimize the total number of clusters that the pair ends can form. given the complexity of the data, and the different inference methodologies, all of these methods have significant type 1 (false-positive), and type 2 (falsenegative) errors. further, as the authors of variationhunter point out, there are a number of confounding factors for discovery. for example, repetitive regions, sequencing errors, could all lead to incorrect mappings. at the same time, incorrect calls cannot be easily detected because tools need to be modified to re-examine the source data. in addition, the run time of the entire pipeline of those tools is not negligible given that they have to parse the raw data. a starting point of our work is the observation all tools follow a two-step procedure, implicitly or explicitly, for discovery of variation. the first stepthe evidence-stepinvolves the processing of raw data to fetch (say) the discordant pair-end reads; the second stepthe inference-stepinvolves statistical inference on the evidence to make a variant call. moreover, the evidence gathering step is similar and is typically the data-intensive part of the procedure. for example, in snv discovery, the evidence step is the alignment (pile-up) of nucleotides to a specific location, whereas the inference step involves snv estimation based on alignment quality and other factors. by contrast, for svs such as large deletions, the evidence might be in the form of (a) lengthdiscordant reads and (b) concordant reads mapping to a region; the inference might involve an analysis of the clustering of the length-discordant reads, and looking for copy-number decline and loss of heterozygosity in concordant reads. in this article, we propose a genome query language (gql) that allows for the efficient querying of genomic fragment data to uncover evidence for variation in the sampled genomes. note that our tool does not replace other variant calling tools, but it is complementary to existing efforts. it focuses on the collection of evidence that all inference tools can use to make custom inference. first, by providing a simple interface to extract the required evidence from the raw data stored in the cloud, gql can free callers from the need to handle large data efficiently. second, we show how existing tools can be sped up and simplified using gql, with larger speed-ups possible through a cloud based parallel gql implementation. third, analysts can examine and visualize the evidence for each variant, independent of the tool used to identify the variant. software layers and interfaces for genomics: we also place gql in the context of other genomics software. it is helpful to think of a layered, hourglass, model for genomic processing. at the bottom is the wide, instrument layer (customized for each instrument) for calling reads. this is followed by mapping/ compression layers (the narrow waist of the hourglass), and subsequently, multiple application layers. some of these layers have been standardized. many instruments now produce sequence data as fastq format, which in turn is mapped against a reference genome using alignment tools, such as bwa and maq ; further, aligned reads are often represented in the compressed, bam format that also allows random access. recently, more compressed alignment formats have come into vogue including slimgene cram and others as well as compression tools for unmapped reads . at the highest level, standards such as vcf (vcf) describe variants (the output of the inference stage of). in this context, we propose additional layering. specifically, we advocate the splitting of the processing below the application layer to support a query into an evidence layer(deterministic, large data movement, standardized) and an inference layer (probabilistic, comparatively smaller data movement, little agreement on techniques). for evidence gathering, the closest tools are samtools , bamtools), biohdf and gatk . samtools consists of a toolkit and an api for handling mapped reads; together, they comprise the first attempt to hide the implications of raw data handling by treating datasets uniformly regardless of the instrument source.samtools also provide quick random access to large files and provide a clean api to programmatically handle alignments. the tool combines index sorted bam files with a lightweight and extremely efficient binning that clusters reads that map in neighboring locations. thus, samtools can quickly return a set of reads that overlap with a particular location or create a pileup (i.e. all bases seen in reads that map to any reference locus). bamtools is a c api built to support queries in a json format. bedtools, closely aligned with samtools, allows intervalrelated queries through a clean unix and a python interface. although powerful, these tools still require programmer-level expertise to open binary files, assign buffers, read alignments and manipulate various fields. the gatk is built on top of samtools and reduces the programming complexity of data collection. gatks api provides two main iterator categories to an application programmer. the first iterator traverses individual reads; the second iterator walks through all genome loci, either individually or in adjacent groups. the toolkit, which is written based on the map reduce framework and thus easily parallelizable, is an excellent resource for developers of applications that need to determine local realignment, quality score re-calibration and genotyping . the support of many of these tools for looking at paired-ends, and consequently for structural variation, is limited, depending (in gatks case) on the existence of the optional fields rnext and pnext of the sam/bam alignments (gatk-pairend, 2012). the single biggest difference between our proposed tool, gql and others is that gql has a (sql-like) declarative syntax in its own language, as opposed to a procedural syntax, designed to help programmers rather than the end user. declarative languages, such as gql and sql, not only raise the level of abstraction of data access but also allow automatic data optimization without programmer intervention. by asking users to specify what data they want as opposed to how they want to retrieve it, we will show that gql can facilitate automatic optimizations, such as the use of indices and caching; these seem harder to support in other tools without explicit programmer directives. further, it is feasible to compile gql queries to a distributed, cloud based, back-end. finally, gql queries allow genomes to be browsed for variations of interest, allowing an interactive exploration of the data as we show in our results. although the ucsc browser also allows genome browsing, it does only by position or string, which we refer to as syntactic genome browsing. by contrast, gql allows browsing for all regions containing reads that satisfy a specified property (e.g. discrepant reads) and view the results on the ucsc browser. we refer to this as semantic genome browsing and give many examples in section 2. our performance results indicate that such browsing can be done interactively in minutes using a single cheap cpu.  
