data and text mining optimized data fusion for k-means laplacian clustering motivation: we propose a novel algorithm to combine multiple kernels and laplacians for clustering analysis. the new algorithm is formulated on a rayleigh quotient objective function and is solved as a bi-level alternating minimization procedure. using the proposed algorithm, the coefficients of kernels and laplacians can be optimized automatically. results: three variants of the algorithm are proposed. the performance is systematically validated on two real-life data fusion applications. the proposed optimized kernel laplacian clustering (oklc) algorithms perform significantly better than other methods. moreover, the coefficients of kernels and laplacians optimized by oklc show some correlation with the rank of performance of individual data source. though in our evaluation the k values are predefined, in practical studies, the optimal cluster number can be consistently estimated from the eigenspectrum of the combined kernel laplacian matrix. availability: the matlab code of algorithms implemented in this paper is downloadable fromclustering is a fundamental problem in unsupervised learning and a number of different algorithms and methods have emerged over the years. k-means (km) and spectral clustering are two popular methods for clustering analysis. k-means is proposed to cluster attribute-based data into k numbers of clusters with the minimal distortion . another well-known method, spectral clustering (sc) , is also widely adopted in many applications. unlike km, sc is specifically developed for graphs, where the data samples are represented as vertices connected by non-negatively weighted undirected edges. the problem of clustering on graphs belongs to whom correspondence should be addressed. present address: department of medicine, institute for genomics and systems biology, the university of chicago.to another paradigm than the algorithms based on the distortion measure. the goal of graph clustering is to find partitions on the graph such that the edges between different groups have a very low weight (von). to model this, different objective functions are adopted and the typical criteria include the ratiocut , the normalized cut and many others. to solve these objectives, the discrete constraint of the clustering indicators is usually relaxed to real values; thus, the approximated solution of spectral clustering can be obtained from the eigenspectrum of the graph laplacian matrix. many investigations (e.g.) have shown the connection between km and sc. moreover, in practical applications, the weighted similarity matrix is often used interchangeably as the kernel matrix in km or the adjacency matrix in sc. recently, a new algorithm, kernel laplacian (kl) clustering , is proposed to combine a kernel and a laplacian simultaneously in clustering analysis . this method combines the objectives of km and sc in a quotient trace maximization form and solves the problem by eigen-decomposition. kl is shown to empirically outperform km and sc on real datasets. this straightforward idea is useful to solve many practical problems, especially those pertaining to combine attribute-based data with interaction-based networks. for example, in web analysis and scientometrics, the combination of text mining and bibliometrics has become a standard approach in clustering science or technology fields toward the detection of emerging fields or hot topics . in bioinformatics, proteinprotein interaction network and expression data are two of the most important sources used to reveal the relevance of genes and proteins with complex diseases. conventionally, the data are often transformed into similarity matrices or interaction graphs, then consequently clustered by km or sc. in kl, the similarity-based kernel matrix and the interactionbased laplacian matrix are combined, which provides a novel approach to combine heterogeneous data structures in clustering analysis. our preliminary experiments show that when using kl to combine a single kernel and a single laplacian, its performance strongly depends on the quality of the kernel and the laplacian, which results in a model selection problem to determine the optimal settings of the kernel and the laplacian. to perform model selection on unlabeled data is non-trivial because it is difficult to evaluate the models. to tackle the new problem, we propose a novel algorithm to incorporate multiple kernels and laplacians in kl clustering. our recent work proposes a method to integrate multiple kernel matricesclustering, submitted for publication). the main contribution of the present work lies in the additive combination of multiple kernels and laplacians; moreover, the coefficients assigned to the kernels and the laplacians are optimized automatically. this article presents the mathematical derivations of the additive integration form of kernels and laplacians. the optimization of coefficients and clustering are achieved via a solution based on bi-level alternating minimization . we validate the proposed algorithm on heterogeneous datasets taken from two real applications, where the advantage and reliability of the proposed method are systematically compared and demonstrated.we implement the proposed oklc models to integrate multiple kernels and laplacians on disease data and journal set data.to compare the performance, we also apply six popular ensemble clustering methods mentioned in relevant work to combine the partitions of individual kernels and laplacians as a consolidated partition. these six methods are cspa , hgpa , mcla , qmi , eacal and adacvote . as shown in tables 1 and 2, the performance of oklc algorithms is better than all the compared methods and the improvement is significant. on disease data, the best performance is obtained by oklc model 1, which uses sparse coefficients to combine nine text mining kernels and nine laplacians to identify disease-relevant clusters (ari: 0.5859, nmi: 0.7451). on journal data, all three oklc models perform comparably well. the best one seems coming from oklc model 3 (ari: 0.7336, nmi: 0.7758), which optimizes the non-sparse coefficients on the four kernels and four laplacians. to evaluate whether the combination of kernel and laplacian indeed improve the clustering performance, we first systematically compared the performance of all the individual data sources using km and sc. as shown in supplementary material 4, on disease data, the best km performance (ari 0.5441, nmi 0.7099) and sc (ari 0.5199, nmi 0.6858) performance are obtained on lddb text mining profile. next, we enumerate all the paired combinations of a single kernel and a single laplacian for clustering. the integration is based on equation (12) and the value is set to 0.5 so the objectives of km and sc are combined averagely. the performance of all 45 paired combinations is presented in supplementary material 5. as shown, the best kl clustering performance is obtained by integrating the lddb kernel with ko laplacian (ari 0.5298, nmi 0.6949). moreover, we also found that the integration performance varies significantly by the choice of kernel and laplacian, which proves our previous point that the kl performance is highly dependent on the quality of kernel and laplacian. using the proposed oklc algorithm, there is no need to enumerate all the possible paired combinations. oklc combines all the kernels and laplacians and optimizes their coefficients in parallel, yielding a comparable performance with the best paired combination of a single kernel and a single laplacian. in, two confusion matrices of disease data for a single run are depicted. the values on the matrices are normalized according to r ij = c j /t i , where t i is the total number of genes belonging in disease i and c j is the number of these t i genes that were clustered to belong to class j. first, it is worth noting that oklc reduces the number of misclustered genes on breast cancer (nr.1), cardiomyopathy (nr.2) and muscular dystrophy (nr.11). among the misclustered genes in lddb, five genes (tsg101, dbc1, cttn, slc22a18, ar) in breast cancer, two genes in cardiomyopathy (cox15, csrp3) and two genes in muscular dystrophy (sepn1, col6a3) are correctly clustered in oklc model 1. second, there are several diseases where consistent misclustering occurs in both methods, such as diabetes (nr.6) and neuropathy (nr.12). the intuitive confusion matrices correspond to the numerical evaluation results; as shown, the quality of clustering obtained by oklc model 1 (ari = 0.5898, nmi = 0.7429) is higher than lddb. the performance of individual data sources of journal data is shown in supplementary material 6. the best km (ari 0.6482, nmi 0.7104) is obtained on the idf kernel and the best sc (ari 0.5667, nmi 0.6807) is obtained on the cross-citation laplacian. to combine the four kernels with four laplacians, we evaluate all the 10 paired combinations and show the performance in supplementary material 7. the best performance is obtained by integrating the idf kernel with the cross-citation laplacian (ari 0.7566, nmi 0.7702). as shown, the integration of lexical similarity information and citation-based laplacian indeed improves the performance. in, the confusion matrices (also normalized) of journal data for a single run are illustrated. we compare the best individual data source (idf with kernel km, figure on the left) with the oklc model 1. in the confusion matrix of idf km, 79 journals belonging to agriculture science (nr.1) are misclustered to environment ecology (nr.3), 9 journals are misclustered to pharmacology and toxicology (nr.7). in oklc, the number of agriculture journals misclustered to environment ecology is reduced to 45, and the number to pharmacology and toxicology is reduced to 5. on other journal clusters, the performance of the two models is almost equivalent. we also investigated the performance of combining only multiple kernels or multiple laplacians. on the disease dataset, we combined the nine kernels and the nine laplacians for clustering, respectively, using all the compared methods inbetween the ranks of weights and the ranks of performance on both datasets. the correlations of disease kernels, disease laplacians, journal kernels and journal laplacians are, respectively, 0.5657, 0.6, 0.8 and 0.4. in some relevant work, the average spearman correlations are mostly around 0.4 . therefore, the optimal weights obtained in our experiments are generally consistent with the rank of performance. as a spectral clustering algorithm, the optimal cluster number of oklc can be estimated by checking the plot of eigenvalues (von). to demonstrate this, we investigated the dominant eigenvalues of the optimized combination of kernels and laplacians. in, we compare the difference of three oklc models with the pre-defined k (set as equal to the number of class labels). in practical research, one can predict the optimal cluster number by checking the elbow of the eigenvalue plot. as shown in, the elbow in disease data is quite obvious at the number of 14. in journal data, the elbow is more likely to range from 6 to 12. all the three oklc models show a similar trend on the eigenvalue plot. moreover, in supplementary material 9 we also compare the eigenvalue curves using different k values as input. as shown, the eigenvalue plot is quite stable with respect to the different inputs of k, which means the optimized kernel and laplacian coefficients are quite independent with the k value. this advantage enables a reliable prediction about the optimal cluster number by integrating multiple data sources. to investigate the computational time, we benchmark oklc algorithms with other clustering methods on the two datasets. as shown in, when optimizing the coefficients, oklc algorithm (models 1 and 3) spends longer time than the other methods to optimize the coefficients on the laplacians andthe kernels. however, the proposed algorithm is still efficient. considering the fact that the proposed algorithm yields much better performance and more enriched information (the ranking of the individual sources) than other methods, it is worth spending extra computational complexity on a promising algorithm.the reported values are averaged from 20 repetitions. the cpu time is evaluated on matlab v7.6.0 + windows xp2 installed on a laptop computer with intel core 2 duo 2.26 ghz and 2 g memory. and different experimental settings. the proposed oklc algorithms perform significantly better than other methods. moreover, the coefficients of kernels and laplacians optimized by oklc show strong correlation with the rank of performance of individual data source. though in our evaluation the k values are predefined, in practical studies, the optimal cluster number can be consistently estimated from the eigenspectrum of the combined kernel laplacian matrix. the proposed oklc algorithm demonstrates the advantage of combining and leveraging information from heterogeneous data structures and sources. it is potentially useful in bioinformatics and many other application areas, where there is a surge of interest to integrate similarity-based information and interaction-based relationships in statistical analysis and machine learning.  
