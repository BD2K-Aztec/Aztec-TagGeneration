genome analysis scoring-and-unfolding trimmed tree assembler: concepts, constructs and comparisons motivation: mired by its connection to a well-known n p-complete combinatorial optimization problemnamely, the shortest common superstring problem (scsp)historically, the whole-genome sequence assembly (wgsa) problem has been assumed to be amenable only to greedy and heuristic methods. by placing efficiency as their first priority, these methods opted to rely only on local searches, and are thus inherently approximate, ambiguous or error prone, especially, for genomes with complex structures. furthermore, since choice of the best heuristics depended critically on the properties of (e.g. errors in) the input data and the available long range information, these approaches hindered designing an error free wgsa pipeline. results: we dispense with the idea of limiting the solutions to just the approximated ones, and instead favor an approach that could potentially lead to an exhaustive (exponential-time) search of all possible layouts. its computational complexity thus must be tamed through a constrained search (branch-and-bound) and quick identification and pruning of implausible overlays. for his purpose, such a method necessarily relies on a set of score functions (oracles) that can combine different structural properties (e.g. transitivity, coverage, physical maps, etc.). we give a detailed description of this novel assembly framework, referred to as scoring-and-unfolding trimmed tree assembler (sutta), and present experimental results on several bacterial genomes using next-generation sequencing technology data. we also report experimental evidence that the assembly quality strongly depends on the choice of the minimum overlap parameter k.since the pioneering work of frederick sanger in 1975, when he developed the basic dna sequencing technology, sanger chemistry has continued to be employed widely in to whom correspondence should be addressed. practically all large-scale genome projects. however, whole-genome sequence assembly (wgsa) pipelines in these projects have usually resorted to the shotgun sequencing strategy in order to reconstruct a genome sequence, despite the limitation that sanger chemistry could only generate moderate-sized reads (around 1000 bp) with no location information. while many recent advances in sequencing technology has yielded higher throughput and lower cost, the limitations imposed by the read lengths (ranging between 35 and 500 bp) still plague the genomics science, forcing it to work with draft-quality, unfinished, genotypic and misassembled genomic data. the problem is, however, complicated by the presence of haplotypic ambiguities, base-calling errors and repetitive genomic sections. recall that to obtain the input read data, the dna polymer is first sheared into a large number of small fragments; and then either the entire fragment or just its ends are sequenced. the resulting sequences are then combined into a consensus sequence using a computer program: dna sequence assembler. it is desired that the consensus has as small a base-level discrepancy with respect to the original dna polymer as possible. researchers first approximated the shotgun sequence assembly problem as one of finding the shortest common superstring of a set of sequences . although this was an elegant theoretical abstraction, it was oblivious to what biology needs to make correct interpretation of genomic data. in fact, it misses the correct model for the assembly problem for at least three different reasons: (i) it does not model possible errors arising from sequencing the fragments; (ii) it does not model fragment orientation (the sequence source can be one of the two dna strands); (iii) most importantly it fails in the presence of repeats in the genome. faced with this theoretical computational intractability (n p-complete), most of the practical approaches for genome sequence assembly were devised to use greedy and heuristic methods that, by definition, restrict themselves to find suboptimal solutions (see). note that if the dna was totally random then the overlap information would be sufficient to reassemble the target sequence and greedy algorithms would perform always well . however, this argument is mostly irrelevant, since the problem is complicated by the presence of various non-random structures, in particular in eukaryotic genomes (e.g. repeated regions, rearrangements, segmental duplications). in the case of human genome, initially two unfinished draft sequences were produced by different methods, one by the international human genome sequencing consortium (ihgsc) and another by celera genomics (cg), with the published ihgsc assembly constructed by the program gigassembler devised atwe have compared sutta to several well-known short read assemblers on three real datasets from illumina next-generation sequencing data using both mated and unmated reads. the following assemblers are used in the comparison: edena 2.1.1 , velvet 1.0.13 , taipan 1.0 , abyss 1.2.3 , ssake 3.6 and euler-sr 1.1.2 . although these datasets do not represent the state of the art in sequencing technology (for example, illumina can currently generate longer reads up to 100 bp), they have been extensively analyzed by previously published short read assemblers.). the experimental results show that sutta has comparable performance to the best state-of-the-art assemblers based on contig size comparison. this comparison is to be interpreted in the context of our experimental evidence that the choice of the minimum overlap parameter k affects both contig size and assembly quality (presented below, see figs 6 and 7).and 2 present the comparison based on contig size analysis for all three genomes. only contigs of minimal length 100 bp are considered in the statistics. a contig is classified as correct if it fully aligns to the genome with a minimum base similarity of 98 (for s.aures and h.acininychis) and 95 (for e.coli). inspecting the results in, it is evident that sutta performs comparatively well relative to these assemblers. in particular, sutta a , thanks to its aggressive strategy, assembles longer contigs but it pays in assembly quality by generating more misassembly errors. sutta c instead behaves more conservatively and generated less errors but without excessively sacrificing contig length. the choice between the aggressive strategy and the conservative one is clearly based on the overall quality of the input set of reads and the genome structure. for example, in the case of an error-free dataset and a genome with few and short repeats, we may opt for an aggressive strategy. in the page: 159 153160 suttacase of mate-pair data, sutta produces shorter contigs compared with abyss, velvet and euler-sr; however, suttas overall assembly quality is superior with fewer and shorter misassembled contigs.sequence assembly accuracy has now become particularly important in: (i) genome-wide association studies, (ii) detecting new polymorphisms and (iii) finding rare and de novo mutations. new-sequencing technologies have reduced cost and increased the throughput; however, they have sacrificed read length and accuracy by allowing more single nucleotide (base-calling) and indel (e.g. due to homo-polymer) errors. overcoming these difficulties without paying for high computational cost requires (i) better algorithmic framework (not greedy), (ii) being able to adapt to new and combined hybrid technologies (allowing significantly large coverage and auxiliary long-rage information) and (iii) prudent experiment design. we have presented a novel assembly algorithm, sutta, that has been designed to satisfy these goals as it exploits many new algorithmic ideas. challenging the popular intuition, sutta enables fast global optimization of the wgsa problem by taming the complexity using the b&b method. because of the generality of the proposed approach, sutta has the potential to adapt to future sequencing technologies without major changes to its infrastructure: technology-dependent features can be encapsulated into the lookahead procedure and well-chosen score functions.  
