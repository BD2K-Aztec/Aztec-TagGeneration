databases and ontologies the amordad database engine for metagenomics motivation: several technical challenges in metagenomic data analysis , including assembling metagenomic sequence data or identifying operational taxonomic units, are both significant and well known. these forms of analysis are increasingly cited as conceptually flawed, given the extreme variation within traditionally defined species and rampant horizontal gene transfer. furthermore, computational requirements of such analysis have hindered content-based organization of metagenomic data at large scale. results: in this article, we introduce the amordad database engine for alignment-free, content-based indexing of metagenomic datasets. amordad places the metagenome comparison problem in a geometric context, and uses an indexing strategy that combines random hashing with a regular nearest neighbor graph. this framework allows refinement of the database over time by continual application of random hash functions, with the effect of each hash function encoded in the nearest neighbor graph. this eliminates the need to explicitly maintain the hash functions in order for query efficiency to benefit from the accumulated randomness. results on real and simulated data show that amordad can support logarithmic query time for identifying similar metagenomes even as the database size reaches into the millions. availability and implementation: source code, licensed under the gnu general public license (version 3) is freely available for download from http://smithlabresearch.org/amordadmetagenomicshasrevolutionizedourknowledgeofmicrobialcommunities. such studies have uncovered intercommunity rules governingthebehaviorofthemicrobialnetworksandvariousaspectsof their symbiotic or parasitic lives within their host ecosystems . with technological advancements on the horizon, whole-metagenome shotgun (wms) sequencing has increasingly become popular. the results of many wms projects ranging from direct study of simple biofilms in specific niches to complicated microbial consortia like human gut microbiota are now publicly accessible. the first phase of the human microbiome project provided an unprecedented opportunity to explore our other genome . however, making sense of these data has proven tremendously challenging. these challenges stem from both the extreme size and number of possible metagenomes, which may be regarded as continuous mixtures of species whose boundaries can be poorly defined. popular existing frameworks such as mg-rast and megan rely heavily on alignment-based algorithms to analyze metagenomic data. assembly of raw reads into longer contigs, binning of sequencing reads and homology searching to characterize the predicted operational taxonomic units (otus) are among tasks that require sequence alignment in such frameworks . the computational requirements of these tasks, however, have reduced their usefulness in large-scale projects. alignment-free approaches have been successfully applied to overcome the scalability problems associated with alignment and assembly . a large class of alignment-free algorithms rely on the the frequency of oligonucleotides of fixed length k, called k-mers, to represent biological sequences . in metagenomics, several k-merbased algorithms have been proposed mostly targeting the phylogenetic characterization of the metagenomes . the idea is to estimate the abundance level of different bacterial families in a metagenome by assigning accurate phylogenetic labels to its reads or contigs. we introduce amordad, a content-based database engine for metagenomics designed to support rapid indexing and retrieval even as data volumes reach massive scales. in the most basic form, the user makes a query by providing a metagenomic sample sequence and asks for the most similar metagenomes in the database. similarity between two metagenomes is computed, and query responses are ranked according to similarity with the query. we describe a procedure for assigning an empirical significance score to query responses in this context, which provides additional insight about retrieved metagenomes. proximity scores also allow, for a given query, efficient identification of the full set of metagenomes sharing some threshold similarity with the query. amordad does not attempt any sequence alignment or assembly, and is agnostic of otus, focusing on the raw metagenome itself as the fundamental data element. indexing is based on feature vectors, specifically k-mer sequences. in the next section, we describe the combination of random hashing and regular nearest neighbor graph strategies, which enable low complexity queries, both in terms of time and memory usage. together, these dual index structures allow the database to reorganize itself continually as new data is added, an essential property that is difficult to achieve in highdimensional geometric databases. we demonstrate the efficiency of amordad in handling up to millions of metagenomes. to whom correspondence should be addressedwe have two objectives in studying the performance of amordad: (i) investigating its potential for identifyingwe presented amordad, a database engine for whole-metagenome sequencing data. amordad uses alignment-free principles, representing metagenomes as points in a high-dimensional geometric space. lsh is used to efficiently index the database points. to improve accuracy and efficiency beyond what is practical from lsh alone, amordad augments this indexing with a regular nearest neighbor graph. the randomness in amordad is continually refreshed by resampling new random hash functions. although only a fixed number of hash functions is alive within amordad at any given time, those that are extinct have contributed to optimizing connectivity in the graph, and thus continue to assist queries even if they are no longer used for hashing. results from a series of experiments have demonstrated this approach to have a significant effect on query efficiency and accuracy. the ability to cope with the rapid accumulation of data was a central design goal. lsh is a well-known approach to index high-dimensional data, but it is also a randomized method. a major drawback is the number of hash tables one must maintain to provide guarantees on the accuracy of queries, imposing time and space constraints. this is a well-known problem, and solutions have been proposed to overcome this pitfall . these solutions mostly rely on the fact that each hash function (i.e. each bit in our scheme) provides a ranking based on the proximity of points to a certain query. therefore, even if the closest neighbor is not hashed to the same bucket as the query, it is highly probable that buckets close to the query bucket contain the nearest neighbor. consequently, one may avoid generating many hash tables at the cost of checking more buckets from each table. this clever strategy has a direct effect on the indexing memory consumption . however, as the distance between the query and its nearest neighbors increases, there is a rapid growth in the number of buckets that must be checked. this presents a challenge in our applications because of the inherent diversity of microbial communities in many circumstances. in fact, if we view a set of biologically related metagenomes as a cluster in high-dimensional space, the diameter of some complex clusters like human gut microbiota , might be large enough to make the process of searching close buckets to the query bucket time-consuming. our approach, augmenting the lsh indexing with a graph structure, avoids having to explicity search additional buckets, and depends on transitivity of relationships encoded in the graph. intuitively, if the underlying distance measure between metagenomes satisfies the triangle inequality, the graph should guide the search in the most appropriate directions. additionally, the lsh provides a means of streamlining the database maintenance process. hash tables are maintained in a fixed size queue that is continually updated, and each new hash table contributes to refining edges in the graph. as a consequence, we are able to keep the graph in a near ideal state without requiring explicit operations to maintain the graph after metagenomes are added to (or removed from) the database. this design decision acknowledges that in the near future large (and distributed) metagenome databases will be updated frequently, and efficiency of queries will be more important to users of amordad.it is clear that many applications of amordad will involve maintaining large metagenomic data clusters so that new metagenomes can be understood through their neighborhoods in the graph. from this perspective, the graph becomes the primary structure, and the lsh serves simply to find the right neighborhood without exhaustive search. finally, we address some limitations of this work. we represent each metagenome by a feature vector that included the frequencies of all k-mers for a fixed k. we did not use feature selection, and were bound to relatively small values of k. many of the k-mers were almost certainly irrelevant in our experiments on real data. more flexibility could be obtained if a metagenome is represented by a bag of features composed of a variety of words (i.e. subsets of k-mers for varying k). from statistical point of view, this is equivalent to shifting the paradigm from a full markov chain of order k toward a variable length markov chain (b uhlmann and wyner, 1999). the main challenge is to find a parsimonious algorithm for feature extraction at large scale. more important than irrelevant features, however, are features representing technical artifacts, for example, of the sequencing experiments.  
