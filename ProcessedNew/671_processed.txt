bayesian evolutionary model testing in the phylogenomics era: matching model complexity with computational efficiency motivation: the advent of new sequencing technologies has led to increasing amounts of data being available to perform phylogenetic analyses, with genomic data giving rise to the field of phylogenomics. high-performance computing is becoming an indispensable research tool to fit complex evolutionary models, which take into account specific genomic properties, to large datasets. here, we perform an extensive bayesian phylogenetic model selection study, comparing codon and nucleotide substitution models, including codon position partitioning for nucleotide data as well gene-specific substitution models for both data types. for the best fitting partitioned models, we also compare independent partitioning with standard diffuse prior specification to conditional partitioning via hierarchical prior specification. to compare the different models, we use state-of-the-art marginal likelihood estimation techniques, including path sampling and stepping-stone sampling. results: we show that a full codon model best describes the features of a whole mitochondrial genome dataset, consisting of 12 protein-coding genes, but only when each gene is allowed to evolve under a separate codon model. however, when using hierarchical prior specification for the partition-specific parameters instead of independent diffuse priors, codon position partitioned nucleotide models can still outperform standard codon models. we demonstrate the feasibility of fitting such a combination of complex models using the beagle library for beast in combination with recent graphics cards. we argue that development and use of such models needs to be accompanied by state-of-the-art marginal likelihood estimators because the more traditional and computationally less demanding estimators do not offer adequate accuracy. contact:the startling advances in sequencing technology have led to a dramatic increase in the scale and ambition of phylogenetic analyses. as more and more complete genomes are sequenced, phylogenetics is entering a new erathat of phylogenomics, which uses phylogenetic principles to extract information from genomic data . until recently, molecular phylogenies based on a single or few orthologous genes often yielded contradictory results . one branch of the expanding field of phylogenomics aims to reconstruct the evolutionary history of organisms on the basis of their genomes, as opposed to performing single-gene studies, which have long dominated the field . by expanding the number of characters that can be used in phylogenetic reconstruction from a few thousand to tens of thousands, access to genomic data could potentially alleviate sampling problems that hampered previous phylogenetic analyses. phylogenomic analyses involve estimating the underlying evolutionary history of sequences either as an intermediate goal or as an end point. statistical phylogenetics provides a framework for estimating historical patterns, inferring intrinsic parameters of evolutionary processes, and testing hypotheses under the auspices of the neutral theory of molecular evolution . in contrast to the few genes that were previously available, the complete genomes of many species that are now accessible for inferring evolutionary relationships constitute large quantities of data that lead to reduced estimation errors associated with site sampling, to very high power in the rejection of simple evolutionary hypotheses and to high confidence in estimated phylogenetic patterns. traditionally, phylogenetic analyses based on many genes combined data into a contiguous block, a practice that is still commonly used today. under this concatenated model, all genes are not only assumed to evolve under the same tree but also to evolve at the same rate. although multigene datasets have the advantage of providing greater resolutionwith more information it is likely to find trees that more accurately reflect evolutionary historyit may prove challenging to account for the heterogeneous nature of the data. different genes undergo different selective pressures, and the degree of site rate heterogeneity may vary from gene to gene . likelihood calculations on trees have been shown to clearly benefit from accommodating different evolutionary pressures (as observed in different codon positions, or different genes) in heterogeneous data . selection is a key evolutionary process in shaping genetic diversity and a major focus of phylogenomics investigations . using statistical methods in evolutionary genetics, researchers frequently evaluate the strength of selection operating on genes or even individual codons in the entire phylogeny or in a subset of branches. codon substitution models have been particularly useful for this purpose because they allow estimating the ratio of non-synonymous and synonymous substitution rates (dn/ds) in a phylogenetic framework. two different versions of codon substitution models were simultaneously introduced that both allowed estimating a single dn/ds ratio across all sites and branches . various extensions have since been proposed, such as codon models that model variation in dn/ds among sites complemented with empirical bayes approaches to identify the sites under specific selection regimes . codon substitution models can to some extent be approximated by partitioning nucleotide models according to codon positions, which accommodates differences in evolutionary dynamics at the three codon positions. for example, yang (1996a, b) takes into account the nucleotide frequency bias, the substitution rate bias and the difference in the extent of rate variation among the three codon positions and shows that incorporating these features can yield drastically different divergence time estimates compared with models not incorporating this complexity. although full codon models approximate biological reality more closely, codon position partitioned nucleotide models have the advantage to be far more computationally efficient. while yang (1996a, b) only used the hky evolutionary model in his analyses,also included the gtr model (tavare,tavare, 1986) in a comprehensive model evaluation study (see also).showed that codon position partitioned nucleotide models are biologically motivated, computationally practical alternatives to codon models for the analysis of protein-coding sequences. it is therefore not surprising that such approaches are also being exploited for detecting positively selected sites . the large state space of full codon substitution models renders these models computationally expensive compared with standard nucleotide substitution models, which explains why they are frequently fit to trees to scrutinize selection processes but generally not used to reconstruct phylogenies. however, as computational power has increased, phylogenetic inference using codon-based models is becoming more and more realistic. here, we examine whether the use of full codon models is feasible in the phylogenomics era by fitting several codon models to a full genome mitochondrial dataset in a bayesian framework. we use state-of-the-art model-selection approaches to assess whether increased biological realism, as obtained by modelling gene-specific properties, goes hand in hand with increased model performance. finally, we provide estimates of computation time required on both multi-core cpu systems and a system equipped with one of the latest graphics cards available.we first compared the standard hky and gtr nucleotide models, assuming varying rates across sites [modelled using a discrete gamma distribution with four rate categories (yang, 1996a, b)], with the hky-based and gtr-based codon position partitioned nucleotide models analysed in the work ofand the codon model of goldman and yang (1994), also accommodating varying substitution rates across codons. for each of the models, the marginal likelihoods were calculated assuming both a strict clock and a uncorrelated relaxed clock with an underlying lognormal distribution (ucld) . the results of this model comparison are listed in. we consider the common codon partition (cp) models where all three codon positions are considered separately (denoted cp 123 ) and where the first and second codon position are grouped together (denoted cp 112 ), as the third codon positions generally evolve much faster than the first and the second codon positions. none of the marginal likelihood estimators inselects the gy94 codon model as the best-fitting mode, indicating that increased biological realism offered by explicitly model substitution in codon space is not reflected in an increased model fit. the model that seems to most adequately capture the substitution complexity in the data is a full fledged codon position partitioned nucleotide model, which assumes a separate gtr model for each codon position, as well as different rate heterogeneity patterns and nucleotide frequency compositions across codon positions (the partitioning also specifies different relative rates for the codon positions). the various estimators consistently select that model as the best model, with apparently only small differences in the ranking of the models. we note, however, that the hme and shme yield drastically different estimates for the log marginal likelihood compared with ps and ss, which supports earlier claims that such posterior-based estimators overestimate the marginal likelihood . the gy94 codon model ranks among the various codon position partitioned nucleotide models, albeit in different parts of the overall ranking, depending on the estimator used. standard nucleotide models, with or without varying rates across lineages, appear to be too simplistic for this dataset, as established by all the model-selection approaches. for each of the models tested, a relaxed molecular clock (with underlying lognormal distribution; ucld) is consistently shown to outperform a strict clock. next, we introduce gene-specific partitioning for the models tested in. to this purpose, we assume one model instance for each of the 12 genes present in the dataset, resulting for example in 12 gy94 codon models being estimated and 36 gtr models being estimated for the most parameter-rich codon position partitioned models, along with 36 gamma distributions being estimated and 36 sets of empirical frequencies being used. for each of these models, we have calculated marginal likelihoods assuming both a strict and a relaxed clock .note: posterior-based model selection approaches (i.e. hme and shme) were run for 10 million iterations (of which 1 million serve as burn-in; 2 million for the codon models), while ps and ss were run for 64 path steps/ratios with 250 000 iterations (25 000 iterations burn-in) per path step/ratio after an initial 1 million iterations that serve as burn-in (2 million for the codon models). rescaling in beagle was set to default (delayed).the gene-specific partitioning tested inyields a different model ranking compared with. whereas a gene-independent codon model was outperformed by its codon position partitioned nucleotide approximation, this is no longer the case when gene-specific evolutionary patterns are taken into account. the gene-specific codon model that allows for different (relative) rates between the different genes, with a different gamma distribution to model varying rates across sites within each gene, achieves the best performance as assessed by both ps and ss. however, the posterior-based model-selection approaches (hme and shme) fail to detect this, and also other subtle differences can be observed for these estimators when comparing various versions of the gene-specific codon position partitioned nucleotide models in. the strong tendency to overestimate marginal likelihoods by the hme is also reflected inreveals that each of the models analysed benefits from taking into account gene-specific properties of the dataset studied, i.e. that it is composed of 12 protein-coding genes (seefor gene-specific estimates of the key parameters of the codon model). one gene that stands out from all the others is the atp8 gene, exhibiting a dn/ds ratio that is between 3 to 15 times higher than that of the other genes and the lowest transition/transversion ratio of the genes considered. further, the atp8 gene is the only gene with a rate heterogeneity parameter lower than 1, implying that most sites have very low substitution rates (yang, 1996a, b), but few sites also have high rates, whereas all the other genes have on average more intermediate rates across sites. the cox1, cox2, cox3, cytb and nd1 genes have clearly lower dn/ds ratios, and the cytb, nd1 and nd3 genes have a much higher transition/ transversion ratio. finally, allowing for different relative rates between the different genes shows that certain genes (such as atp6 and cytb) may evolve twice as fast as some of the other genes (e.g. atp8, cox1 and cox2). gene partitioning allows capturing variation in the substitution process but considerably increases the number of parameters that need to be estimated, in particular for the gtr-based codon position partitioned nucleotide substitution models. because standard diffuse priors offer little protection againstnote: posterior-based model-selection approaches (hme and shme) were run for 10 million iterations (of which 1 million serve as burn-in; 2 million for the codon models), while ps and ss were run for 64 path steps/ratios with 250 000 iterations (25 000 iterations burn-in) per path step/ratio after an initial 1 million iterations of burn-in (2 million for codon models). rescaling in beagle was set to default (delayed). over-parameterization, and prior specification impacts marginal likelihood estimates, we also explore hpm approaches allowing to share information between different genes through hierarchical prior specification . we put hierarchical priors on the gene-specific and ! parameters of the gy94 codon model, on the parameters and the five free evolutionary parameters of the gtr model. log marginal likelihoods, shown in, were calculated using the same settings as in tables 1 and 2. the hpm approach offers marked improvements of the marginal likelihoods, and as expected, this is more pronounced for the parameter-rich codon position partitioned nucleotide models (about 300 log units higher) compared with the codon model (about 100 log units higher). as a consequence, a gtr-based nucleotide model partitioned according to gene and codon position now yields the highest marginal likelihood. similar to the partitioning with independent prior specification, we can notice discrepancies in model selection outcome between the hme and shme on the one hand, and ps and ss on the other . in the past decades, codon models have largely been avoided for phylogenetic reconstruction because they are computationally prohibitive. recent development of dedicated apis and high-performance computing libraries have, however, made it possible to harness the large numbers of computing cores available in graphics cards, which is particularly useful in the case of datasets with many unique site patterns , such as the one analysed here. at the time of the introduction of these techniques in statistical phylogenetics, the dataset analysed here could not be analysed in double precision on what were state-of-the-art graphics cards at that time because they were limited in the amount of memory. hence, to analyse this dataset using codon models, three graphics cards were combined in the analysis of this dataset , amounting to a considerable cost, we compare the performance of a gtx 590 graphics card and a 40-core xeon(r) e7-4870 2.40 ghz system using a auto-sizing thread pool, two different beagle rescaling schemes and different numbers of beagle instances, both for a single codon model and a codon model for each gene. we demonstrate that, for a single codon model and using a delayed rescaling scheme (the default in beagle) with a single-core instance, a state-of-the-art graphics card offers a speed increase with a factor over 30 compared with a singlecore instance on a modern cpu running beast (i.e. without beagle). because the number of beagle instances divides the likelihood calculations into two or more parts, thereby allowing each core to calculate part of the likelihoods corresponding to unique site patterns, multi-core or multi-gpu systems may benefit from using two or more beagle instances. the speedup further increases to a factor 460 when two such beagle instances are used, which allows for both gpus on the graphics card to be used and half of the site patterns being calculated on each gpu. the speed increase provided by a multi-core cpu system levels off at a factor of 12, with the maximum performance being reached when using approximately 32 cores, illustrating that a costly multi-core cpu architecture cannot achieve the same degree of speed ups as a graphics card. no further increases in performance by using additional cores could be obtained. although we cannot exclude communication latency as a possible cause for this observation, the most likely explanation is the considerable difference in memory bandwidth between both systems, with the gtx 590 sporting a theoretical total memory bandwidth of 327.7 gb/s, much higher than the performance of a multi-core cpu system such as ours, yielding a typical memory bandwidth of 67 gb/s [benchmarked using a multithreaded version of stream;. the situation is different when attempting to fit independent codon models to each of the 12 genes and each with its own gamma distribution to model site heterogeneity. the partitioning strategy implies a change in the meaning of beagle instances; the specification of 12 partitions translates into 12 likelihoods that need to be evaluated, which are naturally distributed over multiple cores as 12 instances. the specification of more beagle instances partitions the likelihood calculations even more by splitting further each partition (e.g. two instances would yield 24 sets of likelihood). this implies that there is little benefit in using instances on the gtx 590 graphics card, as each of the gpus already handles six likelihood sets. however, this approach may still profit from a 40-core cpu system, where initial increases in the number of beagle instances yield the highest speedups, but there is a diminishingthe bayesian phylogenetic model comparison we present here consistently shows that partitioning by gene yields an increased model fit. using standard diffuse priors, a separate codon model for each gene accompanied with gene-specific among-codon rate variation and gene-specific relative substitution rates offers the best performance, followed by codon partition models and trailed by standard nucleotide models. however, when substituting the independent diffuse priors by a hierarchical prior specification over the gene-specific parameters, a more parameter-rich gtr-based nucleotide substitution model partitioned according to gene and codon position emerges again as the best fitting model. these results can only be uncovered using recent model-selection approaches, such as path sampling and stepping-stone sampling. by demonstrating an increased model fit for gene partitioning, we corroborate the results of earlier studies, e.g. by, who combined morphology and nucleotide data from four genes in a study on model heterogeneity across data partitions. through bayes factor comparisons the authors showed a dramatic increase in model fit when extending twopartition models (one partition for the morphology data and one joint partition for the four genes) to five-partition equivalents (one partition for each of the four genes), emphasizing the importance of accommodating across-partition heterogeneity. the authors also showed that within-partition rate variation was by far the most important model component (i.e. much more than across-partition heterogeneity), but that the difference in fit between substitution models was only pronounced when comparing jc and gtr (tavare,tavare, 1986). it is important to note that the bayes factor comparisons reported in are calculated using the hme . because convincing evidence has been presented for the poor performance of the hme in recent years (,b), we advocate for caution when interpreting such results and encourage the use of ps and ss (over hme and shme). consistent with the increased model fit for the gene-partitioned models, we observed considerable variation in evolutionary parameters across the 12 mitochondrial genes. we examined whether this parameter variation observations could be associated with the asymmetrical mutation bias gradients in vertebrate mitochondrial genomes. faith and pollock (2003) andobtained the same relative gene order with respect to the duration of the single-strand state of the parental h strand: cox15cox25atp85atp65cox35nd35 nd4l5nd45nd15nd55nd25cytb. when comparing the relative orders of , , ! and the relative rate inagainst this relative gene order, only ranking according to the relative rate results in a similar order, with the exception of atp6 and the nd genes: cox15cox25atp85cox35nd4l 5nd55nd15nd25nd35nd45atp65cytb. our comparison of independent diffuse priors and hierarchical priors for the gene partitioned models illustrates the impact of prior specification on marginal likelihood estimation and model selection. we have previously highlighted that the outcome of bayesian model selection is dependent on prior choices . through hierarchical prior specification, hpms offer a middle ground between the extreme scenarios of independently fitting different models across genes and fitting a single model to all genes . while accommodating parameter variation among genes will be appropriate in most cases, there is less information available in each gene to inform the gene-specific parameters. hpms allow borrowing of strength of information from one partition by another, providing more precise gene-specific parameter estimates, and resulting in further model improvements in our comparisons. beast supports a generic implementation of hierarchical prior specification and hpm approaches can therefore be applied to different problems, such as hiv within-host evolution for different (groups of) patients and phylogeographic problems . by adopting hierarchical prior specification across gene-specific parameters in gtr nucleotide substitution models with both gene and codon position partitions, we notice a better model fit compared with gene partitioning with a standard codon substitution models. however, we are essentially comparing the most complex parametrization among conventional nucleotide substitution models with the simplest codon substitution model, and many assumptions can still be relaxed to make codon models more realistic. to illustrate this point, we observe a marginal likelihood improvement of about 300 log units or more between the hky-based and gtr-based codon position partitioned nucleotide models. so, we also expect model fit improvements for a codon model that would consider different substitution rates for the different types of nucleotide substitutions within a codon, instead of merely distinguishing between transitions and transversions as is done in the gy94 model. such a gtr-type of model applied at the nucleotide level, but with the constraint that the nucleotide sequence must encode some fulllength amino acid sequence, is the rationale of the codon substitution models in the style of the codon model of muse and gaut (1994). the codon model of muse and gaut (1994) may offer a more realistic parameterization than the gy94 model, which has no natural mechanistic interpretation at the nucleotide level , resulting in a possible increase in model fit over the gy94 model. more importantly, codon models can model varying selective pressure, but a gene-specific dn/ds is a very coarse approximation of this variation and many realistic codon codons now accommodate among-site variation in dn/ds , in dn and ds separately (kosakovsky), and/or among lineage variation in dn/ds . further research is needed to implement such models in beast and to assess their model fit. for each of the models tested in this manuscript, be it with or without gene partitioning, a relaxed molecular clock (with underlying lognormal distribution; ucld) is shown to outperform a strict clock, using all of the estimators. given that mammalian datasets of mitochondrial dna exhibit a wide variation in substitution rate across lineages, additional clock models, such as autocorrelated [see e.g.] or random local clocks should ideally be included in our model comparison. for example,have shown that the distribution of estimated mitochondrial substitution rates across species shows a very large variance, with the rates spanning two orders of magnitude. the authors also show that the family taxonomic level explains 75 of this variance, while the order taxonomic level explains 21, indicating that entire orders could all have (for example) low substitution rates, which may be appropriately modeled using autocorrelated relaxed clock models . finally, we have reported massive increases in computation speed using the beagle library for beast in combination with the latest graphics cards. however, the gtx 590 we used here is essentially designed to offer tremendous single precision performance, as required for visualization purposes in the gaming community. hence, its double precision performance is not keeping the same development pace, which will also be the case for future cards from the same series. double precision performance has recently increased with the advent of a new line of nvidia tesla k20 graphics cards, designed for scientific computing. further research will be needed to determine to what extent these new cards can improve efficiency in the field of phylogenetics.  
