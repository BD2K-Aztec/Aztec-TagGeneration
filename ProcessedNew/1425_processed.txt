disk-based compression of data from genome sequencing motivation: high-coverage sequencing data have significant, yet hard to exploit, redundancy. most fastq compressors cannot efficiently compress the dna stream of large datasets, since the redundancy between overlapping reads cannot be easily captured in the (relatively small) main memory. more interesting solutions for this problem are disk based, where the better of these two, from cox et al. (2012), is based on the burrowswheeler transform (bwt) and achieves 0.518 bits per base for a 134.0 gbp human genome sequencing collection with almost 45-fold coverage. results: we propose overlapping reads compression with minimizers, a compression algorithm dedicated to sequencing reads (dna only). our method makes use of a conceptually simple and easily parallelizable idea of minimizers, to obtain 0.317 bits per base as the compression ratio, allowing to fit the 134.0 gbp dataset into only 5.31 gb of space. availability and implementation: http://sun.aei.polsl.pl/orcom under a free license.it is well known that the growth of the amount of genome sequencing data produced in the last years outpaces the famous moores law predicting the developments in computer hardware . confronted with this deluge of data, we can only hope for better algorithms protecting us from drowning. speaking about big data management in general, there are two main algorithmic concerns: faster processing of the data (at preserved other aspects, like mapping quality in de novo or referential assemblers) and more succinct data representations (for compressed storage or indexes). in this article, we focus on the latter concern. raw sequencing data are usually kept in fastq format, with two main streams: the dna symbols and their corresponding quality scores. older specialized fastq compressors were lossless, squeezing the dna stream down to about 1.51.8 bpb (bits per base) and the quality stream to 34 bpb, but more recently it was noticed that a reasonable solution for lossy compression of the qualities has negligible impact on further analyzes, for example, referential mapping or variant calling performance . this scenario became thus immediately practical, with scores lossily compressed to about 1 bpb or less . note also that illumina software for their hiseq 2500 equipment contains an option to reduce the number of quality scores (even to a few), since it was shown that the fraction of discrepant single nucleotide polymorphisms grows slowly with diminishing number of quality scores in illuminas casava package (http://support.illumina.com/sequencing/sequencing_software/casava.ilmn). it is easy to notice that now the dna stream becomes the main compression challenge. even ifhigher order modeling or lz77-style compression can lead to some improvement in dna stream compression, we are aware of only two much more promising approaches. both solutions are disk based. yanovsky (2011) creates a similarity graph for the dataset, defined as a weighted undirected graph with vertices corresponding to the reads of the dataset. for any two reads s 1 and s 2 the edge weight between them is related to the profitability of storing s 1 and the edit script for transforming it into s 2 versus storing both reads explicitly. for this graph, its minimum spanning tree (mst) is found. during the mst traversal, each node is encoded using the set of maximum exact matches between the nodes read and the read of its parent in the mst. as a backend compressor, the popular 7zip is used. recoil compresses a dataset of 192 m illumina 36 bp reads (http://www.ncbi.nlm.nih.gov/sra/srx001540), with coverage below 3-fold, to 1.34 bpb. this is an interesting result, but recoil is hardly scalable; the test took about 14 h on a machine with 1.6 ghz intel celeron cpu and four hard disks. more recently, cox et al. (2012) took a different approach, based on the burrowswheeler transform (bwt). their result for the same dataset was 1.21 bpb in less than 65 min, on a xeon x5450 (quad-core) 3 ghz processor. the achievement is however more spectacular if the dataset coverage grows. for 44.5-fold coverage of real human genome sequence data, the compressed size improves to as little as 0.518 bpb (actually inthe authors report 0.484 bpb, but their dataset is seemingly no longer available and in our experiments we use a slightly different one.) allowing to represent the 134.0 gbp of input data in 8.7 gb of space. note that if the reference sequence is available, either explicit or can be reconstructed (presumed, in the terminology of c novas and moffat 2013), then compressing dna reads is much easier and high compression ratios are possible. several fastq or sam/bam compressors make use of a reference sequence, to name quip , fastqz and fqzcomp in one of their modes, slimgene , cram , goby , deez and fqzip . several techniques for compressing sam files, including mapping reads to a presumed reference sequence, were also explored in c novas and moffat (2013). in this article, we present a new reference-free compressor for fastq data, overlapping reads compression with minimizers (orcom), achieving compression ratios surpassing the best known solutions. for the two mentioned human datasets it obtains the compression ratios of 1.005 and 0.317 bpb, respectively. orcom is also fast, producing the archives in about 8 and 77 min, respectively, using eight threads on an amd opteron 6136 2.4 ghz machine.we tested our algorithm versus several competitors on real and simulated datasets, detailed in  
