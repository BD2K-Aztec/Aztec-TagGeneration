conveyor: a workflow engine for bioinformatic analyses motivation: the rapidly increasing amounts of data available from new high-throughput methods have made data processing without automated pipelines infeasible. as was pointed out in several publications, integration of data and analytic resources into workflow systems provides a solution to this problem, simplifying the task of data analysis. various applications for defining and running workflows in the field of bioinformatics have been proposed and published, e.g. galaxy, mobyle, taverna, pegasus or kepler. one of the main aims of such workflow systems is to enable scientists to focus on analysing their datasets instead of taking care for data management, job management or monitoring the execution of computational tasks. the currently available workflow systems achieve this goal, but fundamentally differ in their way of executing workflows. results: we have developed the conveyor software library, a multitiered generic workflow engine for composition, execution and monitoring of complex workflows. it features an open, extensible system architecture and concurrent program execution to exploit resources available on modern multicore cpu hardware. it offers the ability to build complex workflows with branches, loops and other control structures. two example use cases illustrate the application of the versatile conveyor engine to common bioinformatics problems. availability: the conveyor application including client and server are available atworkflows have become an important aspect in the field of bioinformatics during the last years (e.g.). applications like galaxy , taverna , pegasus and kepler offer an easy way to access local and remote resources and perform automatic analyses to test hypotheses or process data. in many cases, they have become a reasonable alternative to write simple software tools like perl scripts, especially for users without an in-depth computer science background. libraries like ruffus add workflow to whom correspondence should be addressed. functionality to programming languages, providing methods to define and build workflow within own applications. a workflow is built from several linked steps that consume inputs, process and convert data and produce results. the most simple workflows are linear chains of steps to convert input data to the required output. more complex setups may also include loops, branches, parallel and conditional processing, reading from various sources and writing different outputs in various formats. for creating reusable workflow components, a processing step may be composed of nested processing steps, allowing the user to build complex pipelines for higher level analysis. many workflow engines act as a wrapper using existing command line utilities or enact and orchestrate existing web services as basic modules for their processing steps. as a result, adding new processing steps by wrapping existing applications often does only require little or no programming effort. of course, this comes at a price. passing data between processing steps depends on a common data format, especially in the case of distributed processing nodes. most analysis tools available as command line applications or web services are consuming simple text formats, e.g. the fasta format for dna and amino acid sequences. these formats are in turn used by the workflow engines to exchange data between processing nodes. integrating other processing nodes or input sources requires explicit data conversion prior to processing. this often leads to the loss of information; e.g. gene features annotated in embl or genbank entries cannot retain all qualifiers after converting them to fasta format. an analysis done byshows that most tasks used in publicly available taverna workflows are dedicated to data conversion. to some extent, this problem is solved by meta information provided with types that allow the definition of type hierarchies and interfaces. the biomoby data type management is an example for an ontology-based approach to data type handling; nonetheless, it requires extra efforts by the developer and/or maintainer. other attempts to define common data types like bioxsd orwere made, but none of them has been successfully adopted by the community yet. the situation is even worse if legacy data from applications are to be integrated into a workflow. accessing data, e.g. stored in a relational database or available by a local application only, requires special processing steps; passing the data between distributed processing nodes may not be possible at all. another problem arises from the nature of web services used in processing steps. although they offer an elegant and easy way to provide and consume useful services, users have to be aware of the pitfalls of web services if they rely on them for an analytical workflow. a service may become unavailable without prior noticethe use cases shown in the previous section demonstrate the usefullness of the conveyor system to common problems. both use cases were processed with the set of available escherichia coli genomes 2 (accession numbers nc_011602, nc_011603, nc_008253, nc_011748, nc_008563, nc_009837, nc_009838, nc_012947, nc_012759, nc_012967, nc_004431, nc_010468, nc_009786, nc_009787, nc_009788, nc_009789, nc_009790, nc_009791, nc_009801, nc_011745, nc_009800, nc_011741, nc_011750, nc_010473, nc_000913, ac_000091, nc_002127, nc_002128, nc_002695, nc_002655, nc_007414, nc_011350, nc_011351, nc_011353, nc_013008, nc_013010, nc_011742, nc_011747, nc_011407, nc_011408, nc_011411, nc_011413, nc_011415, nc_011416, nc_011419, nc_010485, nc_010486, nc_010487, nc_010488, nc_010498, nc_011739, nc_011749, nc_011751, nc_007941, nc_007946, nc_011740, nc_011743) by concatenating their genbank formatted files available at the ncbi web site. the workflows in both use cases were executed on a single host with four intel xeon e7540 cpus running at 2 ghz. each cpu provides six physical cores and six additional virtual cores by hyper threading, summing up to 48 cores managed by the operation system. the system is equipped with 256 gb system ram and 200 gb swap. conveyor was configured to use a threadbased processing model with one thread per workflow node, and a process-based processing model for external applications. the number of parallel running processes were set to one, two, four,eight and sixteen. each setup was run five times. execution time was measured using the time command, reporting the overall time (real), the accumulated cpu time of all processes (user) and the time spend in the kernel (sys). the complete benchmark data is available as supplementary material.as presented in the preceding sections, the conveyor system offers a comprehensive and versatile system for data analysis. although it is designed to work in any field of application, the use cases presented in the former sections clearly prove its fidelity to the field of bioinformatics. the unique design, especially the powerful object model for both data and nodes, allows the system to fill the gap between web service-based approaches and writing custom software.  
