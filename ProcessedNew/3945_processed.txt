reference-based compression of short-read sequences using path encoding motivation: storing, transmitting and archiving data produced by next-generation sequencing is a significant computational burden. new compression techniques tailored to short-read sequence data are needed. results: we present here an approach to compression that reduces the difficulty of managing large-scale sequencing data. our novel approach sits between pure reference-based compression and reference-free compression and combines much of the benefit of reference-based approaches with the flexibility of de novo encoding. our method, called path encoding, draws a connection between storing paths in de bruijn graphs and context-dependent arithmetic coding. supporting this method is a system to compactly store sets of kmers that is of independent interest. we are able to encode rna-seq reads using 311 of the space of the sequence in raw fasta files, which is on average more than 34 smaller than competing approaches. we also show that even if the reference is very poorly matched to the reads that are being encoded, good compression can still be achieved. availability and implementation: source code and binaries freely available for download atthe size of short-read sequence collections is often a stumbling block to rapid analysis. central repositories such as the nih sequence read archive (sra; http://www.ncbi.nlm.nih.gov/sra) are enormous and rapidly growing. the sra now contains 2.5 petabases of dna and rna sequence information, and due to its size, it cannot be downloaded in its entirety by anyone except those with enormous resources. when select experiments are downloaded, the local storage burden can be high, limiting large-scale analysis to those with large computing resources available. use of cloud computing also suffers from the data size problem: often transmitting the data to the cloud cluster represents a significant fraction of the cost. data sizes also hamper collaboration between researchers at different institutions, where shipping hard disks is still a reasonable mode of transmission. local storage costs inhibit preservation of source data necessary for reproducibility of published results. compression techniques that are specialized to short-read sequence data can help to ameliorate some of these difficulties. if data sizes can be made smaller without loss of information, transmission and storage costs will correspondingly decrease. while general compression is a long-studied field, biological sequence compression though studied somewhat before short-read sequencing (e.g.)is still a young field that has become more crucial as data sizes have outpaced increases in storage capacities. in order to achieve compression beyond what standard compressors can achieve, a compression v c the author 2015. published by oxford university press.we have provided a novel encoding scheme for short read sequence data that is effective at compressing sequences to 1242 of the uncompressed, 2-bit encoded size. to do this, we introduced the novel approach of encoding paths in a de bruijn graph using an adaptive arithmetic encoder combined with a bit tree data structure to encode start nodes (see section 2 for a description). these two computational approaches are of interest in other settings as well. path encoding achieves better compression than both de novo schemes and mapping-based reference schemes. because the reference for the human transcriptome is small (92 mb) compared with the size of the compressed files, the overhead of transmitting the reference is recovered after only a few transmissions. in addition, although it would be possible to shrink the reference using a custom format, in our current implementation the reference is intentionally chosen to be merely a gzipped version of the transcriptomea file that most researchers would have stored anyway. path encoding is more general than reference-based schemes because we have more flexibility in choosing how to initialize the statistical model with the reference sequence. for example, the reference could be reduced to simple context-specific estimates of gc content. this will naturally lead to worse compression but will also eliminate most of the need to transmit a reference. technology-specific error models could also be incorporated to augment the reference to better deal with sequencing errors. in addition, single nucleotide polymorphism (snp) data from a resource such as the hapmap project could be included in the reference to better deal with genomic variation. framing the problem using a statistical generative model as we have done here opens the door to more sophisticated models being developed and incorporated. another source of flexibility is the possibility of lossy sequence compression. path encoding naturally handles with errors since they will have low probability because they are typically seen only once (or a few times) in contrast to correct kmers that are more frequently seen. while encoding, a base that has low probability in a particular context could be converted to a higher probability base under the assumption that the low probability base is a sequencing error. implementation of this technique does indeed reduce file sizes substantially, but of course at the loss of being able to reconstruct the input sequence. while lossy compression may be appropriate for some analyses (such as isoform expression estimation) and error correction can be viewed as a type of lossy encoding, because we are interested in lossless compression, we do not explore this idea more here. an interesting direction for future research is to explore the use of the reference to improve the encoding of the read heads, using for example, a huffman encoding-like scheme. another important direction for future work is to reduce the time and memory requirements of the implementation of path encoding. part of the reason for the higher computational demands is use of dynamic arithmetic encoding, which is needed because the probabilities of kmers need not be stationary. for example, in a file with many as in the first reads but many cs in the later reads, the dynamic ac will adapt to this non-stationary distribution, leading to improved compression. another direction for future work is to apply similar ideas to genomic reads, where the reference is much larger. a recent line of work (e.g.) aims at producing searchable, compressed representations of sequence information. allowing sequence search limits the type and amount of compression that can be applied and requires some type of random access into the encoded sequences. arithmetic encoding does not generally allow such random access decoding because the constructed interval for a given symbol depends on all previously observed symbols. however, decompression with our path encoding scheme can be performed in a streamingmanner: the encoded file is read once from start to finish, and the decoder produces reads as they are decoded. this would allow reads to be decoded as they are being downloaded from a central repository. the other dimension of compressing short-read data is storing the quality values that typically accompany the reads. path encoding does not attempt to store these quality values as there are other, more appropriate approaches for this problem . path encoding can be coupled with one of these approaches to store both sequence and quality values. in fact, in many cases, the quality values are unnecessary and many genomic tools such as bwa and sailfish now routinely ignore them.showed that quality values can be aggressively discarded and without loss of ability to distinguish sequencing errors from novel snps. thus, the problem of compression of quality values is both very different and less important than that of recording the sequence reads. our main contribution is the design of a high-performing compression scheme. we hope that our compression results will spur further reference-based read compression work in the new direction that we propose here: mapping-free, statistical compression with accompanying supporting pre-processing.  
