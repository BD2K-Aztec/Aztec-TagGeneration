genome analysis gage-b: an evaluation of genome assemblers for bacterial organisms motivation: a large and rapidly growing number of bacterial organisms have been sequenced by the newest sequencing technologies. cheaper and faster sequencing technologies make it easy to generate very high coverage of bacterial genomes, but these advances mean that dna preparation costs can exceed the cost of sequencing for small genomes. the need to contain costs often results in the creation of only a single sequencing library, which in turn introduces new challenges for genome assembly methods. results: we evaluated the ability of multiple genome assembly programs to assemble bacterial genomes from a single, deep-coverage library. for our comparison, we chose bacterial species spanning a wide range of gc content and measured the contiguity and accuracy of the resulting assemblies. we compared the assemblies produced by this very high-coverage, one-library strategy to the best assemblies created by two-library sequencing, and we found that remarkably good bacterial assemblies are possible with just one library. we also measured the effect of read length and depth of coverage on assembly quality and determined the values that provide the best results with current algorithms.a high-quality assembly of a bacterial genome provides the basis for research into a wide range of questions about prokaryotic biology. increasingly in recent years, investigators have turned to rapid whole-genome sequencing of bacteria as part of efforts to trace the source of infectious disease outbreaks, to understand the source of pathogenesis and to understand multidrug resistance, among other questions. the human microbiome project, which has identified thousands of new microbial strains and species as it explores the bacteria that live on our bodies, has dramatically increased the number of new bacterial genomes that are being sequenced on a daily basis. for most bacterial genome projects, the first step in analysis is the assembly of the raw read data into larger, contiguous sequences that represent the original bacterial chromosomes. second-and third-generation sequencing technologies allow for remarkably fast high-throughput sequencing. the latest technologies capture longer read lengths than just a few years ago, which is expected to improve the quality of assemblies. the illumina hiseq machine routinely generates reads of 100 bp and can generate 600 gb in a single run. a single lane of a hiseq generates 435 gb, which far exceeds what is necessary for a bacterial genome. through multiplexing, it is feasible to generate deep sequencing data for 2030 different bacteria in a single lane. because the cost of preparing the dna (including library construction) can be greater than the cost of sequencing, many researchers have begun to adopt a strategy of sequencing just a single library for each of many bacterial strains. however, it has long been assumed that whole-genome assembly projects will include data from two or more libraries with different fragment lengths, beginning with the first bacterial genome project . a typical strategy will use one short library, with paired reads from both ends of relatively short fragments, e.g. 200600 bp with todays technology. as a general rule, repeats longer than the library fragment size cannot be reliably assembled and will create gaps in the assembly. thus, a second jumping library will use long fragments, in the range of 200020 000 bp, to jump across these repeats. next-generation assembly algorithms use these libraries to great advantage, and they are particularly important for large genomes . jumping libraries are more difficult to create, and small fragment libraries (up to 500 bp on the hiseq and 600 bp on the miseq instrument) are the fastest, most efficient way to generate deep coverage of a genome today. this motivated us to design the current study to evaluate the effectiveness of different genome assembly software on a single, short-fragment library across a range of bacterial species. in recent years, various assembly tools have been used to assemble genomes of different sizes. some assemblers, such as velvet , were originally designed for assembling small, prokaryotic-sized genomes, whereas others, such as soapdenovo and allpaths , were built to assemble large, mammalian-sized genomes. although some assemblers might not be able to handle large genomes, almost all of them have been used in assembling to whom correspondence should be addressed. the author 2013. published by oxford university press. all rights reserved. for permissions, please e-mail: journals.permissions@oup.com this is an open access article distributed under the terms of the creative commons attribution license (http://creativecommons.org/licenses/by/3.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited. bacteria. several recent studies have compared the ability of assemblers with assemble large genomes . to date, no comprehensive evaluation has appeared that has considered which of these assemblers performs best, although hundreds (and probably thousands) of bacteria have been assembled from next-generation sequence data. following the standards set by the original gage assembly comparison , gage-b (genome assembly gold-standard evaluation for bacteria) evaluates how genome assemblers compare on a spectrum of bacterial genomes sequenced by the newest sequencing technologies. as with gage, we followed a strict standard of reproducibility, which required us to use only freely available assembly software, and our results include the full recipe used for each assembler for each of the genomes. our experiments were designed to answer the following questions: which assembler generates the best assemblies of bacterial organisms from a single shotgun library? what depth of coverage and which software parameters should be used to produce the optimal assemblies?how does high coverage by a single library compare with the use of multiple libraries?how do assemblies from the longer, 250 bp miseq reads compare with assemblies of 100 bp hiseq reads?the statistics presented in tables 2 and 3 and supplementary tables s1s12 support a number of conclusions about the capabilities of the various assembly methods and provide answers to the four questions posed in section 1. first, we consider which assembler generates the best assemblies of bacterial species from a single whole-genome shotgun library. although no assembler won on all the various metrics, the masurca assembler had the largest contig sizes, measured by either n50 or corrected n50 values, for 10 of the 12 experiments. the spades assembler, a relatively recent entry into the next-generation assembly field, came in first or essentially tied for first for 4 of the 12 genomes. these results were consistent across both 100 (hiseq) and 250 bp (miseq) reads, although spades had a larger boost in improvement for the longer reads. when considering the number of errors, including local errors, abyss and sga consistently produced assemblies with the fewest errors. these assemblers also tended to produce smaller contigs than most of the others, suggesting that they use a conservative assembly strategy that trades off contig size for accuracy. this result is consistent with the results of the original gage evaluation on larger genomes. in most cases, the number of errors did not greatly reduce the n50 sizes, because of, at least in part, the distinction made here between local errors, which involve insertions, deletions and rearrangements 51 kb and larger errors. we only split contigs and scaffolds on larger errors for this study, in contrast to our earlier gage study where we split on all errors45 bp. we noted that for some assemblers, the number of local errors tended to be larger, e.g. soapdenovo had 50 local errors in its assembly of v.cholerae from 100 bp reads, whereas the other methods had only 018 local errors. these errors would further reduce n50 values if they were used to split assemblies. a new quality metric that we computed across all 12 datasets (supplementary tables s1s12) was the number of proteins contained fully within contigs. this metric is tolerant of divergence between the sequenced strain and the reference because protein sequences diverge much more slowly than nucleotide sequences. in addition, this metric captures a feature of biological interest: whether a typical protein is fully contained within a contig. for this computation, we used all annotated protein coding genes from the reference genome. on this metric, the spades assemblies performed the best, with the largest value for 10 of the 12 genomes. for most genomes, multiple assemblers performed similarly, and there was no clear winner. overall, masurca and spades produced the best assemblies across these 12 bacterial organisms. however, even these assemblers have some weaknesses that should be pointed out. spades sometimes generated many small contigs that did not align to the reference genome. on inspection, we found that most of these had low coverage, and because spades provides detailed coverage information, one can easily filter out the low-coverage contigs. masurca has a different problem: it sometimes creates good contigs that it labels degenerate based on internal coverage statistics, which can cause it to omit some parts of the genome from the assembly. this problem can be solved simply by including degenerate contigs above some minimum length threshold as part of the assembly. one of our primary motivations in gage-b was to answer the question of whether deep sequencing from a single,, the contigs created by both masurca and spades from a single deep-coverage library were considerably larger than those from the two-library data, which was at lower coverage (100). the number of errors was also slightly lower, although this could be due to improvements in assembly algorithms. however, the lack of long jumping pairs makes significant difference in the size of scaffolds. a single library of paired reads from relatively short fragments simply cannot span many of the repetitive sequences in a genome. thus, although the biggest scaffold with the two-library strategy was over 2.5 mb in length and spanned more than half of the main chromosome, the best scaffolds for the one-library assembly were less than onetenth as long. we conclude that for gene-level questions, a singlesequencing library today, made from either 100 or 250 bp reads, will produce a good de novo assembly in which over half the genome is contained in contigs 4100 kb, and in which a large majority of genes are contained within scaffolds. if a closely related genome is available, it can be used to aid scaffolding, but otherwise a jumping library is still necessary to produce truly large scaffolds. we also considered the question of whether 250 bp reads are superior to 100 bp reads at a comparable cost. because these reads are more expensive to produce, we used 2.5-fold lower coverage (100 versus 250) for the long reads. we obtained both read lengths for three species: m.abscessus, r.sphaeroides and v.cholerae, for all of which we had a near-identical finished genome. tables 2 and 3 and supplementary tables s2s8 show the details of these assemblies. for m.abscessus and v.cholerae, the best assemblies for the 100 and 250 bp reads had similar n50 sizes, although for r.sphaeroides, the best n50 was 30 larger with 100 bp reads. the number of proteins contained in contigs was higher in all 100 bp assemblies except for those built by spades. it may be tempting to conclude that 100 coverage in miseq reads is inferior to 250 coverage in hiseq reads, but in looking at the details, we observed that among all the assemblies of r.sphaeroides, five had higher contig n50 values when based on miseq versus hiseq data. for v.cholerae, four assemblers performed better with miseq data and the other four performed better with hiseq data. we also observed that masurca always generated better results using hiseq datasets, whereas spades always generated better results using miseq data. other assemblers did not show such an obvious preference. our hypothesis is that many algorithms have not yet had time to adapt to the longer 250 bp reads, and once they do, lower coverage with longer reads may be superior, as it already is in some cases. overall, our results support a conclusion that with deep sequence coverage, the latest genome assemblers can produce good de novo assemblies from just a single, short-fragment dna library. today this strategy represents the lowest-cost method for capturing the entire genome of a bacterium or other species with small genomes. the vast majority of protein-coding genes will be contained wholly within contigs using this strategy, although an important caveat is that large-scale changes in genome structure, particularly large rearrangements, will likely not be captured. our findings suggest that multiplexing many genomes in the same sequencing run will provide a highly effective means for studying hundreds if not thousands of bacterial strains in the near future.  
