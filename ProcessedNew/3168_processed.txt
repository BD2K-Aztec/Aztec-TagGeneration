multiscale dna partitioning: statistical evidence for segments motivation: dna segmentation, i.e. the partitioning of dna in com-positionally homogeneous segments, is a basic task in bioinformatics. different algorithms have been proposed for various partitioning criteria such as guanine/cytosine (gc) content, local ancestry in population genetics or copy number variation. a critical component of any such method is the choice of an appropriate number of segments. some methods use model selection criteria and do not provide a suitable error control. other methods that are based on simulating a statistic under a null model provide suitable error control only if the correct null model is chosen. results: here, we focus on partitioning with respect to gc content and propose a new approach that provides statistical error control: as in statistical hypothesis testing, it guarantees with a user-specified probability 1 that the number of identified segments does not exceed the number of actually present segments. the method is based on a statistical multiscale criterion, rendering this as a segmen-tation method that searches segments of any length (on all scales) simultaneously. it is also accurate in localizing segments: under benchmark scenarios, our approach leads to a segmentation that is more accurate than the approaches discussed in the comparative review of elhaik et al. in our real data examples, we find segments that often correspond well to features taken from standard university of california at santa cruz (ucsc) genome annotation tracks. availability and implementation: our method is implemented in function smucer of the r-package stepr available atit has been observed a long time ago that dna sequences are not composed homogeneously and that bases fluctuate in their frequency. these inhomogeneities often have an evolutionary or a functional interpretation, and can be relevant for the subsequent analysis of sequence data. because it correlates with many features of interest, the gc content, i.e. the relative frequency of the bases g and c, is one of the most commonly studied sequence properties. large-scale regions, typically 300 kb , of approximately homogeneous gc content have been called isochores. in view of the somewhat vague notion of approximate homogeneity and conceptual criticism in studies such asor, there is less interest in isochores nowadays. however, there is no doubt about variation in gc content along genomes, and search is done instead for domains of any length exhibiting distinct local gc content; see, for instance, elhaik et al. (2010b). several factors can influence the gc content of a region. at larger scales, it correlates with the density of genes, with gene-rich regions typically exhibiting an elevated gc content compared with regions of low gene density. at smaller scales, there is fluctuation in the gc content, for instance, because of repetitive elements and gpc islands. the gc content is also known to vary between exons and introns. especially for long introns, their lower gc content seems to play a role in splice site recognition . there is also a correlation between the gc content and the local recombination rate . for a further discussion of features correlated to the gc content, see. in gene expression studies, regions of homogeneous gc content are of interest because the gc content of a region affects the number of reads mapped to this region. for dna and rna-seq experiments with the illumina genome analyzer platform, this has been, for instance, investigated in benjamini and speed (2012) and. segmentation algorithms aim to partition a given dna sequence into stretches that are homogeneous in their composition but differ from neighboring segments. the classical approach of using moving windows is simple and available, for instance, as an option with the ucsc and ensembl genome browsers. however, it has some disadvantages. for instance, the choice of the window size is difficult because it defines implicitly a fixed scale at which segments primarily will be detected. further, the involved smoothing blurs abrupt changes. without additional statistical criteria, the method also does not tell us whether differences between neighboring windows are statistically significant. therefore, several more sophisticated approaches have been proposed. these methods include hidden markov models and walking markov models . there are also change-point methods available; see, for instance,. a bayesian approach to whom correspondence should be addressed. that relies on the gibbs sampler has been proposed by. an older approach based on information criteria can be found in. furthermore, recently developed methods based on entropy criteria have been shown to perform particularly well; see elhaik et al. (2010a) and elhaik et al. (2010b). a review of segmentation methods can be found in the article by braun and muler (1998), and for a more recent comparative evaluation of the more popular approaches, see. in this paper, we focus on binary segmentation, where the four-letter alphabet of a dna sequence is converted into a two-letter code. for gc content, we set the response to be 1 for g or c at a position and 0 for a/t; we use y i to denote the response at position i and summarize the responses for a sequence of length n by y y 1 , y 2 ,. .. , y n : 1we model the responses y i to be independent and bernoulli bin1, i distributed, and also assume that there is a partition 0 0 5 1 5 5 k n into an unknown number k of segments on which the i are piecewise constant, i.e. i p j for i 2 i j. here, i j : j1 , j denotes the jth segment with response probability p j for 1 j k. a segmentation algorithm provides estimates k for the number of segments, for the internal segment boundaries,and for the response probabilities, p j , on the estimated segmentsin the following, we will identify a segmentation with p, i, where p p 1 ,. .. , p k and i i 1 ,. .. , i k . our proposed algorithm provides a parsimonious estimate k for k: k will not exceed the actual number of segments k, except for a small user-specified error probability ; as a default value, we suggest 5, the error probability also chosen in our simulations and data analyses. relaxing this significance level to a larger value, say 20, will typically lead to more identified segments but at the cost of statistical accuracy.we evaluated our segmentation approach both on simulated data and on data taken from the human genome and the longknown-phage. in our simulations, we used the benchmark scenarios proposed in. because an extensive comparison of popular dna segmentation algorithms under these benchmark scenarios is already available in elhaik et al.(2010a), we provide a comparison of our approach with the method that performed best in, namely, the one based on the jensenshannon divergence. this recursive approach (called d js ) splits one of the current segments in each step. this is done by adding a new break point such that the improvement in jensenshannon divergence is maximized. the algorithm stops when the improvement does not reach a threshold value obtained via simulations. here, we used the matlab implementation djsegmentation.m of the algorithm, which is publicly available as part of isoplotter 2.4 (http://code. google.com/p/isoplotter/). there, 5:8 10 5 is taken as a threshold, a value obtained from simulating long (1 mb) homogeneous sequences. although this value seems to work well for the considered benchmark scenarios and might also be useful to prevent false-positive findings when searching for long homogeneous sequences, it might be less suitable for balancing false-positives and false-negatives under other scenarios. therefore, a modified version (called isoplotter) of d js has been proposed briefly after that uses critical values dependent both on the segment length and the standard deviation of the gc content. therefore, we also report on the performance of isoplotter 2.4 (again under the default parameter settings) and provide detailed results in the supplementary material. to facilitate the comparison and to accelerate the computations for longer sequences, we binned the data and applied our algorithm to the resulting binomial frequencies. we choose the bin size equal to 32, which is the default value with the d js and isoplotter software and has also been used in  
