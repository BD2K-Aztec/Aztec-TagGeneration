compacting de bruijn graphs from sequencing data quickly and in low memory motivation: as the quantity of data per sequencing experiment increases, the challenges of fragment assembly are becoming increasingly computational. the de bruijn graph is a widely used data structure in fragment assembly algorithms, used to represent the information from a set of reads. compaction is an important data reduction step in most de bruijn graph based algorithms where long simple paths are compacted into single vertices. compaction has recently become the bottleneck in assembly pipelines, and improving its running time and memory usage is an important problem. results: we present an algorithm and a tool bcalm 2 for the compaction of de bruijn graphs. bcalm 2 is a parallel algorithm that distributes the input based on a minimizer hashing technique, allowing for good balance of memory usage throughout its execution. for human sequencing data, bcalm 2 reduces the computational burden of compacting the de bruijn graph to roughly an hour and 3 gb of memory. we also applied bcalm 2 to the 22 gbp loblolly pine and 20 gbp white spruce sequenc-ing datasets. compacted graphs were constructed from raw reads in less than 2 days and 40 gb of memory on a single machine. hence, bcalm 2 is at least an order of magnitude more efficient than other available methods. availability and implementation: source code of bcalm 2 is freely available at: https://github.com/ gatb/bcalm contact: rayan.chikhi@univ-lille1.frmodern sequencing technology can generate billions of reads from a sample, whether it is rna, genomic dna, or a metagenome. in some applications, a reference genome can allow for the mapping of these reads; however, in many others, the goal is to reconstruct long contigs. this problem is known as fragment assembly and continues to be one of the most important challenges in bioinformatics. fragment assembly is the central algorithmic component behind the assembly of novel genomes, detection of gene transcripts (rna-seq) , species discovery from metagenomes, structural variant calling . continued improvement to sequencing technologies and increases to the quantity of data produced per experiment present a serious challenge to fragment assembly algorithms. for instance, while there exist many genome assemblers that can assemble bacterial sized genomes, the number of assemblers that can assemble a high-quality mammalian genome is limited, with most of them developed by large teams and requiring extensive resources . for even larger genomes, such as the 20 gbp picea glauca (white spruce), graph construction and compaction took 4.3 tb of memory, 38 h and 1380 cpu cores . in another instance, the whole genome assembly of 22 gbp pinus taeda (loblolly pine) required 800 gb of memory and three months of running time on a single machine . most short-read fragment assembly algorithms use the de bruijn graph to represent the information from a set of reads. given a set of reads r, every distinct k-mer in r forms a vertex of the graph, while an edge connects two k-mers if they overlap by k 1 characters. the use of the de bruijn graph in fragment assembly consists of a multi-step pipeline, however, the most data intensive steps are usually the first three: nodes enumeration, compaction and graph cleaning. in the first step (sometimes called k-mer counting), the set of distinct k-mers is extracted from the reads. in the second step, all unitigs (paths with all but the first vertex having in-degree 1 and all but the last vertex having out-degree 1) are compacted into a single vertex. in the third step, artifacts due to sequencing errors and polymorphism are removed from the graph. the second and third step are sometimes alternated to further compact the graph. after these initial steps, the size of the data is reduced gradually, e.g. for a human dataset with 45 coverage, to overcome the scalability challenges of fragment assembly of large sequencing datasets, there has been a focus on improving the resource utilization of de bruijn graph construction. in particular, k-mer counting has seen orders of magnitude improvements in memory usage and speed. as a result, graph compaction is becoming the new bottleneck; but, it has received little attention . recently, we developed a compaction tool that uses low memory, but without an improvement in time . other parallel approaches for compaction have been proposed, as part of genome assemblers. however, most are only implemented within the context of a specific assembler, and cannot be used as modules for the construction of other fragment assemblers or for other applications of de bruijn graphs (e.g. metagenomics). in this paper, we present a fast and low memory algorithm for graph compaction. our algorithm consists of three stages: careful distribution of input k-mers into buckets, parallel compaction of the buckets, and a parallel reunification step to glue together the compacted strings into unitigs. the algorithm builds upon the use of minimizers to partition the graph ; however, the partitioning strategy is completely novel since the strategy ofdoes not lend itself to parallelization. due to the algorithms complexity, we formally prove its correctness. we then evaluate it on whole-genome human, pine and spruce sequencing data. the de bruijn graph for a whole human genome dataset is compacted in roughly an hour and 3 gb of memory using 16 cores. for the 20 gbp pine and spruce genomes, k-mer counting and graph compaction take only 2 days and 40 gb of memory, improving on previously published results by at least an order of magnitude.in this paper, we present bcalm 2, an open-source parallel and lowmemory tool for the compaction of de bruijn graphs. bcalm 2 constructed the compacted de bruijn graph of a human genome sequencing dataset in 76 mins and 3 gb of memory. furthermore, k-mer counting and graph compaction using bcalm 2 of the 20 gbp white spruce and the 22 gbp loblolly pine sequencing datasets required only 2 days and 40 gb of memory each. bcalm 2 is different from previous approaches in several regards. first, it is a separate module for compaction, with the goal that it can be used as part of any other tools that build the de bruijn graph. while parallel genome assemblers offer impressive performance, there are many situations where differences in data require the development of a new assembler, and hence it is desirable to buildfor bcalm 2 and bcalm we used k 55, and 8 and 10, respectively; abundance cutoffs were set to 5 for chr 14 and 3 for whole human. we used 16 cores for the parallel algorithms abyss, meraculous 2 and bcalm 2. meraculous 2 aborted with a validation failure due to insufficient peak k-mer depth when we ran it with abundance cutoffs of 5. we were able to execute it on chromosome 14 with a cutoff of 8, but not for the whole genome. for the whole genome, we show the running times given in. the exact memory usage was unreported there but is less than 1 tb. meraculous 2 was executed with 32 prefix blocks.the k-mer size was 31 and the abundance cutoff for k-mer counting was 7.  
