genome analysis eceo: an efficient cloud epistasis computing model in genome-wide association study motivation: recent studies suggested that a combination of multiple single nucleotide polymorphisms (snps) could have more significant associations with a specific phenotype. however, to discover epistasis, the epistatic interactions of snps, in a large number of snps, is a computationally challenging task. we are, therefore, motivated to develop efficient and effective solutions for identifying epistatic interactions of snps. results: in this article, we propose an efficient cloud-based epistasis computing (eceo) model for large-scale epistatic interaction in genome-wide association study (gwas). given a large number of combinations of snps, our eceo model is able to distribute them to balance the load across the processing nodes. moreover, our eceo model can efficiently process each combination of snps to determine the significance of its association with the phenotype. we have implemented and evaluated our eceo model on our own cluster of more than 40 nodes. the experiment results demonstrate that the eceo model is computationally efficient, flexible, scalable and practical. in addition, we have also deployed our eceo model on the amazon elastic compute cloud. our study further confirms its efficiency and ease of use in a public cloud. availability: the source code of eceo is available atit is becoming increasingly important and challenging in genomewide association study (gwas) to identify single nucleotide polymorphisms (snps) associated with phenotypes such as human diseases (e.g. breast cancer, diabetes and heart attacks). traditionally, researchers focused on the association of individual snps with the phenotypes. such methods can only find weak associations as they ignore the genomic and environmental context of each snp . however, snps may interact (known as epistatic interaction) and jointly influence to whom correspondence should be addressed. the phenotypes. as such, there has been a shift away from the one-snp-at-a-time approach toward a more holistic and significant approach that detects the association between a combination of multiple snps with the phenotypes . in the meantime, the number of discovered snps is becoming larger and larger. for example, the dataset from the hapmap project contains 3.1 million snps and the 1000 genome project provides 15 million snps. from a computational perspective, it is very time consuming to determine the interactions of snps. given n snps, the number of k-locus is n c k = n! k!(nk)!. this renders existing statistical modeling techniques (which work well for a small number of snps) impractical. likewise, techniques that enumerate all possible interactions are not scalable for a large number of snps. to reduce the computation overhead, heuristics have also been developed. these schemes add a filtering step to select a fixed number of candidate epistatic interactions and fit them to a statistical model. however, these approaches risk missing potentially significant epistatic interactions that may have been filtered out. therefore, a scalable and efficient approach becomes attractive for such a computationally intensive task in a large-scale gwas. a promising solution to the computation challenge is to exploit parallel processing. there are a variety of high-performance computing solutions. for example, in, a tool is provided for processing single-locus and two-locus snps analyses using a supercomputer. however, it is not easy for researchers to rewrite their own programs on specialized hardware. as another example, in, two-locus snps analysis is performed using the graphics processing units (gpu). however, this requires the users to understand the gpu architecture well to fully exploit the computation power of the gpu. instead, we aim to develop a cloud-based solution which has a number of benefits. first, the mapreduce framework (available in most cloud services) offers high scalability, ease of programming and fault tolerance. secondly, most software can be easily deployed on the cloud and made accessible to all. thirdly, there are already lowcost commercially available cloud platforms [e.g. amazon elastic compute cloud (. fourthly, the pay-as-you-use model of such commercial platforms also makes them attractiveapache hadoop is an open-source equivalent implementation of the mapreduce framework, running on hadoop distributed file system (hdfs). we conduct a series of experiments on our local cluster with over 40 nodes, and a public cloud environment, amazon elastic compute cloud (amazon ec2). for our local cluster, each node consists of a ax3430 4(4) @ 2.4 ghz cpu running centos 5.4 with 8 gb memory and 2x 500g sata disks. for amazon ec2, we use 20 extra large instances, each with 8 ec2 compute units (4 virtual cores with 2 ec2 compute units each), 15 gb of memory and 1690 gb of local instance storage running on a 64-bit platform. moreover, since our tasks at hand are computationally intensive, we set the number of reducers per node to be equal to the number of cores at the node, which is 4 in our local cluster and 8 in ec2 instances. this guarantees that each reducer can get one core. therefore, there are a total of 4n and 8n reducers which can be run simultaneously on a n-node local cluster and ec2 clusters, respectively. effect of number of reducers: for hadoop application, a user can specify the number of reducers to be used in one job. because we have preconfigured the total number of reducers to be 4n for a n-node cluster, this may require multiple phases to complete a job. for example, if n = 30, then by specifying 120 reducers in one job, we can complete it in one phase; with 360 reducers, it will then take three phases to complete the job. our first experiment is to investigate the optimal number of reducers that should be set for one job based on a given cluster size. this experiment is conducted with a 50 000 snps dataset on a local 30-node cluster. note that all the datasets we used include 2000 samples.presents the running time for the greedy model. as shown, there is a certain optimal number of reducers that should be used. when the number of reducers is too small, the computation resources are not fully utilized. on the other hand, when the number of reducers is too large, the processing may require multiple phases that increases the communication page: 1050 10451051overhead. we note that the greedy model is optimal when the number of reducers corresponds to the actual configured value (i.e. 120).presents the running time for the square-chopping model. here, the 50 000 snps are evenly split into 10, 16, 20, 25 and 40 partitions corresponding to 55, 136, 210, 325 and 820 reducers needed. from the results, we observe that when the reducer number is close to a multiple of n, where n is the total number of reducers configured in the cluster, its performance is good; otherwise (n rn), reducers in the last phase, where r is the reducer number set in the job, will be wasted. looking at the results for the greedy and the square-chopping models, we observe that the square-chopping model is generally inferior to the greedy model. this is because of wasted reducers in the last phase (as discussed above). its performance, however, is closer to the greedy model as the partition number increases because the task in each reducer is smaller, and hence the wasted reducers in the last phase will not affect the total performance so much. based on these results, for the subsequent experiments, we only use the greedy model. scalability: first, we study the scalability of the eceo model as the system resources increase.shows the completion time to analyze 50 000 snps as the cluster sizes increases from 10 to 40 nodes. the reducer numbers in each job are set as 40, 80, 120 and 160, respectively. from the result, we can see that completion time reduces with increasing number of nodes. in fact, we observe a (almost) linear speedup in performance. when we double the resources, the execution time reduces by half. now, let us consider the scalability of eceo as the number of snps increases.shows the processing time for exhaustively computing all the significant interactions for two locus with 50 000, 100 000, 200 000 and 500 000 snps on a local 43node cluster, and output the results whose p-values are smaller than 0.05. we made two interesting observations. first, the result shows that our eceo offers a feasible and practical solution to perform pairwise epistasis for a large number of snps. according to, it would require 1.2 years to do the pairwise epistasis testing of 500 000 snps using the serial program on a 2.66 ghz single processor without parallel processing. our eceo model can accomplish this task in not more than 9 h using only a 43node cluster. second, we note that the processing time is essentially proportional to the number of interacting snp pairs to be evaluated.this article aims at providing an efficient epistasis computing model for large-scale epistatic interaction in gwas which can be run on a computing cluster (local or cloud-based). we have proposed an efficient and feasible solution, called eceo based on the mapreduce framework. as such, eceo inherits the nice properties of mapreduce, which is high scalability and good fault tolerance. moreover, it can leverage cloud computing with almost unlimited elastic computing resources. we have demonstrated the practical advantage of using eceo model to exhaustively search two-locus and three-locus epistatic interactions. our eceo model can also retrieve top-k most significant interactions. we have conducted extensive experimental study on a local cluster of over 40 nodes and 20 instances on amazon ec2. the results showed that our eceo model is computationally efficient, flexible, scalable and practical. as future work, we plan to implement more test statistics. we also plan to explore the possibility of integrating eceo as a filtering step to other methods, e.g. those based on statistical model fitting. finally, we plan to develop pruning strategies based on domain knowledge, and integrate these into our scheme. for example, by knowing that certain snps do not interact, their computations can be avoided totally.  
