screenbeam: a novel meta-analysis algorithm for functional genomics screens via bayesian hierarchical modeling motivation: functional genomics (fg) screens, using rnai or crispr technology, have become a standard tool for systematic, genome-wide loss-of-function studies for therapeutic target discovery. as in many large-scale assays, however, off-target effects, variable reagents potency and experimental noise must be accounted for appropriately control for false positives. indeed, rigorous statistical analysis of high-throughput fg screening data remains challenging, particularly when in-tegrative analyses are used to combine multiple sh/sgrnas targeting the same gene in the library. method: we use large rnai and crispr repositories that are publicly available to evaluate a novel meta-analysis approach for fg screens via bayesian hierarchical modeling, screening bayesian evaluation and analysis method (screenbeam). results: results from our analysis show that the proposed strategy, which seamlessly combines all available data, robustly outperforms classical algorithms developed for microarray data sets as well as recent approaches designed for next generation sequencing technologies. remarkably, the screenbeam algorithm works well even when the quality of fg screens is relatively low, which accounts for about 8095 of the public datasets. availability and implementation: r package and source code are available at: https://github.com/recent technological advances have significantly improved our ability to perform systematic and informative functional genomics (fg) studies in mammalian cells. specifically, during the past decade, rna interference (rnai) has become a standard technique for studying phenotype-specific gene function via suppression of gene specific mrna expression or translation . more recently, the crispr/cas9 system has emerged as an even more effective tool to implement complete gene knock-out. as a result, loss of function genome-scale screens have become popular, especially in pooled shrna library format . these have been widely used not only to identify essential genes in a specific context , but also genes that are differentially essential in different contexts or synthetic-lethal genes . results from these screens may ultimately inform discovery of novel therapeutic targets in tissue-specific, subtypespecific, or mutation-specific contexts, not only in cancer but in many other diseases and physiologic contexts. fg screen design has also been extended to identify tumor suppressors that increase cell growth upon repression or genes that modulate drug sensitivity . complementing direct knock out methodologies, the crispr/cas9 gene-editing technology has also been scaled up for high-throughput fg screens to study the effect of specific mutations on cellular phenotypes . however, progress in large-scale fg screens has been hampered by a nontrivial modelling of false positives . for example, many candidate therapeutic targets identified from shrna screens have failed validation in independent assays . among the possible reasons for lack of robustness, one must consider rnai off-target effects, variable potency and knock-down/out efficiency of rnai and crispr reagents, biological and technical noise in highthroughput screens , cell line-specific efficiency of viral pool infection and toxicity from additional viral vector expression cassettes (e.g. fluorescence reporter proteins). this suggests that sophisticated statistical methods may be required to model the complexity of these experiments, thus reducing false positive and negative rates and leading to improved, more robust interpretation of fg screen results. a key computational challenge, in genome-scale fg screens, is to score gene-level activity from individual reagents. whole genome fg libraries normally include multiple optimally designed shrnas hairpins or crispr sgrnas targeting the same gene to increase the likelihood of an effective gene knock-down/out. for instance, on average 23 or 5 hairpins per gene were used with gipz and trc shrna libraries, respectively. similarly 4 and 10 sgrnas per gene were used in the zhang or sabatini_lander crispr libraries, respectively. it is thus critical that such diverse evidence from multiple sh/sgrnas targeting a gene is integrated when assessing its contribution to a specific endpoint phenotype. in addition, both microarray and next generation sequencing technologies (ngs) technologies have been used to quantitatively assess sh/sgrnas representation or abundance in pooled fg screens, each one introducing bias and measurement noise. traditional methods to summarize gene-level activity usually rely on single-probe level analysis. specifically, shrna are first individually scored and then the scores of representative (e.g. highscoring) shrnas or of all shrnas targeting a specific gene are combined. several algorithms have been proposed to select or combine shrna-level evidence, including choosing the second best or most depleted shrna (riger_sb), averaging the two shrnas that produced the largest scores (riger_ws), performing enrichment analysis of all shrnas targeting one gene against all shrnas in the library (riger_ks), comparing rank distributions of effective size of all shrnas per gene (rsa) and more recent modelbased mageck and hitselect . an intrinsic limitation with all of these approaches is that they rely on the accurate assessment of an individual sh/sgrna activity, which is difficult to achieve in large-scale screens that typically have a relatively small number of replicate samples. moreover, off-target effects, variable silencing efficiency, differences among sh/sgrnas targeting the same gene, and experimental/technical noise make heuristic selection of representative sh/sgrnas problematic, causing significant false discovery rates. to overcome these limitations, we propose a novel screenbeam (screening bayesian evaluation and analysis method) algorithm via bayesian hierarchical modeling to directly assess gene-level activity from all relevant measurements. due to its robustness, hierarchical modeling , also known as multilevel modeling, has been increasingly valuable in largescaled omics studies . in this context, screenbeam algorithm analyses all sh/sgrnas targeting the same gene as a set, instead of one at the time, and then fits a linear mixture model that directly models the potential activity variability of different hairpins, as a random effect. this multi-probe analysis strategy improves parameter estimation, by increasing sample size, and reduces prediction error and false positive rate, by integrating information from multiple shrnas. use of bayesian inference with markov chain monte carlo (mcmc) techniques, in this analysis, further improves accuracy and robustness of scoring metrics. systematic benchmark assays, using large-scale, publicly available shrna (rnai) and sgrna (crispr) screens designed to profile gene essentiality by microarray or ngs , suggest that the screenbeam method robustly outperforms existing single-probe analysis algorithms. the screenbeam algorithm improvements are especially significant with assays with lower data quality, which accounts for about 80-95 of the fg screens considered in this manuscript.meta-analysis of shrna screening data increase gene-level inference robustness remains difficult. we proposed a novel multi-probe analysis strategy, screenbeam (screening bayesian evaluation andscreenbeamanalysis method), implemented via a bayesian hierarchical modeling, to address this problem. the evaluation results demonstrated that the screenbeam method outperformed traditional single-probe analysis approaches (riger and rsa) and recent model-based methods (mageck and hitselect). this was especially relevant when the screen data was in relatively low quality which account for about 8095 cases. hierarchical modelling, also known as partial pooling, can be viewed as a compromise between two extremes. one extreme, complete pooling, assumes the equal knock-down effect across all shrna classes targeting the same gene. the other extreme, no pooling, ignores the similarity of the replicates within one shrna group and treat each hairpin replicate separately. the assumptions of these two extreme methods are too strong for shrna screening design to be considered for integration of multiple shrna evidences because different shrnas targeting the same gene in the library might have significantly different silencing efficiencies. hierarchical modeling comprises two extremes by allowing between-group variance and considering within-group effects, thus making an appropriate solution to this question. the problem of multiple comparisons can also disappear in bayesian hierarchical models . partial pooling in hierarchical models shifts estimates toward each other whereas classical procedures for multiple comparison correction typically adjust p-values corresponding to intervals of fixed width. thus screenbeam fitting results in reliable and conservative estimates for main effects or gene-level effects in this context. for single-probe analysis strategy, a few other possible algorithms might be considered to integrate shrna-level scores for the same gene, for example, fishers method to combine signed p-values, or stouffers method to combine z-statistics . however, these integrating p-values or z-scores methods easily over-estimate the significance of gene-level activity and generate a long list of significant candidates. also, they ignore the magnitude of knock-down effects for each hairpin by only considering the statistical significance of how the effect is away from zero, and require strong assumptions. thereby, these methods might not be comparable to this screenbeam algorithm, or could be even worse than the other single-probe analysis methods. additionally, other enrichment analysis algorithms such as gsa have been used in this context and might perform better than ks-based gsea method; however, these algorithms still bear the drawbacks of single-probe analysis strategy, making them less powerful than screenbeam. the valuable point from enrichment-type methods that might improve screenbeam is to borrow information from all shrnas or genes in the library because current screenbeam algorithm only considers shrnas corresponding to one gene. looking at entire list of candidates might produce more robust statistics for cut-off based hits selection, but probably would not change the rank of a gene as a potential candidate. ngs has dominated as a cost-effective technology for quantitatively measuring the abundance of short-length dna or rna in a short time, and this multiplexing parallel technology has been used in genome-wide fg shrna and crispr sgrna screens. compared to microarray-based approaches, ngs offers several potential advantages in terms of coverage of targeting genes, flexibility of input library, scalability and dynamic range, which will possibly replace microarray for fg screens in the near future, however, the data quality of ngs-based genome-wide fg screens still has a big room to improve as only 6 of over 250 achilles screens are in good category, compared to 22 of microarray-based data (supplementary). the two existing best algorithms (mageck and hitselect) were specifically designed for ngs-based count data; however, screenbeam can handle both microarray-based intensity data and ngs-based count data. using conserved housekeeping genes or rnai screen-identified essential genes as the gold-standard of essential genes identified by loss-of-function screens, we demonstrated that the screenbeam algorithm improves the sensitivity by up to 50 of that by classical approaches without loss of precision. overall, screenbeam demonstrated the most robust and consistent performance identifying true hits in all scenarios, even from small-sized noisy high-throughput screens which accounts for about 8095 of the public datasets. high quality data in high-throughput loss-of-function screens is rarely achieved (22 in microarraybased and 6 in ngs-based data), due to a variety of error and variability sources. for high quality screens, method selection is less relevant as all of the tested methods had small-difference performance. yet, for the lower quality sh/sgrnas, within a high quality dataset, screenbeam would still produce significant improvements. as a result, there would be potential advantages even in this kind of datasets. in summary, we developed a novel hierarchical modelling algorithm within bayesian framework for meta-analysis of large-scale fg screens. this novel multi-probe approach performs more robustly than previously established analysis methods, especially with noisy high-throughput data.  
