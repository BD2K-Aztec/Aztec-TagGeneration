information-theoretic evaluation of predicted ontological annotations motivation: the development of effective methods for the prediction of ontological annotations is an important goal in computational biology , with protein function prediction and disease gene prioritization gaining wide recognition. although various algorithms have been proposed for these tasks, evaluating their performance is difficult owing to problems caused both by the structure of biomedical ontologies and biased or incomplete experimental annotations of genes and gene products. results: we propose an information-theoretic framework to evaluate the performance of computational protein function prediction. we use a bayesian network, structured according to the underlying ontology, to model the prior probability of a proteins function. we then define two concepts, misinformation and remaining uncertainty, that can be seen as information-theoretic analogs of precision and recall. finally, we propose a single statistic, referred to as semantic distance, that can be used to rank classification models. we evaluate our approach by analyzing the performance of three protein function predictors of gene ontology terms and provide evidence that it addresses several weaknesses of currently used metrics. we believe this framework provides useful insights into the performance of protein function prediction tools.ontological representations have been widely used in biomedical sciences to standardize knowledge representation and exchange . modern ontologies are typically viewed as graphs in which vertices represent terms or concepts in the domain of interest, and edges represent relational ties between terms (e.g. is-a, part-of). although, in theory, there are no restrictions on the types of graphs used to implement ontologies, hierarchical organizations, such as trees or directed acyclic graphs, have been frequently used in the systematization of biological experiments, organismal phenotypes or structural and functional descriptions of biological macromolecules. in molecular biology, one of the most frequently used ontologies is the gene ontology (go) , which standardizes the functional annotation of genes and gene products. the development of go was based on the premise that the genomes of all living organisms are composed of genes whose products perform functions derived from a finite molecular repertoire. in addition to knowledge representation, go has also facilitated large-scale analyses and automated annotation of gene product function . as the rate of accumulation of uncharacterized sequences far outpaces the rate at which biological experiments can be carried out to characterize those sequences, computational function prediction has become increasingly useful for the global characterization of genomes and proteomes as well as for guiding biological experiments via prioritization . the growing importance of tools for the prediction of go annotations, especially for proteins, presents the problem of how to accurately evaluate such tools. first, because terms can automatically be associated with their ancestors in the go graph, the task of an evaluation procedure is to compare the predicted graph with the true experimental annotation. furthermore, the structure of the ontology introduces dependence between terms, which must be appropriately considered when comparing two graphs. second, go, as most current ontologies, is generally unfinished and contains a range of specificities of functional descriptions at the same depth of the ontology . third, protein function is complex and context dependent; thus, a single biological experiment rarely results in complete characterization of a proteins function. this is particularly evident in cases when only high-throughput experiments are used for functional characterization, leading to shallow annotation graphs. this poses a problem in evaluation, as the ground truth is incomplete and noisy. finally, different computational models produce different outputs that must be accounted for. for example, some models simply predict an annotation graph, possibly associating it with a numerical score, whereas others assign a score to potentially each node in the ontology, with an expectation that a good decision threshold would be applied to provide useful annotations. there are two important factors related to the development of evaluation metrics. first, because both the experimental and predicted annotation of genes can be represented as subgraphs of the generally much larger go graph, it is unlikely that a given computational method will provide an exact prediction of the experimental annotation. thus, it is necessary to develop metrics that facilitate calculating degrees of similarity between pairs of graphs and appropriately address dependency between nodes. ideally, such a measure of similarity would be able to characterize not only the level of correct prediction of the true (albeit incomplete) annotation but also the level of misannotation. the second important factor related to the evaluation metric is its interpretability. this is because characterizing the predictors performance should be meaningful to a downstream user. ideally, an evaluation metric would have a simple probabilistic interpretation. in this article, we develop an information-theoretic framework for evaluating the prediction accuracy of computer-generated ontological annotations. we first use the structure of the to whom correspondence should be addressed. the author 2013. published by oxford university press. this is an open access article distributed under the terms of the creative commons attribution non-commercial license (http://creativecommons.org/licenses/ by-nc/3.0/), which permits non-commercial re-use, distribution, and reproduction in any medium, provided the original work is properly cited. for commercial re-use, please contact journals.permissions@oup.com ontology to probabilistically model, via a bayesian network, the prior distribution of protein experimental annotation. we then apply our metric to three protein function prediction algorithms selected to highlight the limitations of typically considered evaluation metrics. we show that our metrics provide added value to the current analyses of the strengths and weaknesses of computational tools. finally, we argue that our framework is probabilistically well founded and show that it can also be used to augment already existing evaluation metrics.in this work, we propose an information-theoretic framework for evaluating the performance of computational protein function prediction. we frame protein function prediction as a structured-output learning problem in which the output space is represented by consistent subgraphs of the go graph. we argue that our approach directly addresses evaluation in cases where there are multiple true and predicted (leaf) terms associated with a protein by taking the structure of the ontology and the dependencies between terms induced by a hierarchical ontology into account. our method also facilitates accounting for the high level of biased and incomplete experimental annotations of proteins by allowing for the weighting of proteins based on the information content of their annotations. because we maintain an information-theoretic foundation, our approach is relatively immune to the potential dissociation between the depth of a term and its information content, a weakness of often-used topological metrics in this domain such as precision/ recall or roc-based evaluation. at the same time, because we take a holistic approach to considering a proteins potentially large set of true or predicted functional associations, we resolve many of the problems introduced by the practice of aggregating multiple pairwise similarity comparisons common to existing semantic similarity measures. although there is a long history and a significant body of work in the literature regarding the use of semantic similarity measures , to the best of our knowledge, all such metrics are based on singlestatistics and are unable to provide insight into the levels of remaining uncertainty and misinformation that every predictor is expected to balance. therefore, the methods proposed in this work extend, modify and formalize several useful informationtheoretic metrics introduced during the past decades. in addition, both remaining uncertainty and misinformation have natural information-theoretic interpretations and can provide meaningful information to the users of computational tools. at the same time, the semantic distance based on these concepts facilitates not only the use of a single performance measure to evaluate and rank predictors but can also be exploited as a loss function during training. one limitation of the proposed approach is grounded in the assumption that a bayesian network, structured according to the underlying ontology, will perfectly model the prior probability distribution of a target variable. an interesting anomaly with this approach is that the marginal probability, and subsequently the information content, of a single term (i.e. consistent graph with a single leaf term) calculated from a bayesian network does not necessarily match the relative term frequency in the database (instead, the conditional probability tables are estimated as relative frequencies). ad hoc solutions that maintain the term information content are possible but would result in sacrificed interpretability of the metric itself. one such solution can be obtained via a recursive definition iav iv p u2pv iau and iaroot 0, where i(v) is estimated directly from the database. finally, rationalizing between evaluation metrics is a difficult task. the literature presents several strategies where protein sequence similarity, proteinprotein interactions or other data are used to assess whether a performance metric behaves according to expectations . in this work, we took a somewhat different approach and showed that the demonstrably biased protein function data can be shown to provide surprising results with well-understood prediction algorithms and conventional evaluation metrics. thus, we believe that our experiments provide evidence of the usefulness of the new evaluation metric.  
