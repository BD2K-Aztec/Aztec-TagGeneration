hapcol: accurate and memory-efficient haplotype assembly from long reads motivation: haplotype assembly is the computational problem of reconstructing haplotypes in dip-loid organisms and is of fundamental importance for characterizing the effects of single-nucleotide polymorphisms on the expression of phenotypic traits. haplotype assembly highly benefits from the advent of future-generation sequencing technologies and their capability to produce long reads at increasing coverage. existing methods are not able to deal with such data in a fully satisfactory way, either because accuracy or performances degrade as read length and sequencing coverage increase or because they are based on restrictive assumptions. results: by exploiting a feature of future-generation technologiesthe uniform distribution of sequencing errorswe designed an exact algorithm, called hapcol, that is exponential in the maximum number of corrections for each single-nucleotide polymorphism position and that minimizes the overall error-correction score. we performed an experimental analysis, comparing hapcol with the current state-of-the-art combinatorial methods both on real and simulated data. on a standard benchmark of real data, we show that hapcol is competitive with state-of-the-art methods, improving the accuracy and the number of phased positions. furthermore, experiments on realistically simulated data-sets revealed that hapcol requires significantly less computing resources, especially memory. thanks to its computational efficiency, hapcol can overcome the limits of previous approaches, allowing to phase datasets with higher coverage and without the traditional all-heterozygous assumption.diploid organisms such as humans contain two sets of chromosomes, one from each parent. reconstructing the two distinct copies of each chromosome, called haplotypes, is crucial for characterizing the genome of an individual. the process is known as phasing or haplotyping and the provided information may be of fundamental importance for many applications, such as analyzing the relationships between genetic variation and gene function, or between genetic variation and disease susceptibility . in diploid species, haplotyping requires v c the author 2015. published by oxford university press. all rights reserved. for permissions, please e-mail: journals.permissions@oup.com assigning the variants to the two parental copies of each chromosome, which exhibit differences in terms of single-nucleotide polymorphisms (snps). since a large scale direct experimental reconstruction of the haplotypes from the collected samples is not yet cost-effective , a computational approachcalled haplotype assemblythat considers a set of reads, each one sequenced from a chromosome copy, has been proposed. reads (also called fragments) have to be assigned to the unknown haplotypes, using a reference genome in a preliminary mapping phase, if available. this involves dealing in some way with sequencing and mapping errors and leads to a computational task that is generally modelled as an optimization problem . minimum error correction (mec) is one of the prominent combinatorial approaches for haplotype assembly. it aims at correcting the input data with the minimum number of corrections to the snp values, such that the resulting reads can be unambiguously partitioned into two sets, each one identifying a haplotype. wmec is the weighted variant of the problem, where each possible correction is associated with a weight that represents the confidence degree assigned to that snp value at the corresponding position. this confidence degree is a combination of the probability that an error occurred during sequencing (phred-based error probability) for that base call and of the confidence of the read mapping to that genome position. the usage of such weights has been experimentally validated as a powerful way to improve accuracy . haplotype assembly benefits from technological developments in genome sequencing. in fact, the advent of next-generation sequencing (ngs) technologies provided a cost-effective way of assembling the genome of diploid organisms. however, to assemble accurate haplotypes, it is necessary to have reads that are long enough to span several different heterozygous positions . this kind of data is becoming increasingly available with the advent of future-generation sequencing technologies such as single molecule real-time technologies like pacbio rs ii (http:// www.pacificbiosciences.com/products/) and oxford nanopore flow cell technologies like minion (https://www.nanoporetech.com/). these technologies, thanks to their ability of producing single end reads longer than 10 000 bases, eliminate the need of paired-end data and have already been used for tasks like genome finishing and haplotype assembly . besides read length, the future-generation sequencing technologies produce fragments with novel features, such as the uniform distribution of sequencing errors, that are not properly addressed (or exploited) in most of the existing methods that, instead, are tailored to the characteristics of traditional ngs technologies. recently, mec and wmec approaches have been used in the context of long reads, confirming that long fragments allow to assemble haplotypes more accurately than traditional short reads . since mec is np-hard , exact solutions have exponential complexity. different approaches tackling the computational hardness of the problem have been proposed in literature. integer linear programming techniques have been recently used , but the approach failed to optimally solve some difficult blocks. there were also proposed fixedparameter tractable (fpt) algorithms that take time exponential in the number of variants per read and, hence, are well-suited for short reads but become unfeasible for long reads. for this kind of data, heuristic approaches have been proposed to respond to the lack of exact solutions . most of the proposed heuristics, such as refhap , make use of the traditional all-heterozygous assumption, that forces the heterozygosity of all the phased positions. these heuristics have good performances but do not offer guarantees on the optimality of the returned solution . two recent articles aim at processing future-generation long reads by introducing algorithms exponential in the sequencing coverage, a parameter which is not expected to grow as fast as read length with the advent of future-generation technologies. the first algorithm, called probhap , is a probabilistic dynamic programming algorithm that optimizes a likelihood function generalizing the objective function of mec. albeit probhap is significantly slower than the previous heuristics, it obtained a noticeable improvement in accuracy. the second approach, called whatshap , is the first exact algorithm for wmec that is able to process long reads. it was shown to be able to obtain a good accuracy on simulated data of long reads at coverages up to 20 and to outperforms all the previous exact approaches. however, it cannot handle coverages higher than 20, and its performance evidently decreases when approaching that limit. in this article, we exploit a characteristic of future-generation technologies, namely the uniform distribution of sequencing errors, for introducing (section 2) an exact fpt algorithm for a new variant, called k-cmec, of the wmec problem where the parameters are (i) the maximum number k of corrections that are allowed on each snp position and (ii) the coverage. the new algorithm, called hapcol, is based on a characterization of feasible solutions given inand its time complexity is ocov k1 lm (albeit it is possible to prove a stricter bound), where cov is the maximum coverage, l is the read length and m is the number of snp positions. hapcol is able to work without the all-heterozygous assumption. in section 3, we experimentally compare accuracy and performance of hapcol on real and realistically simulated datasets with three state-of-the-art approaches for haplotype assemblyrefhap, probhap and whatshap. on a real standard benchmark of long reads , we executed each tool under the all-heterozygous assumption, since this dataset has low coverage ($3 on average) and since the covered positions are heterozygous with high confidence. hapcol turns out to be competitive with the considered methods, improving the accuracy and the number of phased positions. we also assessed accuracy and performance of hapcol on a large collection of realistically simulated datasets reflecting the characteristics of future-generation sequencing technologies that are currently (or soon) available (coverage up to 25, read length from 10000 to 50 000 bases, substitution error rate up to 5 and indel rate equal to 10) . when considering higher coverages, interesting applications such as snp calling or heterozygous snps validation become feasible and reliable . since these applications require that haplotypes are reconstructed without the all-heterozygous assumption, on the simulated datasets we only considered the tools that do not rely on this assumptionwhatshap and hapcol. results on the simulated datasets with coverage 1520 show that hapcol, while being as accurate as whatshap (they achieve an average error of $2), is faster and significantly more memory efficient ($2 times faster and $28 times less memory). the efficiency of hapcol allows to further improve accuracy. indeed, the experimental results show that hapcol is able to process datasets with coverage 25 on standard workstations/ small servers (whereas whatshap exhausted all the available memory, 256 gb) and that, since the number of ambiguous/uncalled positions decreases, the haplotypes reconstructed by hapcol at coverage 25 are $9 more accurate than those reconstructed at coverage 20.  
