sequence analysis lossy compression of quality scores in genomic data motivation: next-generation sequencing technologies are revolutionizing medicine. data from sequencing technologies are typically represented as a string of bases, an associated sequence of per-base quality scores and other metadata, and in aggregate can require a large amount of space. the quality scores show how accurate the bases are with respect to the sequencing process, that is, how confident the sequencer is of having called them correctly, and are the largest component in datasets in which they are retained. previous research has examined how to store sequences of bases effectively; here we add to that knowledge by examining methods for compressing quality scores. the quality values originate in a continuous domain, and so if a fidelity criterion is introduced, it is possible to introduce flexibility in the way these values are represented, allowing lossy compression over the quality score data. results: we present existing compression options for quality score data, and then introduce two new lossy techniques. experiments measuring the trade-off between compression ratio and information loss are reported, including quantifying the effect of lossy representations on a downstream application that carries out single nucleotide poly-morphism and insert/deletion detection. the new methods are dem-onstrably superior to other techniques when assessed against the spectrum of possible trade-offs between storage required and fidelity of representation. availability and implementation: an implementation of the methods described here is available at https://github.com/rcanovas/genome sequencing methods have evolved at the same astonishing rate as computing technology. next-generation devices parallelize the process, producing billions of sequences (reads) and generating file sizes potentially in the terabyte range, at costs that are decreasing on a yearly basis. each read is a fragment of data extracted from the processing of a single genome, represented as a string of bases. as well, a number of metadata fields are associated with each read. some of these fields are more expensive to store than the sequence of bases.the mechanics of effectively storing and querying this information are fundamental to the field of bioinformatics . several standard formats to store genome data have been adopted, each aiming to be easy to manipulate and parse using text-processing tools such as perl and python. the most common are the fasta and fastq formats and the sam/bam formats .we have described and measured the performance of a range of lossy compression techniques for quality scores. the qualcomp approach, and our two new methods, offer superior trade-off options when fidelity is assessed according to the measures in. our new approaches outperformed qualcomp in the max:min distance criteriona measure that is well-suited, we believe, to bioinformatics applications. our experiments also quantified the extent to which use of lossy compression affected the performance of a typical downstream application of genetic data, and demonstrate that variation detection can still be reliably carried out, even with relatively compact storage of the quality scores.the approaches described here make use of sequentially greedy generation of blocks. one interesting question that we have not yet examined is whether mechanisms for globally optimal block construction will make a measurable difference in overall outcomes. another area that we have not yet fully explored is the coding mechanisms used for the block representatives and for the block lengths; it may be that tailored codes (rather than binary and gamma) can offer further space savings once the particular characteristics of the data streams are taken into account. we are also interested in quantifying the effect that lossy compression has on other downstream applications of genetic sequencing data.  
