an automated workflow for parallel processing of large multiview spim recordings selective plane illumination microscopy (spim) allows to image developing organisms in 3d at unprecedented temporal resolution over long periods of time. the resulting massive amounts of raw image data requires extensive processing interactively via dedicated graphical user interface (gui) applications. the consecutive processing steps can be easily automated and the individual time points can be processed independently, which lends itself to trivial paralleliza-tion on a high performance computing (hpc) cluster. here, we introduce an automated workflow for processing large multiview, multichannel, multiillumination time-lapse spim data on a single workstation or in parallel on a hpc cluster. the pipeline relies on snakemake to resolve dependencies among consecutive processing steps and can be easily adapted to any cluster environment for processing spim data in a fraction of the time required to collect it. availability and implementation: the code is distributed free and open source under the mit licensethe duration and temporal resolution of 3d fluorescent imaging of living biological specimen is limited by the amount of laser light exposure the sample can survive. selective plane illumination microscopy (spim) alleviates this by illuminating only the imaged plane thus reducing photo damage dramatically. additionally, spim achieves fast acquisition rates due to sensitive wide-field detectors and sample rotation enables complete coverage of large, nontransparent specimen. taken together, spim allows imaging of developing organisms in toto at single cell resolution with unprecedented temporal resolution over long periods of time . this powerful technology produces massive, terabyte size datasets that need computationally expensive and time-consuming processing before analysis. existing software solutions implemented in fiji (; preibisch, unpublished (https://github.com/fiji/spim_registration)) or in zeiss zen black are performing chained processing steps on a single computer and require user inputs via a gui. as the spatial and temporal resolution of the light sheet data increase, such approaches become inconvenient since processing can take days. in controlled experiments, spim image processing is robust enough to be automated and key steps are independent from time point to time point. hpc is inherently designed for such time v c the author 2015. published by oxford university press.we compared the performance of the pipeline on a 175 gb, single channel spim recording of a drosophila embryo consisting of 90 time points and 5 views, processed either on a single computer or on a hpc cluster (supplementary). the processing using average fusion takes almost precisely one day on a single powerful computer. in contrast, using the full cluster resource the dataset can be processed in 1 h 31 min, which represents a 16-fold speedup in processing. since the time-lapse covers 23 h of drosophila embryonic development the processing becomes real time with respect to the acquisition. using deconvolution on a cluster with only 4 gpus (supplementary) still brings a more than 3-fold speed up (supplementary). a dataset of 2.2 tb in size with 715 time points would take an estimated week to process on a single computer. using this method, the processing is reduced to only 13 h with typical cluster workload from other users.  
