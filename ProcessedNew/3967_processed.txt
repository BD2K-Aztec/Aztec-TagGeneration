genetics and population analysis reliable abc model choice via random forests motivation: approximate bayesian computation (abc) methods provide an elaborate approach to bayesian inference on complex models, including model choice. both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard abc techniques. results: we propose a novel approach based on a machine learning tool named random forests (rf) to conduct selection among the highly complex models covered by abc algorithms. we thus modify the way bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with rf and postponing the approximation of the posterior probability of the selected model for a second stage also relying on rf. compared with earlier implementations of abc model choice, the abc rf approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least 50) and (iv) it includes an approximation of the posterior probability of the selected model. the call to rf will undoubtedly extend the range of size of datasets and complexity of models that abc can handle. we illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. availability and implementation: the proposed methodology is implemented in the r package abcrf available on the cran.approximate bayesian computation (abc) represents an elaborate statistical approach to model-based inference in a bayesian setting in which model likelihoods are difficult to calculate (due to the complexity of the models considered). since its introduction in population genetics , the method has found an ever increasing range of applications covering diverse types of complex models in various scientific fields (see, e.g.). the principle of abc is to conduct bayesian inference on a dataset through comparisons with numerous simulated datasets. however, it suffers from two major difficulties. first, to ensure reliability of the method, the number of simulations is large; hence, it proves difficult to apply abc for large datasets (e.g. in population genomics where tens tohundred thousand markers are commonly genotyped). second, calibration has always been a critical step in abc implementation . more specifically, the major feature in this calibration process involves selecting a vector of summary statistics that quantifies the difference between the observed data and the simulated data. the construction of this vector is therefore paramount and examples abound about poor performances of abc model choice algorithms related with specific choices of those statistics , even though there also are instances of successful implementations. we advocate a drastic modification in the way abc model selection is conducted: we propose both to step away from selecting the most probable model from estimated posterior probabilities and to reconsider the very problem of constructing efficient summary statistics. first, given an arbitrary pool of available statistics, we now completely bypass selecting among those. this new perspective directly proceeds from machine learning methodology. second, we postpone the approximation of model posterior probabilities to a second stage, as we deem the standard numerical abc approximations of such probabilities fundamentally untrustworthy. we instead advocate selecting the posterior most probable model by constructing a (machine learning) classifier from simulations from the prior predictive distribution (or other distributions in more advanced versions of abc), known as the abc reference table. the statistical technique of random forests (rf) represents a trustworthy machine learning tool well adapted to complex settings as is typical for abc treatments. once the classifier is constructed and applied to the actual data, an approximation of the posterior probability of the resulting model can be produced through a secondary rf that regresses the selection error over the available summary statistics. we show here how rf improves upon existing classification methods in significantly reducing both the classification error and the computational expense. after presenting theoretical arguments, we illustrate the power of the abc-rf methodology by analyzing controlled experiments as well as genuine population genetics datasets.this article is purposely focused on selecting a statistical model, which can be rephrased as a classification problem trained on abc simulations. we defend here the paradigm shift of assessing the best fitting model via a rf classification and in evaluating our confidence in the selected model by a secondary rf procedure, resulting in a different approach to precisely estimate the posterior probability of the selected model. we further provide a calibrating principle for this approach, in that the prior error rate provides a rational way to select the classifier and the set of summary statistics which leads to results closer to a true bayesian analysis. compared with past abc implementations, abc-rf offers improvements at least at four levels: (i) on all experiments we studied, it has a lower prior error rate; (ii) it is robust to the size and choice of summary statistics, as rf can handle many superfluous statistics with no impact on the performance rates (which mostly depend on the intrinsic dimension of the classification problem , a characteristic confirmed by our results); (iii) the computing effort is considerably reduced as rf requires a much smaller reference table compared with alternatives (i.e. a few thousands versus hundred thousands to billions of simulations) and (iv) the method is associated with an embedded and error-freereliable abc model choiceevaluation which assesses the reliability of abc-rf analysis. as a consequence, abc-rf allows for a more robust handling of the degree of uncertainty in the choice between models, possibly in contrast with earlier and over-optimistic assessments. because of a massive gain in computing and simulation efforts, abc-rf will extend the range and complexity of datasets (e.g. number of markers in population genetics) and models handled by abc. in particular, we believe that abc-rf will be of considerable interest for the statistical processing of massive snp datasets whose production rapidly increases within the field of population genetics for both model and non-model organisms. once a given model has been chosen and confidence evaluated by abc-rf, it becomes possible to estimate parameter distribution under this (single) model using standard abc techniques or alternative methods such as those proposed by  
