deliminateâ€”a fast and efficient method for loss-less compression of genomic sequences an unprecedented quantity of genome sequence data is currently being generated using next-generation sequencing platforms. this has necessitated the development of novel bioinformatics approaches and algorithms that not only facilitate a meaningful analysis of these data but also aid in efficient compression, storage, retrieval and transmission of huge volumes of the generated data. we present a novel compression algorithm (deliminate) that can rapidly compress genomic sequence data in a loss-less fashion. validation results indicate relatively higher compression efficiency of deliminate when compared with popular general purpose compression algorithms, namely, gzip, bzip2 and lzma. availability and implementation: linux, windows and mac implementations (both 32 and 64-bit) of deliminate are freely available for download at: http://metagenomics.atc.tcs.com/compression/ deliminate. contact:the volume of sequence data being deposited in major public sequence repositories is witnessing an exponential rate of growth. this presents an important challenge with respect to developing efficient compression and storage methods. addressing this challenge directly/indirectly impacts bandwidth and cost-related issues of data transmission/dissemination. major public sequence repositories store sequence data either in its raw format (as fastq files) or in a processed format (as single/multi-fasta files). this study pertains to compression of files containing nucleotide sequences in single/multi-fasta format. such files contain information of sequences along with their corresponding headers. four nucleotide bases, i.e. a, t/u, g and c, usually constitute the majority of text characters in the sequence portion of fasta files. using a two-bit encoding for these four characters is obviously the simplest way of reducing the file/data size by a factor of four. several studies (see review by) have proposed various approaches to further reduce the bits/base ratio to less than 2. while a majority of these approaches work by tracing (and optimally encoding) repeat patterns in genomic data, others work by capturing differences between various sequences constituting a multi-fasta file. a few reference-based strategies have also been reported recently. in spite of the availability of several specialized genome compression algorithms (sgcas), which achieve reasonably high compression gains, it is observed that the majority of public sequence repositories still usey general purpose compression algorithms (gpcas) such as gzip and bzip2 for compressing and storing sequence data. the likely reasons for this observation are the following: (1) unlike gpcas, most sgcas are not equipped to handle non-atgc characters. this limits their capability to perform a loss-less compression/decompression. (2) in contrast to gpcas, the compression efficiency of most sgcas is generally observed to be achieved at the cost of huge time and memory requirements. (3) the utility of reference-based compression approaches is also subject to availability of closely related reference genomes. in summary, the above observations indicate the need for a compression algorithm (and a practical implementation of the same) that (1) achieves loss-less compression and decompression as rapidly as gpcas, (2) has significantly better compression efficiency than gpcas, (3) has low memory requirements thus enabling it to handle files of any size and (4) is compatible with popular software platforms (unix/linux, windows, etc.) and system architectures (32-bit or 64-bit). in this article, we present deliminate (a combination of delta encoding and progressive elimination of nucleotide characters) a novel method that implements the above mentioned features of an ideal compression algorithm.four different datasets [fna, ffn, eukaryotic and nextgeneration sequencing (ngs) dataset] were used for evaluating the compression efficiency of deliminate. details of these datasets are provided in supplementary material 2. files constituting these four datasets were compressed using deliminate (both variants), bzip2, gzip and lzma. all experiments were performed on a linux workstation (32-bit) having a 2.33 ghz dual core processor and 2gb ram. the results obtained were compared in terms of percentage compression ratio (pcr) and the time taken for compression and decompression. pcr was calculated using the following formula. pcr size of compressed dataset=size of original dataset 100:the results of both variants of deliminate (in terms of pcr) when compared with various gpcas are summarized in. detailed results for individual files in all validation datasets (for all methods) are provided in supplementary tables 14. the results indicate that both variants of deliminate achieve better compression ratios when compared with other algorithms. except for the ngs dataset, all gpcas fail to achieve a compression ratio lower than 2 bits/ base (525) in most cases. in contrast, for all datasets, both variants of deliminate generate compression ratios 525, thus indicating significant compression gains. files constituting the ngs dataset are observed to be highly compressible using gpcas. this is expected since ngs datasets are typically characterized by high sequencing coverage, and consequently end up having extensively repeated sequence strings, making them highly amenable to compression. interestingly, even for the ngs dataset, both variants of deliminate are observed to significantly outperform gpcas. the percentage improvement in compression ratio (picr) of delim-2 (i.e. the better performing deliminate variant) when compared with gpcas was quantified using the following formula. picr 1 pcr of deliminate=pcr of compared algorithm  100:values of picr are summarized in supplementary. the results in this table indicate that the compression ratios obtained using delim-2 are on an average 727 better when compared with that obtained using gpcas. in some cases, the compression gains (in terms of picr) obtained using delim-2are observed to be as high as 40. the performance efficiency of delim-2 was also compared with two specialized genome compression algorithms, namely, gencompress and xm-compress . details of the datasets used in this evaluation and a discussion of the results obtained are provided in supplementary material 3. the results with respect to time indicate that the compression time of delim-2 is slightly higher when compared with default gzip and bzip2 algorithms . however, it is observed that the compression speed of delim-2 is around two to seven times faster when compared with gzip with 9 option (i.e. the best compression mode) and lzma algorithms. although the decompression speed of delim-2 is faster than its compression speed, it is not as fast as the decompression speed of gpcas. it is to be noted that the values (of compression time) provided for both variants of deliminate refer to the total time taken for compressing a fasta file, which includes the final 7-zip archiving step. furthermore, all values (of compression and decompression time) indicated incorrespond to the real or wall-clock elapsed time (not user system time). although measuring real time confers an advantage to programs such as deliminate (both variants) and 7-zip (which can simultaneously utilize 2 or more available processing cores, unlike single-cpu compression tools such as gzip and bzip2), this practice was adopted given that most of the present day workstations possess two or more processing cores. in the present comparison, both variants of deliminate (including the piped calls to 7-zip) were run with two processing cores. on another note, lzmathe best performing gpca, in terms of compression ratiouses a low amount of memory during compression (default mode) compared with deliminate. it may appear that providing higher amount of memory would enable lzma to achieve better compression ratio. however, results inindicate that no significant compression gains (when compared with the default mode) could be achieved by lzma even when 2 gb of memory (maximum possible allocation on the benchmarking system) was allocated to it for compressing the validation datasets. overall, validation results suggest that both variants of deliminate (especially delim-2) are able to achieve significant gains in compression ratio as well as in processing time by providing genome sequences represented in a unique (partially compressed) format as input to a general purpose compression algorithm. in order to verify as to what extent the splitting and transformation steps adopted by delim-2 contribute to these compression gains, two experiments were performed. in the first experiment, the f1 file obtained at the end of the first phase (see supplementary material 1) containing a single un-split stream of a, t, g and c was converted to binary format (2 bits for each nucleotide base) and was provided as input to 7-zip. the results of this experiment, shown in supplementary table 6 (under the head delim-b), demonstrate that the compression ratio of delim-2 and delim-b are more or less comparable. in the second experiment, the bits corresponding to nucleotides at odd and even positions in the f1 file were split into two binary data streams which were then compressed in parallel using 7-zip. the results obtained in this experiment (provided inunder the head delim-s) also indicate that delim-2 and delim-s obtain similar compression levels. however, a comparison of compression time of delim-2, delim-s and delim-b (supplementary) suggests that splitting the data streams and processing them using parallel threads (as in delim-2 and delim-s) positively impacts the overall time required for compression. these results therefore imply that though the transformation steps adopted in delim-2 do not help it attain any significant gain in compression ratio (compared with what could be attained by providing a homogeneous stream of four standard nucleotides to 7-zip/lzma), they contribute to a faster compression process.  
