data and text mining effect of separate sampling on classification accuracy motivation: measurements are commonly taken from two phenotypes to build a classifier, where the number of data points from each class is predetermined, not random. in this separate sampling scenario, the data cannot be used to estimate the class prior probabilities. moreover, predetermined class sizes can severely degrade classifier performance, even for large samples. results: we employ simulations using both synthetic and real data to show the detrimental effect of separate sampling on a variety of classification rules. we establish propositions related to the effect on the expected classifier error owing to a sampling ratio different from the population class ratio. from these we derive a sample-based mini-max sampling ratio and provide an algorithm for approximating it from the data. we also extend to arbitrary distributions the classical population based anderson linear discriminant analysis minimax sampling ratio derived from the discriminant form of the bayes classifier. availability: all the codes for synthetic data and real data examples are written in matlab. a function called mmratio, whose output is an approximation of the minimax sampling ratio of a given dataset, is also written in matlab.the medical community is being confronted with serious problems of reproducibility in the development of biomarkers. the issue has been highlighted by a recent report regarding comments by janet woodcock, fda drug division head. the report states, based on conversations woodcock has had with genomics researchers, she estimated that as much as 75 percent of published biomarker associations are not replicable. this poses a huge challenge for industry in biomarker identification and diagnostics development, she said . many issues affect reproducibility, including the measurement platform, specimen handling, data normalization and sample compatibility between the original and subsequent studies. these matters concern experimental procedures and are not our concern here; rather, we are interested in the methodology for designing classifiers. one issue in this regard is the impact of inaccurate error estimation owing to small samples. this has been previously quantified . here we are interested in a different problem, one that will confront us even if we have large samples and perfect error estimation: the effect of having predetermined sample sizes so that sampling is not random. in classification studies it is typically a tacit assumption that sampling is random; indeed, it is commonplace for this assumption to be made throughout a text on classification. for instance, devroye et al. declare on page 2 of their text that all sampling is random . the assumption is so pervasive that it can be applied without mention. with regard to the problem at hand,state, in typical supervised pattern classification problems, the estimation of the prior probabilities presents no serious difficulties. but, in fact, there are often serious difficulties. under the assumption of random sampling, the data set,, is drawn independently from a fixed distribution of feature-label pairs, (x, y); in particular, this means that if a sample of size n is drawn for a binary classification problem, then the numbers of sample points, n 0 and n 1 , in classes 0 and 1, respectively, are random variables such that n 0 n 1 n. an immediate consequence of the random-sampling assumption is that the prior probability c pr(y 0) can be consistently estimated by the sampling ratio, namely, by c n0 n. consistency is nothing but bernoullis weak law of large numbers, namely, n0 n ! c in probability. thus, if the sample is large, we can expect the sampling ratio to be close to the prior probability. suppose the sampling is not random, in the sense that the ratios n0 n and n1 n are chosen prior to sampling. in this separate (stratified) sampling case, s n s n0 [ s n1 , where the sample points in s n0 and s n1 are selected randomly from 0 and 1 but, given n, the individual class counts n 0 and n 1 are not random. then, in effect, we have no sensible estimate of c. one could let c n0 n , but there would be no reason to do so. since our aim is to use the data to train a classifier, does the inability to consistently estimate c matter? clearly in the case of linear discriminant analysis (lda) it does, since the lda classifier is defined by n x 1 if d samp x 0 and n x 0 if d samp x40, whereand l 0 and l 1 are the sample means of the class-conditional populations 0 and 1 , respectively, and d is the pooled sample covariance matrix. the rationale for the lda discriminant is that the estimators converge to the population parameters to whom correspondence should be addressed. as n ! 1, in which case the resulting discriminant, d bayes x, defines the bayes (optimal) classifier in the two-class gaussian model with common covariance matrix. it is obvious from equation (1) that an estimate of c is required for lda and a bad choice of c will negatively impact the classifier. this fact, which is a consequence of separate sampling, has long been recognized . the situation is less transparent with model-free classification rules such as support vector machines. in this article we use simulation to study the effect of separate sampling on several different classification rules, where the role of c does not appear explicitly in classifier learning. we generate separate samples with different ratios r n0 n and consider the expected error, e n jr, of the designed classifier, given r, where the error of classifier n is defined by n pr n x 6 y, the probability of misclassification. we will see that the penalty for separate sampling without knowledge of c can be severe. with random (or, mixed) sampling, rather than being fixed prior to sampling, r is a sample-dependent random variable. in this case, e n jr denotes the expectation of the error conditioned on r and the expected classification error is given by e n e r e n jr, where the outer expectation is relative to the distribution of r. the classifier error is likely to be smaller when the sampling ratio r is close to c. hence, if one happens to fix r sufficiently close to c, then e n jr5 e n . because r ! c in probability as n ! 1 for mixed sampling, as n gets larger the distribution of r gets more tightly concentrated around c, so that the distribution of e n jr (as function of r) gets more tightly packed around e n , which in turn means that to have e n jr5 e n one must choose r very close to c. to illustrate this phenomenon, consider 2d gaussian class-conditional densities with means at (0.3, 0.3) and (0.8, 0.8), possessing common covariance matrix 2 i, where i is the identity matrix and 2 0:4, and with c 0:6: for this model, the bayes error is bayes 0:27.shows the difference e n e n jr for different values of r and different sample sizes when using lda. if r 0:6, then e n jr5 e n for all n. if r 0:7, which is fairly close to c 0:6, then e n jr5 e n for n 25. notice the lack of symmetry, both 0.5 and 0.7 being equally close to c. this should not be surprising because we should not expect the distribution of e n jr to be symmetric. let us examinefrom the practitioners perspective. suppose that cost limits the sample to a given size n. if the sample is random, then the expected error of the designed classifier will be e n , which is unknown since the feature-label distribution is unknown. consider three cases: (i) if c is accurately known from existing population statistics regarding the two classes, say brca1 and brca2 breast cancer, then no matter what the sample size, it is best to do separate sampling with n 0 cn; (ii) if c is approximately known, meaning that the practitioner believes that c is close to c 0 , then, for small n, it may be best, or at least acceptable, to do separate sampling with n 0 c 0 n, and the results will likely still be acceptable for large n, though not as good as with random sampling; (iii) if the practitioner has no idea what c is, then sampling must be random because the penalty for separate sampling can be very large. while, at this point, these comments refer specially to, which is for lda, a salient point to be made in this article is that they are quite general and, moreover, can be extended to the commonplace separate sampling situation where one cannot choose n 0 and n 1. why is all of this a major issue for bioinformatics? simply put separate sampling is ubiquitous in bioinformatics, in particular, with genomic classification, where a standard approach is to take tissue samples from two classes, say, different types of cancer or different stages of cancer, for which the number of specimens in each class is not chosen randomly, and then to design a classifier. the supplementary material lists 20 published studies using separate sampling. in each case we give the classification problem, sample sizes, classification rule and error estimator. even if an error estimate is exact for the problem at handthat is, for the sampling ratio represented by the datawhat does it mean relative to the classification error for future observations (say, patients)? that depends on the true prior probabilities, which we do not know.  
