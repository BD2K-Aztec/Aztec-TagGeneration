genome analysis musket: a multistage k-mer spectrum-based error corrector for illumina sequence data motivation: the imperfect sequence data produced by next-generation sequencing technologies have motivated the development of a number of short-read error correctors in recent years. the majority of methods focus on the correction of substitution errors, which are the dominant error source in data produced by illumina sequencing technology. existing tools either score high in terms of recall or precision but not consistently high in terms of both measures. results: in this article, we present musket, an efficient multistage k-mer-based corrector for illumina short-read data. we use the k-mer spectrum approach and introduce three correction techniques in a multistage workflow: two-sided conservative correction, one-sided aggressive correction and voting-based refinement. our performance evaluation results, in terms of correction quality and de novo genome assembly measures, reveal that musket is consistently one of the top performing correctors. in addition, musket is multi-threaded using a masterslave model and demonstrates superior parallel scalability compared with all other evaluated correctors as well as a highly competitive overall execution time. availability: musket is available at http://musket.the emergence and rapid progress of next-generation sequencing (ngs) technologies has enabled the high-throughput production of short dna sequences (reads) at low cost. the ever increasing throughput and decreasing cost has significantly altered the landscape of whole-genome sequencing, providing an opportunity for scientists to initiate whole-genome sequencing projects for almost any organism, including those whose genomes span billions of base pairs, such as the giant panda and humans . many ngs sequencing technologies have been developed, among which illumina is most widely used. however, reads produced from ngs platforms are never perfect and can contain various types of sequencing errors, including substitutions and indels (insertions or deletions). these sequencing errors complicate data processing for many biological applications such as de novo genome assembly and short-read mapping . to improve data quality, one frequently used approach is to trim reads from the more error-prone 3 0 ends, which unfortunately results in loss of information . this has ignited the interest of researchers in conceiving more sophisticated algorithms to detect and correct sequencing errors in ngs data, for example, schroder. as substitution is the dominant error type for data produced by illumina sequencing technology, most approaches focus on correcting this type of errors . the core of substitution-error-based methods is to compute consensus bases using the highly redundant coverage information. when a sequencing error occurs in a read that originated from a certain position on the genome, all reads covering the erroneous position could be piled up to compute the consensus base. considering that sequencing errors are generally random and infrequent, this consensus base is likely to be correct. however, as we assume that the source genome is unknown beforehand, we can neither determine the read locations on the genome nor the correctness of reads directly. instead, reads that cover overlapping genomic positions can be inferred by assuming that they typically share common substrings. furthermore, we can approximate the source genome using a k-mer spectrum, which was first introduced by. given a dataset with sufficient coverage of a genome, the k-mer spectrum is defined as the set of all k-mers in the dataset, where the k-mers whose multiplicity exceeds a coverage cut-off are deemed to be trusted and otherwise, untrusted. the first k-mer spectrum-based corrector was proposed by, using an iterative spectral alignment problem approach. cuda-ec and decgpu accelerated this spectral alignment problem approach using graphics processing unit computing. to further improve correcting quality,introduced a dynamic programming approach that corrects errors by minimizing edit distances. the soap corrector adopted a similar approach. quake introduced a probabilistic model derived from base quality scores of reads. for a k-mer, quake accumulates the correctness probability of all its occurrences and defines this sum as the multiplicity of the k-mer, instead of just the number of occurrences. reptile relies on a hamming graph to resolve ambiguities for a tile-based correction, whereas hammer combines the hamming graph with a probabilistic model to deal with datasets with non-uniform coverage. hitec supports multiple k-mer sizes in a single launch to whom correspondence should be addressed. and relies on witness clusters that are constructed from suffix arrays. sga uses a memoryefficient burrows wheeler transform and an fm-index to represent the k-mer spectrum. in addition to k-mer spectrum-based approaches, some correctors based on other techniques have been developed. shrec uses a generalized suffix trie to detect and correct substitution errors, which is further extended by hybrid shrec to additionally correct indels. coral and echo are two correctors that use the concept of multiple sequence alignment, using k-mers as seeds. coral corrects errors by constructing consensus sequences from multiple alignments, and echo by computing consensus bases using a maximum-a-posterior estimate over position specific substitution matrices. in this article, we present musket (multistage k-mer spectrumbased corrector), an efficient substitution-error-based corrector for illumina sequence data based on a k-mer spectrum approach . we introduce three techniques, namely, two-sided conservative correction, one-sided aggressive correction and voting-based refinement, to form a multistage correction workflow. the performance of musket is evaluated using both simulated and real datasets for short-read data originating from small-sized (escherichia coli), medium-sized (human chromosome 21) and large-sized (human chromosome 1) genomes. in terms of correction quality, musket is consistently one of the top performing correctors compared with hitec, sga, shrec, coral, quake, reptile and decgpu. in terms of de novo genome assembly using the sga assembler , musket yields better performance than all other evaluated correctors with respect to some commonly used metrics. in addition, musket provides support for multithreading using a masterslave model and demonstrates superior scalability on a shared-memory multi-cpu workstation, as well as highly competitive overall execution speed.we have evaluated the performance of musket using both simulated and real short-read datasets from the following three perspectives: (i) correction quality; (ii) impact on de novo genome assembly quality; and (iii) speed, parallel scalability and memory consumption. performance is compared with several publicly available correctors: hitec (v1.0.2), sga (v0.9.18), shrec (v2.2), coral (v1.4), quake (v0.3.1), reptile (v1.1) and decgpu (v1.0.6). for all correctors, we have used the default settings and disabled read trimming/discarding. for coral, we have disabled the correction of indels. as quake sometimes exceptionally exits, we have manually run each step of quake in this article. all tests are conducted on a workstation with 2 six-core intel xeon x5650 2.67 ghz cpus and 96 gb random access memory, running linux (ubuntu 12.04 lts).we have presented musket, an efficient substitution-error-based error corrector for short dna reads produced by illumina sequencing technology. this reference-free error corrector uses the k-mer spectrum approach and aims at correcting as many errors as possible while introducing few new errors. three correction techniques, including two-sided conservative correction, one-sided aggressive correction and voting-based refinement, have been introduced to form a multistage correction workflow. we have assessed the performance of musket in comparison with several established error correctors: hitec, sga, shrec, coral, quake, reptile and decgpu. the assessment is conducted using both simulated and real reads in terms of correction quality and de novo genome assembly measures. in terms of correction quality, musket is consistently one of the top performing correctors for reads simulated from the small-sized e.coli genome as well as large-sized human chromosomes. the best performance of musket is obtained using simulated reads from the human chromosomes, which suggests that musket can perform well on genomic sequences with complex repeat structures. in terms of de novo genome assembly, musket is superior to all other evaluated correctors in terms of the following metrics: n50 contig size, error-corrected n50 contig size, number of contig errors and genome coverage. through this study, we have found that pre-assembly error correction does not always guarantee to yield better assembly quality in terms of all metrics, but have demonstrated the capability of improving contig contiguities and reducing mis-assemblies. besides de novo genome assembly, short-read error correctors can also be used before short-read alignment. as many state-of-the-art short-read aligners tolerate multiple sequencing errors in the full-read length, substitution-error-based error correctors might not be able to make significant impact on the improvement of alignment performance. hence, the use of musket might be more valuable before de novo genome assembly than short-read alignment. in addition, musket uses multi-threading, based on a master slave model, to leverage the compute power of common sharedmemory multi-cpu platforms. this approach results in superior parallel scalability compared with all other evaluated correctors in terms of the number of cpu threads as well as in overall execution time. all existing standalone correctors target haploid genome sequencing .showed that error correction can benefit single nucleotide polymorphism calling in haploid genomes. however, the effect of error correction on diploid genomes has not been explored yet. a heterozygous site results in a percentage of k-mers supporting alternative alleles. in a moderate or high coverage dataset, we would expect these k-mers to be within the distribution of trusted k-mers. however, this might not be the case when the coverage is low or allelic distribution is imbalanced. in such cases, diploid single nucleotide polymorphisms could be lost as a result of false-positive corrections. up to date, almost all correctors based on k-mer spectrum use a single coverage cut-off to differentiate trusted k-mers from the untrusted ones. this static coverage cut-off ignores the confidence difference in the correctness of bases that is represented as base quality scores in ngs reads. hence, a dynamic coverage cut-off as in coral (salmela and schroderschroder, 2011) might be advantageous to further improve correction quality of correctors based on k-mer spectrums. a possible solution might be the use of a position specific cut-off matrix derived from base quality scores for a single read. conflicts of interest: none declared.  
