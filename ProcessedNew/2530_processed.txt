systems biology intensity quantile estimation and mappingâ€”a novel algorithm for the correction of image non-uniformity bias in hcs data motivation: image non-uniformity (nu) refers to systematic, slowly varying spatial gradients in images that result in a bias that can affect all downstream image processing, quantification and statistical analysis steps. image nu is poorly modeled in the field of high-content screening (hcs), however, such that current conventional correction algorithms may be either inappropriate for hcs or fail to take advantage of the information available in hcs image data. results: a novel image nu bias correction algorithm, termed intensity quantile estimation and mapping (iqem), is described. the algorithm estimates the full non-linear form of the image nu bias by mapping pixel intensities to a reference intensity quantile function. iqem accounts for the variation in nu bias over broad cell intensity ranges and data acquisition times, both of which are characteristic of hcs image datasets. validation of the method, using simulated and hcs microtubule polymerization screen images, is presented. two requirements of iqem are that the dataset consists of large numbers of images acquired under identical conditions and that cells are distributed with no within-image spatial preference. availability and implementation: matlab function files are available at http://nadonthe effectiveness of high-content screening (hcs) depends critically on data processing and analysis techniques that are appropriate to its unique technologies and data formats . in particular, image preprocessing, which represents a conditioning or preparation of the image data, is an often overlooked but essential step . image non-uniformity (nu) is a spatially varying bias that is intrinsic to all image-based biological datasets . the quality of image nu bias correction, which occurs during the preprocessing stage, can affect the validity of all subsequent data analysis steps which include image quantification and statistical inference. in particular, the methods implemented in these steps usually presuppose that the incoming image data are free of bias. image nu produces a bias in the pixel intensities across images, which causes objects of interest and their surrounding background to appear spuriously brighter or darker depending on their spatial location (e.g.;, figures 45 and 51). this can create difficulties in the identification of consistent thresholds that distinguish objects of interest from background and, as a result, reduce the accuracy of their segmentation. bias and variability are also induced in measured cell metrics which depend on both the pixel intensities and the segmentation step. reduced quality in cell metric data reduces the sensitivity and specificity of hit identification which typically relies on treatment-control comparisons of cell-metric-derived statistics . cell classification accuracy (e.g.) will also be diminished if the classification criteria depend on pixel intensity in any way. the within-image spatial variation in cell intensity caused by image nu bias also introduces spurious heterogeneity between cells which can obscure the existence of real, biologically based cell subpopulations, a topic that has gained recent interest and pertinence . in a particular, striking example of the impact of nu bias,show how the separation of the g1-and m-phase peaks in a cell-cycle classification analysis is obscured by the uncorrected image nu bias. further discussion of the impact of image nu on image analysis and inference can be found in,the field of hcs, however, the nature of this bias, its impact on subsequent analyses and appropriate correction methods are often poorly understood or overlooked, which leads to the choice of suboptimal correction methods. for example, a large number of hcs studies make use of background subtraction type algorithms (such as the rolling ball) which, despite providing visually appealing results, does not correct the higher cell foreground intensities whose quantification is of paramount interest in hcs data analysis. multiplicative correction and calibration-based methods also have limitations, since hcs assays typically comprise extreme ranges of cellular intensities (e.g.) and are also susceptible to instrumental drift effects. in general, various pre-existing nu bias correction methods and models developed for low-throughput microscopy, which may be either inappropriate for hcs or fail to take advantage of the additional information available in high-throughput contexts, have been the methods of choice for hcs studies. we present the intensity quantile estimation and mapping (iqem) algorithm, a novel image nu correction method that makes use of the asymptotic spatial homogeneity property found in large throughput (hcs) datasets (c.f. section 3.1.1 and supplementary material appendix b). (the method is not applicable to the correction of individual images acquired in a low throughput setting). the method accounts for characteristics and analysis objectives of hcs which include (1) large image datasets that comprise extremely diverse ranges of a priori unknown cell phenotypes and thus intensity levels; (2) image acquisition times that can extend over days and which lead to instrumental biases or drifts that can vary with experimental batch and (3) the need for multiple inter-image and inter-plate statistical comparisons over large numbers of images, and the critical importance of controlling the error rates in the resultant hit detection. concepts of image nu bias and limitations of existing correction methods used in hcs are first described, followed by a theoretical description and practical implementation of the iqem method. finally, validation tests using simulated image datasets as well as empirical hcs assay images acquired from a multiparametric assay on microtubule polymerization status (c.f. supplementary material appendix a and) are shown.the iqem algorithm represents an improvement over existing methods of image nu correction used in hcs, which are based on varying degrees of simplification to a linear model approximation of nu bias. by estimating the full non-linear form of the nu bias, the iqem method essentially applies a correction factor that is appropriate to each intensity quantile in the measured image. the method is particularly pertinent for the quantification of extremely low-intensity cell phenotypes, where multiplicative correction provides an inaccurate fit to the low-intensity image nu, and where background subtraction does not adequately model the range of dim intensity levels. an additional positive feature of the iqem algorithm is that it can be applied on a batch-specific basis such that a unique image nu correction is estimated for each batch. the iqem method does require that cell placement and intensity are spatially homogeneous within images (i.e. that the asymptotic spatial homogeneity condition holds). assays that have been properly calibrated, and for which there are no systematic within-image gradients caused by poor optical focus, non-uniform fluorophore or reagent concentrations or cell density gradients (c.f. supplementary materials section b.5), for example, should satisfy this condition. the measurement of multiple fields within wells, a standard practice in hcs, further ensures that on average, the spatial homogeneity condition will hold. although the iqem algorithm may be more complex than other conventional methods of nu bias correction, it is readily automated, and its complexity will not be apparent from the users perspective once integrated into an image-processing pipeline. the iqem algorithm arose during the analysis of the multiparametric assay of microtubule status, in which a method of batch-specific nu bias correction effective for very low-intensity, depolymerized phenotypes was needed . the development of iqem provides an illustration that methods adopted from low-throughput science may not necessarily be optimal in a high-throughput context, which involves the analysis of large-scale image datasets, the testing of compound libraries that produce extremely diverse ranges of a priori unknown cell phenotypes/intensities and requires statistical comparisons of metrics taken across the dataset. it should further be noted that while high-throughput, high-content datasets provide challenges to analysis methods , they also provide opportunities. in the present case for example, the enormous size of hcs image datasets, which consist of images that are effectively statistical replicates within each experimental batch, makes possible the existence and estimation of the inter-image iqf; estimation of this entity in turn makes possible the inversion of the non-linear nu bias function via a mapping of intensity quantiles. in general, hcs systems comprise and integrate many complex state-of-the-art subsystems, ranging over biological experimentation (cell cultures and immunofluorescence staining for example), automated image acquisition, imaging instrumentation and image processing, each of which acts to produce or condition the high-content data, and also to contribute bias and variability that determine the resultant statistical characteristics of the data . by more fully understanding the technical and technological issues of each subsystem within an hcs workflow, it becomes possible to better model the statistical distribution of the data, and thus to improve the sensitivity and specificity of statistical inference methods used to draw scientific conclusions from hcs experiments. the current work represents a step in this direction, in the context of the image nu bias produced by hcs imaging instrumentation.  
