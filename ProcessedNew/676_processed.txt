bayesian inference with historical data-based informative priors improves detection of differentially expressed genes motivation: modern high-throughput biotechnologies such as microarray are capable of producing a massive amount of information for each sample. however, in a typical high-throughput experiment, only limited number of samples were assayed, thus the classical large p, small n problem. on the other hand, rapid propagation of these high-throughput technologies has resulted in a substantial collection of data, often carried out on the same platform and using the same protocol. it is highly desirable to utilize the existing data when performing analysis and inference on a new dataset. results: utilizing existing data can be carried out in a straightforward fashion under the bayesian framework in which the repository of historical data can be exploited to build informative priors and used in new data analysis. in this work, using microarray data, we investigate the feasibility and effectiveness of deriving informative priors from historical data and using them in the problem of detecting differentially expressed genes. through simulation and real data analysis, we show that the proposed strategy significantly outperforms existing methods including the popular and state-of-the-art bayesian hierarchical model-based approaches. our work illustrates the feasibility and benefits of exploiting the increasingly available genomics big data in statistical inference and presents a promising practical strategy for dealing with the large p, small n problem.high-throughput technologies such as microarray and next generation sequencing have become indispensable tools in biomedical research. these technologies can generate a rich set of information for each biological sample, which can be summarized into a comprehensive picture of the underlying biological processes or systems. however, due to the relatively high cost and complexity of sample preparation, the number of samples surveyed in each experiment is much smaller than the number of features surveyed in each sample.the key characteristic of such datasets can be summarized as large p, small n . this presents a tremendous challenge when conducting statistical inference on these data such as detecting differentially expressed (de) genes a fundamental problem in gene expression data analysis. hierarchical models , which are designed to borrow strength across features, provide a solution under the highdimensional data setting and have been shown to be effective in dealing with high-throughput genomics data (kerr and churchill,). the hierarchical model framework has been increasingly utilized in genomics research . it is widely accepted that hierarchical models possess key advantages over navenave, separate inference on individual features. the key idea is that hierarchical models assume that distribution parameters of all features (e.g. probes) are random draws from an upper-level prior distribution (hyperprior). as a result, the posterior distributions of these parameters regress toward the middle. despite its success, such a strategy could be a double-edged sword. on the one hand, this approach alleviates the problem of poor inference results due to small sample size; on the other hand, it inadvertently introduces biases when estimating the variances of genes that have intrinsic high or intrinsic low variance. it is a reasonable strategy if no additional information except the current experimental data is available. however, in reality, given the explosion of genomics datasets that are publicly available, there is abundant information that can be utilized and should be considered. a unique and fundamental advantage of the bayesian inference framework lies in its capability to incorporate existing prior information. bayesian inference achieves seamless integration of prior knowledge and observed data hence is desirable in solving real practical problems . because technologies like microarray have been widely adopted, there are plenty of publicly available data (referred to as historical data hereafter). we believe such information should be taken advantage of, and the bayesian framework provides an attractive avenue for implementing such a strategy. although historical data have been exploited in other contexts (for example,applied a historical database of microarray experiments to adjust background for dna microarrays), we found none of the existing methods for detecting de genes explicitly utilizes historical data in a bayesian setting. our hypothesis is that the expression value of each gene (or its surrogate probe on the microarray) has its unique distribution which reflects its intrinsic biological properties. for example, when historical data collected under diverse conditions were aggregated together, compounded with limited signal range of the microarray technology, measurements of house-keeping genes tend to show high means but relatively small variances across conditions; whereas genes responding to stimuli tend to have large variances since their expression values can go either way. therefore, assuming proper normalization has been performed across samples, to perform statistical inference, we believe it is perhaps a better strategy to use data that are collected from different experiments but the same gene, than data collected from the same experiment but different genes. to illustrate the point, using 566 normal solid tissue microarray datasets obtained by affymetrix genechip u133a from the global gene expression map of microarray data built by(details about the datasets can be found in the results section), we plot standard deviation versus mean on 22 283 probes of their normalized and log-transformed expression values . we observe a crescent shape in the plot, probes with low or high means tend to have small variance (measured by standard deviation in figures and tables), while probes with mid-level means tend to have large variance. we choose 100 probes from each of the three spots that correspond to low mean/small variance, mid-level mean/large variance and high mean/small variance, respectively, and perform a gene ontology (go) enrichment analysis on each set of the corresponding genes using david . more details can be found in the supplementary tables s1s3 and. the result appears to support our hypothesis. we find that the genes in group 3 are mostly involved in housekeeping activities evidenced by enriched functional categories such as translation elongation or ribosome-related. genes in group 2 are mostly known for being responsive to stimuli. genes in group 1 show no functional enrichment, perhaps because they are barely expressed.in this study, we present a novel strategy of reutilizing relevant information contained in historical data to improve de gene detection. simulation studies and real data applications show that our method ipbt significantly outperforms other existing methods in terms of both accuracy and consistency in detecting de genes. in particular, when the de genes have relatively low intrinsic variances, methods based on the bayesian hierarchical models perform poorly whereas ipbt maintains its superior performance. bayesian hierarchical model provides an attractive statistical framework for handling large p, small n inference problems.bayesian inference with historical data-based informative priorsbecause it can borrow information from all genes in the genome to aid the inference on a single gene so that the poor performance due to limited sample size can be improved. however, as we showed in this study, the bayesian hierarchical model approach can suffer from the over-correction problem and produce false negatives. in addition, the empirical bayesian approach assumes a common prior for every gene, which will limit the effectiveness of the approach for genes with dramatically different behaviors. in contrast, ipbt assumes gene-specific, informative priors. with the rapid proliferation of high-throughput genomics big data, deriving these informative priors is no longer an issue. meta-analysis is a powerful tool for combining multiple studies of a related hypothesis and has been applied to microarray data . our approach is different from meta-analysis because historical data used in ipbt may come from experiments with a different hypothesis, and the historical data is used indirectly in the form of informative priors in bayesian inference. there is much room for improvement in ipbt. first, the informative prior used in ipbt is gene-specific so de gene analysis is done gene-by-gene. in reality we know some genes are correlated with each other such as genes located in the same pathway or sharing similar biological functions. a potential extension of ipbt is to introduce correlation among genes. correlation information can be derived from biological knowledge or historical data. recent studies have demonstrated the benefit of incorporating correlation information in the inference of de genes . second, the current ipbt method uses normal distribution to model log transformed expression measures. the distribution choice is made mainly for mathematical convenience. one can replace normal distribution with other non-normal ones to achieve robustness in inference in the same way ashave done in their study of de gene detection. third, we assume the expression values used by ipbt have already been background-corrected and normalized. this is possible with the powerful normalization techniques such as rma. it is however, desirable if additional consideration is factored in the model to account for subtle experiment-to-experiment biases in the data as shown in studies such as. this will potentially make ipbt more flexible and further improve its performance. we are planning to pursue such extensions to ipbt in future follow-up studies.  
