genome analysis densitycut: an efficient and versatile topological approach for automatic clustering of biological data motivation: many biological data processing problems can be formalized as clustering problems to partition data points into sensible and biologically interpretable groups. results: this article introduces densitycut, a novel density-based clustering algorithm, which is both time-and space-efficient and proceeds as follows: densitycut first roughly estimates the densities of data points from a k-nearest neighbour graph and then refines the densities via a random walk. a cluster consists of points falling into the basin of attraction of an estimated mode of the underlining density function. a post-processing step merges clusters and generates a hierarchical cluster tree. the number of clusters is selected from the most stable clustering in the hierarchical cluster tree. experimental results on ten synthetic benchmark datasets and two microarray gene expression datasets demonstrate that densitycut performs better than state-of-the-art algorithms for clustering biological datasets. for applications, we focus on the recent cancer mutation clustering and single cell data analyses, namely to cluster variant allele frequencies of somatic mutations to reveal clonal architectures of individual tumours, to cluster single-cell gene expression data to uncover cell population compositions, and to cluster single-cell mass cytometry data to detect communities of cells of the same functional states or types. densitycut performs better than competing algorithms and is scalable to large datasets. availability and implementation: data and the densitycut r package is available from https://bit bucket.org/jerry00/densitycut_dev. contact:clustering analysis (unsupervised machine learning), which organizes data points into sensible and meaningful groups, has been increasingly used in the analysis of high-throughput biological datasets. for example, the cancer genome atlas project has generated multiple omics data for individual patients. one can cluster the omics data of individuals into subgroups of potential clinical relevance. to study clonal evolution in individual cancer patients, we can cluster variant allele frequencies of somatic mutations, such that mutations in the same cluster are accumulated during a specific stage of clonal expansion. emerging technologies such as single-cell sequencing have made it possible to cluster single-cell gene expression data to detect rare cell populations, or to reveal lineage relationships . one can cluster single-cell mass cytometry data to study intratumour heterogeneity . as measurement technology advances have drastically enhanced our abilities to generate various high-throughput datasets, there is a great need to develop efficient and robust clustering algorithms to analyze large n (number of data points), large d (dimensions of data) datasets, with the ability to detect arbitrary shape clusters and automatically determine the number of clusters. the difficulties of clustering analysis lie in part with the definition of a cluster. of the numerous proposed clustering algorithms, the density-based clustering algorithms are appealing because of the probabilistic interpretation of a cluster generated by these algorithms. let d fx i g n i1 ; x i 2 r d be drawn from an unknown density function f x; x 2 x & r d. for model-based approaches such as gaussian mixture models f x p c c1 p c n xjl c ; r c , a cluster is considered as the points generated from a mixture component, and the clustering problem is to estimate the parameters of the density function from d . to analyze datasets consisting of complex shape clusters, nonparametric methods such as kernel density estimation can be used to estimate b f x p n i1 k h x; x i , where k h is the kernel function with bandwidth h. here, a cluster is defined as the data points associated with a mode of the density function f x . the widely used mean-shift algorithm belongs to this category, and it locates the modes of the kernel density function b f x by iteratively moving a point along the density gradient until convergence. this algorithm, however, is computationally expensive, having time complexity on 2 t, where t is the number of iterations, typically dozens of iterations are sufficient for most cases. a more efficient, non-iterative graph-based approach constructs trees such that each data point x i represents a node of a tree, the parent of node x i is a point x j which is in the direction closest to the gradient direction r b f x i , and the root of a tree corresponds to a mode of b f x. then each tree constitutes a cluster. this algorithm has been used to reduce the time complexity of the mean-shift algorithm to on 2 , and has been extended in several ways, e.g. constructing trees after filtering out noisy modes . nonparametric clustering methods have been generalized to produce a hierarchical cluster tree . consider the k level set of a density function f(x): lk; f x fxjf x ! kg:the high level clusters at level k are the connected components of lk; f x (in the topological sense, the maximal connected subsets of lk; f x). as k goes from 0 to maxf x, the high level clusters at all levels constitute the level set tree, where the leaves of the tree correspond to the modes of f x . the widely used dbscan algorithm extracts the high level clusters at just one given level k. many original approaches for level set tree construction in statistics take the straightforward plug-in approach to estimating the level set tree from b f x by partitioning the feature space, i.e. x. therefore, they are computationally demanding, especially for high-dimensional data. recently, efficient algorithms have been proposed to partition the samples d directly . recovering the level set tree from a finite dataset is more difficult than partitioning the dataset into separate clusters. correspondingly, theoretical analyses show that for these algorithms to identify salient clusters from finite samples, the number of data points n needs to grow exponentially in the dimension d . moreover, although the level set tree provides a more informative description of the structure of the data, many applications still need the cluster membership of each data point, which is not available directly from the level set tree. the spectral clustering algorithm works on an n by n pairwise data similarity matrix s, where each element s i;j measures the similarity between x i and x j. the similarity matrix can be considered as the adjacency matrix of a weighted graph g v; e, where vertex v i represents x i and the edge weight e i;j s i;j. given the number of clusters c, the spectral clustering algorithm partitions the graph g into c disjoint, approximately equal size clusters, such that the points in the same cluster are similar, while points in different clusters are dissimilar. in contrast to density-based methods, the spectral clustering algorithm does not make assumptions on the probabilistic model which generates data d . therefore, selecting the number of clusters is a challenging problem for spectral clustering algorithms, especially in the presence of outliers or when the number of clusters is large. in addition, the spectral clustering algorithm is time-consuming because it needs to compute the eigenvalues and eigenvectors of the row-normalized similarity matrix s, requiring h n 3 time. instead of using single value decomposition to calculate the eigenvalues and eigenvectors, the power iteration clustering algorithm (pic) iteratively smoothes a random initial vector by the row-normalized similarity matrix, such that the points in the same cluster will be similar in value. then the k-means algorithm is used to partition the smoothed vector into c clusters. although pic has a time complexity of on 2 t, where t is the number of iterations, pic may encounter many difficulties in practice. first, the points from two quite distinct clusters may have very similar smoothed densities, and therefore they may not be distinguishable by k-means. second, the points in a non-convex shape cluster can break into several clusters. as the number of clusters increases, these problems become more severe . in this article, we introduce a simple and efficient clustering algorithm, densitycut, which shares some advantages of both densitybased clustering algorithms and spectral clustering algorithms. as for spectral clustering algorithms, densitycut works on a similarity matrix; thus it is computationally efficient, even for high-dimensional data. using a sparse k-nearest neighbour graph further reduced the time complexity. besides, we can use a random walk on the k-nearest neighbour graph to estimate densities at each point. as for many density-based clustering algorithms, densitycut is simple, efficient, and there is no need to specify the number of clusters as an input. moreover, densitycut inherits both methods advantage of detecting arbitrarily shaped clusters. finally, densitycut offers a novel way to build a hierarchical cluster tree and to select the most stable clustering. we first benchmark densitycut against widely used ten simulation datasets and two microarray gene expression datasets to demonstrate its robustness. we then use densitycut to cluster variant allele frequencies of somatic mutations to infer clonal architectures in tumours, to cluster single-cell gene expression data to uncover cell population compositions, and to cluster single-cell mass cytometry data to detect communities of cells of the same functional states or types.  
