structural bioinformatics drop: an svm domain linker predictor trained with optimal features selected by random forest motivation: biologically important proteins are often large, multidomain proteins, which are difficult to characterize by high-throughput experimental methods. efficient domain/boundary predictions are thus increasingly required in diverse area of proteomics research for computationally dissecting proteins into readily analyzable domains. results: we constructed a support vector machine (svm)-based domain linker predictor, drop (domain linker prediction using optimal features), which was trained with 25 optimal features. the optimal combination of features was identified from a set of 3000 features using a random forest algorithm complemented with a stepwise feature selection. drop demonstrated a prediction sensitivity and precision of 41.3 and 49.4, respectively. these values were over 19.9 higher than those of control svm predictors trained with non-optimized features, strongly suggesting the efficiency of our feature selection method. in addition, the mean ndo-score of drop for predicting novel domains in seven casp8 fm multidomain proteins was 0.760, which was higher than any of the 12 published casp8 dp servers. overall, these results indicate that the svm prediction of domain linkers can be improved by identifying optimal features that best distinguish linker from non-linker regions. availability: drop is available atbiologically significant proteins are often large and consist of many domains, which make them difficult to characterize by highthroughput experimental methods . efficient methods for dissecting proteins into structural domains that can be readily analyzed are gaining practical importance in proteomics research . experimental approaches for identifying structural domains are mostly based on limited proteolysis, but these methods require to whom correspondence should be addressed. significant quantities of proteins, and large amount of time and effort. computational domain prediction methods are thus being actively investigated . methods that can predict domain regions without using sequence similarity to an existing domain cataloged in reference databases such as pfam and prosite are particularly useful, as they may lead to the discovery of novel domains, which are preferred targets of proteomics projects. many domain prediction methods first detect domain boundaries or linkers, and in turn assign the location of the domain regions. this strategy takes advantage of the local nature of the boundary/linker sequence characteristics . initially, simple domain linker predictors that use variations in amino acid composition between domains and domain linkers were proposed [e.g. uma , domcut , armadillo and dlip (. more recently, domain linker prediction performances were improved by the use of machinelearning methods . additionally, position specific scoring matrix (pssm) could further improve the performances of domain linker predictions [e.g. nagarajans method , chopnet and pprodo (. though the ability of machine-learning methods for predicting domain linkers appears well established, their performances might be further improved by selecting optimal features for distinguishing linkers from non-linkers. the previously derived domain linker properties combined with pssm elements as well as 544 recently cataloged amino acid properties would represent a vast feature space from which the best or the nearly best subsets could be searched. however, no systematic search from such a large number of features has yet been reported, and when systematic searches were carried out, they were applied to feature sets of modest sizes . this is probably because a huge number of feature combinations need to be tested by trial-and-error, which requires a significant amount of computational time. random forest, which is based on random sampling, could potentially provide a method for rapidly screening the optimal features . it was originally developed as an ensemble classifier based on a collection of decision trees, and with each decision tree, randomly chosenpage: 488 487494  
