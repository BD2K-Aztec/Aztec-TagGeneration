debwt: parallel construction of burrowsâ€“wheeler transform for large collection of genomes with de bruijn-branch encoding motivation: with the development of high-throughput sequencing, the number of assembled gen-omes continues to rise. it is critical to well organize and index many assembled genomes to promote future genomics studies. burrowswheeler transform (bwt) is an important data structure of genome indexing, which has many fundamental applications; however, it is still non-trivial to construct bwt for large collection of genomes, especially for highly similar or repetitive genomes. moreover, the state-of-the-art approaches cannot well support scalable parallel computing owing to their incremental nature, which is a bottleneck to use modern computers to accelerate bwt construction. results: we propose de bruijn branch-based bwt constructor (debwt), a novel parallel bwt construction approach. debwt innovatively represents and organizes the suffixes of input sequence with a novel data structure, de bruijn branch encoding. this data structure takes the advantage of de bruijn graph to facilitate the comparison between the suffixes with long common prefix, which breaks the bottleneck of the bwt construction of repetitive genomic sequences. meanwhile, debwt also uses the structure of de bruijn graph for reducing unnecessary comparisons between suffixes. the benchmarking suggests that, debwt is efficient and scalable to construct bwt for large dataset by parallel computing. it is well-suited to index many genomes, such as a collection of individual human genomes, with multiple-core servers or clusters. availability and implementation: debwt is implemented in c language, the source code is available at httpswith the rapid development and ubiquitous application of highthroughput sequencing, many genomes have been sequenced in cutting-edge genomics studies. for example, 1000 genomes and uk10k projects have sequenced many thousands of individual human genomes. moreover, as the cost of sequencing continuously decreases, e.g. the cost of sequencing a human sample has already been lower than 1000 dollars , the number of genomes may explosively increase in the future. under this circumstance, it is fundamental to well organize and index the large amount of genomes to facilitate future genomics studies. burrowswheeler transform (bwt;) is a self-indexing data structure having many fundamental applications, such as genome indexing , sequence alignment , genome compression , genome assembly and sequencing error correction . however, the bwt construction of genomic sequence(s) is a non-trivial task. mainly, the core of bwt construction is to determine the lexicographical order of all the suffixes of the input sequence(s). because there could be many repetitive sequences within a genome , the cost would be prohibitively high to straightforwardly compare all the suffixes to determine their lexicographical orders. the problem is even more serious for constructing the bwt of many highly similar genomes, such as a large collection ofwe benchmarked debwt with three datasets mimicking various real application scenarios. (i) a dataset consists of 10 in silico human genomes (totally 30.9 gbp). each of the genomes is generated by integrating the variants of a specific sample from 1000 genomes (the 1000 genomes project) into the human reference genome grch37/hg19. this dataset mimics the indexing of multiple individual human genomes, which has many applications in genomic studies. (ii) a dataset consists of a set of simulated contigs. as long read sequencing technologies, such as single molecular real-time sequencing, have improved the contig n50 of human genome assembly to 10 million bp (http://www. pacb.com/blog/toward-platinum-genomes-pacbio-releases-a-newhigher-quality-chm1-assembly-to-ncbi/), we randomly extracted 3000 sequences (each is about 10m bp long, totally 30.2 gbp) from the 10 in silico human genomes with an in-house script, which is revised from wgsim simulator (; https://github.com/ lh3/wgsim). (iii) a dataset consists of eight primate genomes including gibbon, gorilla, orangutan, rhesus, baboon, chimp, bonobo and human (downloaded from: http://hgdownload.soe.ucsc.edu/down loads.html). this dataset assessed the ability of debwt to index more diverse genomes. the benchmark was implemented on a server with four intel xeon e4820 cpus (32 cores in total) at 2.00 ghz and 1 terabytes ram, running linux ubuntu 14.04. debwt uses jellyfish (version 2.1.4;) for implementing the k-mer counting of the input sequences (the parameter k is configured as 31). two recently published methods, ropebwt2 and parabwt (version 1.0.8-binary-x86_64) , were also performed on the same datasets for comparison. at first, we tested the performance of debwt with 32 threads, i.e. running with all the 32 cpu cores of the server. parabwt was also run with 32 threads, but ropebwt2 was run with its default setting, as it does not support parallel computing. the elapsed time indicates that debwt and parabwt have comparable speed, while ropebwt2 is slower, likely owing to the fact that it does not support parallel computing and the algorithm could not be suited to long sequences. we further investigate the processing of debwt, and found that the speed of debwt was largely slowed down by jellyfish owing to the format of its output file. the default output of jellyfish is a binary file in an unpublished format. as the details about the format is unknown for us, we used the dump command of jellyfish to convert the output file into text file, and then converted the text file into binary file in our own format as the input of further steps. this file conversion costs a couple of hours for all the three datasets, i.e. about 6070 of the total running time. the time cost would be much reduced if the output format of jellyfish was available, or if other k-mer counting tools with similar performance and readable output format were used. deducting the time of file conversion, debwt is much faster than the other two methods. we investigated the time cost of the various steps of debwt . mainly, two issues are observed. first, most of the core steps of debwt, i.e. k-mer counting, kmer sorting, de bruijn branch encoding and / values generation and projection suffixes sorting, are efficient. this is because of a couple of reasons. (i) the de bruijn branch code greatly reduces the cost of sorting suffixes with long common prefixes. we investigated the lengths of the generated de bruijn branch code, and found that, for both of the genomes and the contigs datasets, their lengths are respectively one order shorter than those of the original input sequences. under this circumstance, the comparison between projection suffixes is much less expensive than that of the original suffixes. furthermore, the k-mer partition of bwt also helps to reduce many unnecessary comparison operations. (ii) the designs of these steps are suitable for parallel computing, which can fully use the multiple cpu cores. it is worth noting that, besides the parallel implementation of the core steps of debwt, the state-of-the-art k-mer counting tool also has good parallelization. as k-mer counting is still an open problem with wide application, there are a few choices for this step. we also tried a newer published tool, kmc2 , and obtained even faster speed . however, kmc2 also outputs a binary file, which is hard to directly interpret. it would be beneficial if the state-of-the-art k-mer counting tools have an easy-to-interpret output file. (iii) besides the parallelism, multiple steps, i.e. k-mer counting, the radix sort of k-mers, de bruijn branch encoding and / values generation, have quasi linear time complexity. second, the i/o operation is the main issue slowing down the method. other than the file conversion step mentioned above, there are also many i/o operations in the de bruijn graph analysis and the additional processing steps. that is, in the dbg analysis step, debwt needs to merge the files recording the four ordered lists of k-mers to recognize the multiple-in k-mers; and in the additional processing step, debwt needs to convert the large sequences to be indexed from text (fasta format) into binary format and merge various bwt parts. although these operations theoretically have low time complexity, they also depend on the performance of the file system of the computer as well as the implementation of the program.it is also an important future work for us to further optimize these operations. other than the two issues mentioned above, the time cost of the projection suffixes sorting step is especially critical, as it is the core step to handle the long repetitions within the input sequence(s). the total time cost of this step isis the time for solving the j-th unsolved part of the bwt. as each of the unsolved parts are handled by quicksort, the time cost can be represented as follows :, where n j is the number of the projection suffixes involved in the unsolved part j, and dp ps i is the length of the distinguishing prefix of the i-th projection suffix (denoted as ps i ) of the part. here, the distinguishing prefix of ps i is the shortest prefix of ps i , which is necessary to determine the bwt characters of the corresponding part. for the highly similar input sequences, g 1 , g 2 ,. . ., g nd , the upper bound of dp ps i is o de gm in theory, owing to the existence of long repetitions, where g m is the input sequence having the longest de bruijn branch encoding, and de gm is the length of the corresponding de bruijn branch encoding. for example, two sequences g i and g j could be almost the same; thus, the length of distinguishing prefix could be close to the length of the de bruijn branch encoding of the sequences. as the total number of the branching characters of g m , the value de gm does not only depend on how many unipaths g m has, but also how many copies of the unipaths there are. although the theoretical upper bound is large, however, dp ps i also greatly depends on the distributions of genomic variations as well as the repetitiveness of the input sequences, which could make it lower in practice. to more precisely investigate the time cost, we assessed the dp j values and the dp m j values of the most repetitive dataset in the benchmark (i.e. the 10 human genomes dataset), where dp j and dp m j are respectively the mean and maximal length of the distinguishing prefix of the projection suffixes within the j-th unsolved part. a series of quantiles of dp j and dp m j values of the 10 human genomes dataset are shown in. these quantiles indicate that, for most of the blocks, the distinguishing prefixes are short, e.g. the 0.90 quantile of dp m j is 11 872, indicating that for 90 of the unsolved bwt parts, the max length of the distinguishing prefixes is shorter than 11 872 characters. this is not expensive to determine their lexicographical orders by a straightforward comparison. thus, the overall cost of this step is not high, although there is still a small proportion (0.1) of bwt parts having long distinguishing prefixes (average value is 298k). we also run debwt with 8, 16, 24 threads to investigate its scalability. the results suggest that debwt can gradually speedup with the increase of threads, i.e. it has good scalability. however, the speed of parabwt is nearly the same with the various settings on threads. this is likely owing to the incremental nature of the parabwt method, which may limit its performance on modern servers and clusters. the time of the various steps of debwt with various numbers of threads is in. it indicates that the two core steps, de bruijn branch encoding and / values generation and projection suffixes sorting (steps 4 and 5 in the figure), are most scalable steps, i.e. they speedup with the increasing number of threads. this property is beneficial for implementing the method with more computational resources. we further run debwt on the in silico human genome dataset with various configurations on the k parameter to investigate its effect. it can be observed from the result that, on a large range of k parameters, i.e. k 2331, the total running time is close, but for smaller k parameter, the time consumption is higher. this is likely owing to the fact that the moderate long k-mers (such as 23to 31-mers) may have similar ability to span short repeats. in this situation, the structure of the dbg does not change much with these k configurations, i.e. there are similar numbers of unipaths as well as their copies in the graph. however, when k is smaller, the unipaths will be shorter and have more copies, which would make the de bruijn branch encoding longer and more projection suffixes need to be sorted. k 32 could have better ability to span repeats, which may improve the overall performance; however, it requires much more ram space, as a k-mer cannot be stored by one 64-bits cell. the memory footprint of debwt depends on both of the method itself and the used k-mer counting tool. the memory usage of jellyfish and kmc2 is highly configurable, and we set them to use relatively large memory to accomplish the k-mer counting step as fast as possible. the major ram costs of the three phases of debwt are different. in the first phase, the major cost originates from the data structure of k-mer sorting. briefly, debwt uses a linear table like pde s to bin all the k-mers; however, each cell of the table costs 16 bytes to record the string of the k-mer as well as its number of copies. the cost of the second phase is more complicated. it needs to simultaneously keep the input sequences, the de bruijn branch index and the generated de bruijn branch encoding in memory. thus, the memory usage mainly depends on several issues, i.e. the size of the input sequence(s), the numbers of multiple-out and-in k-mers and the numbers of the copies of the multiple-out and-in kmers. the last two items respectively determine the length of de bruijn branch encoding and the number of unsolved suffixes, which need to record in memory. the numbers of the multiple-out and-in k-mers and their copies highly relate to the repetitiveness of the input genomes. we did statistics on the two human datasets (as they are more repetitive), and observed two issues .first, for both the datasets, the numbers of multiple-out and-in k-mers are much less than s j j, i.e. the number of characters of the input sequences. thus, the cost of the hash table is not expensive comparing with the entire input sequences. moreover, it is also worth noting that for highly similar genomes, the increment of the numbers of multiple-out and-in k-mers would be much smaller comparing with the increment of involved genomes, as there are many common sequences and they would not introduce new branches into the dbg. second, the numbers of the copies of multiple-out and-in k-mers are also an order lower than s j j, although human genomes are repetitive. in this situation, the de bruijn branch encoding can be seen as a dna sequence an order shorter than s, so that the space cost is not large. the major cost originates from the copies of multiple-in k-mers, as it needs to record the / value and the bwt character with a few bytes for each copy. the ram cost of the third step is also similar to that of the second phase. to sort the projection suffixes, it needs to keep the de bruijn branch encoding and the / values and the bwt characters of the copies of the multiple-in k-mers in ram.the well organization and indexing of many genomes will be on wide demand in future genomics studies, with the rapid increase of assembled genomes. as an important genome indexing data structure, bwt may have many applications; however, the construction of bwt for a large collection of genomes, especially highly similar re-sequenced genomes (e.g. many human individual genomes), is still a non-trivial task. moreover, owing to the incremental nature of the state-of-the-art methods, it is hard to construct bwt with scalable parallel computing. this is a bottleneck to fully use the computational resources of modern servers or clusters to handle large amount of data. we propose debwt, a novel parallel bwt construction approach, to break the bottleneck. the main contribution of debwt is its dbg-based representation and organization of suffixes, which facilitates the comparison of suffixes with long common prefixes and avoid unnecessary comparisons. moreover, owing to its nonincremental design, debwt has good scalability to various computational resources. these properties make debwt well-suited to construct bwt for large collections of highly similar or repetitive genomes with modern servers or clusters. in the experiments, debwt achieves a substantial improvement on the speed of indexing multiple individual human genomes and contigs. for more diverse genomes, e.g. multiple primate genomes, debwt also shows faster speed and better parallelization; however, the improvement is smaller, likely owing to that the density of the dbg is lower. that is, there are more k-mers and unipaths to handle, but the overall repetitiveness of the input is lower than highly similar genomes. comparing with state-of-the-art approaches, debwt has obviously larger memory footprint. there are potential solutions to reduce the memory footprints of the various phases of debwt. for phase 1, it is feasible to bin the k-mers into several subsets and separately sort each of the subsets with limited memory. the. time consumption of the various steps of debwt. the bars respectively indicate the elapsed time (in minutes) of the various steps of debwt for the 10 human genomes dataset (a), the human genome contig dataset (b) and the 8 primate genomes dataset (c). bars in the same color correspond to a specific number of threads, i.e. blue, red, green and purple bars are respectively for 8, 16, 24 and 32 threadsdebwt indicates the elapsed time of debwt, and debwt (no conversion) deducts the time of the format conversion of jellyfish output file.for the x/y/z of debwt in the memory columns, the x, y and z values respectively indicate the memory footprints of jellyfish, phase1 of debwt, and phases2 and phases3 of debwt. results of the multiple subsets can be straightforwardly merged into the ordered list of all the k-mers with small memory space. for phase 2, it is also possible to reduce the memory footprint by keeping only a proportion of / values and bwt characters, which can be implemented with the following strategy. because all the multiple-in k-mers and their numbers of copies are known before the second phase, it can partition the whole set of multiple-in k-mers into several subsets. each of the subsets has a limited number of k-mer copies. thus, the second phase can be done with multiple times of scanning on the input sequences, instead of one time. in each time of scanning, only the copies of the multiple-in k-mers within the corresponding subset are recognized, recorded and output to a specific file with limited ram space. as all the subsets are independent to each other for the third phase, the files of the subsets can be separately processed to generate various parts of bwt. further, the bwt parts can be directly merged to accomplish the construction. this strategy is feasible to limited workspace, but at the expense of time owing to the fact that it needs multiple executions of phase 2. for phase 3, it can also keep only a proportion of unsolved of bwt partitions in memory as all such partitions are independent. there are two possible improvements on debwt, which are important future works for us. first, debwt straightforwardly sorts the projection suffixes by quick-sort. because the de bruijn branch encoding can be also seen as a special dna sequence, it is also possible to use other approaches to further accelerate the projection suffix sorting step. for example, the method proposed by karkkainen (2007) uses dcs to accelerate the sorting of the binned suffixes of the original input sequence. this method could be also used for sorting the binned projection suffixes without loss of the ability of parallel computing, as it is non-incremental. second, for the current version of debwt, the i/o-intensive steps are still not optimized, which slowed down the speed. we plan to further optimize the i/o-intensive steps to improve the efficiency of debwt. meanwhile, as k-mer counting is still an open problem, and advanced k-mer counting tools are developing , we also plan to replace jellyfish by other more advanced k-mer counting tools, or remove the file conversion step by directly accessing the default jellyfish output file, to break the practical bottleneck of the method.  
