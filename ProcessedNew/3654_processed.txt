predicting regulatory variants with composite statistic motivation: prediction and prioritization of human non-coding regulatory variants is critical for understanding the regulatory mechanisms of disease pathogenesis and promoting personalized medicine. existing tools utilize functional genomics data and evolutionary information to evaluate the pathogenicity or regulatory functions of non-coding variants. however, different algorithms lead to inconsistent and even conflicting predictions. combining multiple methods may increase accuracy in regulatory variant prediction. results: here, we compiled an integrative resource for predictions from eight different tools on functional annotation of non-coding variants. we further developed a composite strategy to integrate multiple predictions and computed the composite likelihood of a given variant being regulatory variant. benchmarked by multiple independent causal variants datasets, we demonstrated that our composite model significantly improves the prediction performance. availability and implementation: we implemented our model and scoring procedure as a tool, named prvcs, which is freely available to academic and non-profit usage at http://jjwanglab. org/prvcs.interpreting functions of non-coding regulatory variants is an important topic in current genetics study because the majority of the variants discovered by genome-wide association studies (gwass) and large-scale cancer whole-genome sequencing studies are located in the non-coding regulatory regions . thus, evaluating and prioritizing the functional impact of regulatory variants, especially for their roles in disease pathogenicity and applications in personalized medicine, are major challenges in current human genetics. with the accumulation of functional genomics data, computational methods have been developed to predict and prioritize noncoding regulatory variants . strategies such as supervised learning trained on different gold standard datasets as), gwava , funseq , funseq2 , gwas3d , surfr , dann and fathmm-mkl , can achieve satisfactory performances based on different levels of functional annotations and causality assumptions. however, current methods either performed poorly or acted inconsistently compared with in vivo saturation mutagenesis of enhancer region . to systematically assess the performance and consistency of current methods, comprehensive evaluations are needed using different genome-wide benchmark datasets. in addition, computing and querying prediction results from separate algorithm/ database/web server is a time-consuming process. resources which can integrate pre-calculated prediction scores for prevalent algorithms will benefit the functional annotation of regulatory variants. furthermore, it has been demonstrated that combining multiple algorithms significantly outperforms each single measurement in prioritizing disease-causing non-synonymous single nucleotide variants and positively selected loci , which implies potential effectiveness in non-coding regulatory variants prioritization. in this study, we first compiled genome-wide prediction scores from eight tools that prevalently used in predicting non-coding regulatory variants. we observed significant inconsistence among these investigated predictions. to borrow the potential complementarities and strengths of different tools, we used a composite strategy to integrate multiple predictions and compute the composite likelihood of a given variant being causal in gene regulation. we demonstrated that our method significantly improved the performance of regulatory variants prediction and prioritization in several independent benchmark datasets.in summary, we have addressed several essential problems in the field of regulatory genetic variants prioritization. we provide an integrative and lightweight resource to facilitate the efficient query of prediction scores for current prevalent algorithms. the refined training and benchmark data of regulatory variants could be used to evaluate subsequent methods in the future. the inconsistent prioritization among existing tools impedes the identification of true regulatory variants. compared with the field of disease-causal nonsynonymous variant prediction , ensemble methods are urgently needed to predict and prioritize non-coding regulatory variants. our composite strategy takes advantage of the complementary attributes of individual tools to achieve a better performance. identifying the high quality and confident causal regulatory variants training dataset (including functional and pathogenic) and corresponding control is challenging, because the mechanisms of gene regulation are complicated. regulatory variants could affect many different gene regulation processes such as transcription factor binding, nucleosome positioning, epigenomic modification and noncoding rna tethering . the limited number ofpredicting regulatory variants with composite statisticexperimentally validated regulatory variants impedes the comprehensive and sufficient capture of these regulatory events. for example, there are only a few hundred known causal non-coding variants in clinvar and oreganno databases, and these variants are highly region-biased (lots of clinvar pathogenic variants are colocated; many oreganno variants are located in the tss region). although current massively parallel reporter assay has been applied to investigate the allele effect on gene expression , studies were only carried out on limited chromosome regions and inevitability lost chromatin context. on the other hand, current high-density genotyping arrays and sophisticated fine-mapping strategies enable us to identify the most likelycasual variants from large scale gwas and qtl studies. the most widely used hgmd database has integrated many diseaseassociated variants . although we still face difficulty to identify false positive hits from ld proxy of gwas fine-mapped snps, to construct larger and less regionbiased training dataset, incorporating the most reliable gwas/qtl fine-mapping results would be a temporary, practicable solution in the non-coding regulatory variant prediction field. besides, selecting appropriate control dataset could improve the training model. comparing with the random/regional sampling, our control selection strategy can avoid the bias of specific region selection (such as promoter) and remove all causal ld blocks that may contain bias of gwas ascertainment. however, there is still no guarantee that those variants are not functional. also, some of our control snps could locate in the intron region and regulate pre-mrna processing and splicing. furthermore, snps in the exonic region, which were omitted by our selection, can also regulate the gene expression . the correlations among the investigated existing methods are from weak to moderate, which might be attributed to the different perspectives and logics of existing algorithms. cadd and dann applied fixed or nearly fixed human-derived alleles and simulated de novo mutations to train the model, which focus on classifying the deleterious variants from neutral/selected variants. however, our refined training dataset summarized causal variants from a regulatory angle by merging the functional regulatory, deleterious and pathogenic non-coding variants. therefore, compared with tools trained on hgmd (gwava, fathmm-mkl and surfr) or under regulatory assumption (gwas3d, funseq and funseq2), cadd and dann didnt perform well in most of the evaluations on regulatory qtls but obtained good performance on clinvar dataset. this may suggest that our composite method is very suitable to prioritize functional regulatory variants instead of identifying pathogenic non-coding variants using dann and cadd. in addition, certain annotation features were frequently incorporated into many algorithms, resulting in the similar scoring scheme of specific variants. clearly, cadd and dann utilized same feature set and are hence moderately correlated. also, encode genomic/epigenomic annotations, as well as base-wise evolutionary information [like gerp and phastcons (, were substantially adopted in funseq, funseq2, gwas3d, surfr and fathmm-mkl. interestingly, cadd and fathmm-mkl used different training datasets but correlated well with each other, probably due to large number of shared annotation features. the better performance of our subset combination model than the full model may reflect these redundant or even conflicted relationships among existing tools. nevertheless, large and independent gold standard is needed to test the correlation of different tools and stability of reduced combination model. furthermore, for some machine learning-based programs like cadd, dann, gwava and fathmm-mkl, they used training dataset partially overlap with our refined training dataset, so the performance might be inflated in cross validation. therefore, completely independent and highquality causal non-coding regulatory variants are needed.  
